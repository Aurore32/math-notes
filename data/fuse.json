{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Math notes","n":0.707},"1":{"v":"## By Turbo Huang","n":0.5}}},{"i":2,"$":{"0":{"v":"Vector Calculus","n":0.707},"1":{"v":"\n![alt text](assets/images/image-3.png)\n\n> *Is this a consequence of the Divergence Theorem?*","n":0.316}}},{"i":3,"$":{"0":{"v":"Tensors","n":1},"1":{"v":"> Tensors are things that are elements of tensor spaces. Basically, if it looks like a tensor and walks like a tensor, it's a tensor, except for when it isn't; and if it smells like a tensor and talks like a tensor, it's probably a tensor but you should ask it \"what are you?\" just to make sure. If it says \"a fancy list of numbers\", call the nearest pest-control crew or spray it with herbicide yourself; it's three machine-learning computer scientists in disguise. If it says \"past, present, and future\", run quickly and never look back. But if it says \"a tensor\", it might just be a tensor, unless it's not. Please help me get out of here I haven't seen my family in six months","n":0.089}}},{"i":4,"$":{"0":{"v":"Tensors are Tensors","n":0.577},"1":{"v":"## What are tensors?\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A tensor is something that transforms like a tensor.\n\nPerhaps one of the greatest distillations of human knowledge in written form ever to be recorded, alongside similar masterpieces such as \"Newtonian physics are physics that make sense in a Newtonian frame of reference\", \"I would've won if I didn't lose!\", and \"risk of early-onset dementia declines sharply after the age of 70\". \n\nJust like how you could technically define a chair as being \"a four-legged object somebody could sit on, excepting cows, pigs, sofas, and all non-chair things\", a tensor is everything that looks like a tensor except for the things that aren't tensors. So what constitutes \"looking like a tensor\", and what constitutes the \"except\"?\n\n### What do tensors look like?\n\nJust like how vectors look like a list of numbers or matrices look like a list of numbers in need of a weight-loss regime or Harry Potter looks like Daniel Radcliffe, tensors also look like a list of numbers: specifically, what we call an *array*, like a matrix or a vector - except this time it has more dimensions (three dimensions, or four, or a hundred), has more indices ($T_{ijk}$ instead of $T_{ij}$ or $x_i$), and is still a big list of numbers. \n\nAll tensors, therefore, can be represented as arrays: a group of numbers placed together in some $n$-dimensional grid. But are all arrays tensors? If you have an array, what excepts it from being a tensor?\n\n### What do tensors need to be?\n\nWhat makes a vector a vector? You could just be tempted to say \"a list of numbers\", but that's not right - the list of numbers mean something specific: a point in space. A vector is a representation of a point in space.\n\nSimilarly, what makes a matrix a matrix? Basically every page of the however-many-hundred-pages-long course notes over in [[Linear Algebra]] was dedicated to stressing this: a matrix is not just an array of numbers, but a representation of a linear map - something that maps a point in $\\mathbb{R^n}$ to $\\mathbb{R^m}$.\n\nIf vectors and matrices both represent something concrete, something that goes beyond the numbers that are present in the array itself, then it's reasonable to say that if we do something to that vector or that matrix - if we **change its basis** from Cartesian to whatever coordinate system we like, that underlying \"something\" - the point represented by a vector, or the linear map by a matrix, will remain exactly the same.\n\nEssentially, that's what makes a tensor a tensor: invariance under changes of basis. A tensor represents something that doesn't change even in a different coordinate system (and as we'll see, that \"something\" is a **multi-linear map** between vector spaces), and do not depend on any set of particular coordinates: a constant of the universe, like $\\pi$ or $e$ or the speed of light.\n\n## Tensor transformation laws\n\n### Vector transformations\n\nSuppose we have a vector $\\mathbf{x} = x_i \\mathbf{e}_i$, $i = 1, 2, ..., n$, in some orthonormal basis $\\mathbf{e}_i$; this vector is a set of coordinates in the basis $\\mathbf{e_i}$ describing a point whose existence is itself independent from its coordinate system. As such, under a change of basis to an alternate set of **orthonormal** basis vectors $\\mathbf{e_i} \\to \\mathbf{e_i}'$ described by\n$$\n\\begin{aligned}\n\\mathbf{e_i}' &= R_{ij}\\mathbf{e_j} \\\\\n&= R_{i1}\\mathbf{e_1} + R_{i2}\\mathbf{e_2} + ... + R_{in}\\mathbf{e_n} \\\\\n\n\n\n\n\\end{aligned}\n$$\nwhere $R_{ik}$ are the elements of a $n\\times n$ matrix $R$, we have\n$$\n\\mathbf{e_i}' \\cdot \\mathbf{e_j}' = R_{ik}R_{jl}(\\mathbf{e_k}\\cdot\\mathbf{e_l}) =  R_{ik}R_{jl}\\delta_{kl} = R_{ik}R_{jk} = (RR^T)_{ij}\n$$\ndue to orthonormality between $\\mathbf{e_k}$ and $\\mathbf{e_l}$, and due to orthonormality between $\\mathbf{e_i}'$ and $\\mathbf{e_j}'$ we have\n$$\n(RR^T)_{ij} = \\delta_{ij}\n$$\nand thus\n$$\nRR^T = I\n$$\nwhere $I$ is the identity matrix. If $\\mathbf{e_i}' = R_{ij}\\mathbf{e_j}$, then we have\n$$\n\\begin{aligned}\n\\mathbf{e_i}' &= R_{ij}\\mathbf{e_j} \\\\\n\n&=\n\\begin{bmatrix}\nR_{i1}(e_1)_1 + R_{i2}(e_2)_1 + ... + R_{in}(e_n)_1 \\\\\nR_{i1}(e_1)_2 + R_{i2}(e_2)_2 + ... + R_{in}(e_n)_2 \\\\\n\\vdots \\\\\nR_{i1}(e_1)_n + R_{i2}(e_2)_n + ... + R_{in}(e_n)_n\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\n\\mathbf{e_1} & \\mathbf{e_2} & ... & \\mathbf{e_n}\n\\end{bmatrix}\n\n\\begin{bmatrix}\nR_{i1} & R_{i2} & ... & R_{in}\n\\end{bmatrix}^T\n\n\\end{aligned}\n$$\ninterpreting the row vector of vectors as shorthand for the matrix\n$$\n\\begin{bmatrix}\n\\mathbf{e_1} & \\mathbf{e_2} & ... & \\mathbf{e_n}\n\\end{bmatrix}  = \\begin{bmatrix}\n(e_1)_1 & (e_2)_1 & ... & (e_n)_1 \\\\\n(e_1)_2 & (e_2)_2 & ... & (e_n)_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n(e_1)_m & (e_2)_m & ... & (e_n)_m\n\\end{bmatrix}\n$$\nleading to\n$$\n\\begin{bmatrix}\n\\mathbf{e_1}' & \\mathbf{e_2}' & ... & \\mathbf{e_n}'\n\\end{bmatrix} = \\begin{bmatrix}\n\\mathbf{e_1} & \\mathbf{e_2} & ... & \\mathbf{e_n}\n\\end{bmatrix} R^T\n$$\nand\n$$\n\\begin{bmatrix}\n\\mathbf{e_1}' & \\mathbf{e_2}' & ... & \\mathbf{e_n}'\n\\end{bmatrix} R\n = \\begin{bmatrix}\n\\mathbf{e_1} & \\mathbf{e_2} & ... & \\mathbf{e_n}\n\\end{bmatrix}\n$$\ndue to the properties of *orthogonal matrices* $R$ (which, as mentioned in [[Linear Algebra.Transformation Groups]], are either rotations, reflections, or combinations of both). This gives us\n$$\n\\mathbf{e_j} = R_{ij}\\mathbf{e_i}' \n$$\n\nproviding us with our transformation law for vectors: if $\\mathbf{x} = x_i \\mathbf{e}_i$ can be written in an alternate orthonormal basis $\\mathbf{e_i}'$ as $\\mathbf{x} = x_i' \\mathbf{e_i}'$, then\n\n$$\nx_i \\mathbf{e_i} = x_i R_{ji} \\mathbf{e_j}' = x_i' \\mathbf{e_i}' = x_j' \\mathbf{e_j}'\n$$\nsimply changing index labels, resulting in\n$$\nR_{ji}x_i = x_j',\n$$\nor, relabeling,\n$$\nR_{ij}x_j = x_i'.\n$$\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Vector transformation law**. For a vector with components $x_i$ in orthonormal basis $\\mathbf{e_i}$ and new components $x_i'$ in another orthonormal basis $\\mathbf{e_i}'$, the following relationship holds true:\n$$\nR_{ij}x_j = x_i'.\n$$\nAs a vector equation, this can be expressed as\n$$\n\\mathbf{x}' = R\\mathbf{x}.\n$$\n\n> Note that the definition of $R$ here, i.e. $\\mathbf{e_i'}=R_{ij}\\mathbf{e_j}$, is actually the inverse of how we defined the transformation matrix in [[Linear Algebra.Eigenvalues and Eigenvectors.Change of Basis]]: there we had $\\mathbf{e_i}' = R_{ji}\\mathbf{e_j} = R^T_{ij}\\mathbf{e_j}$. The two definitions are inverses of one another.\n\n### Matrix transformations\n\nAs mentioned, matrices can be understood as representations of linear maps: for a vector $\\mathbf{x} \\in \\mathbb{R^n}$ and a $n \\times n$ matrix $A$, the matrix product\n$$\nA\\mathbf{x} = \\mathbf{x'}\n$$\nmaps $\\mathbf{x}$ to a new vector $\\mathbf{x'} \\in \\mathbb{R^n}$ in a way that satisfies the conditions of linearity. A *change of basis* for a linear map is equivalent to asking this: if $A$ took $\\mathbf{x}$ to $\\mathbf{x'}$ with both being written in the basis $\\mathbf{e_i}$, what matrix $A'$ would take $\\mathbf{x}$ to $\\mathbf{x'}$ in the basis $\\mathbf{e_i'}$?\n\nTo accomplish this, simply change basis for both $\\mathbf{x}$ and $\\mathbf{x'}$ as per the vector transformation law (supposing that $R$ is the transformation vector between bases $\\mathbf{e_i}$ and $\\mathbf{e_i}'$):\n$$\nA'(R\\mathbf{x}) = R\\mathbf{x'}\n$$\nwith $A'$ being the matrix after a change of basis. Using $\\mathbf{x'} = A\\mathbf{x}$, we have\n$$\nA' (R\\mathbf{x}) = R(A\\mathbf{x})\n$$\nor $A' R = RA$, giving\n$$\nA' = RAR^{-1} = RAR^{T} \n$$\nand in summation notation,\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Matrix transformation law**. For a $n\\times n$ matrix $A$ and a transformation matrix $R$ between two bases $\\mathbf{e_i}$ and $\\mathbf{e_i'}$, we have\n$$\nA'_{ij} = R_{ik}A_{kl}R^T_{lj} = R_{ik}R_{jl}A_{kl}.\n$$\n> with $A'$ being the matrix under a change of basis by $R$.\n\nNote that - crucially - **the matrix changes under a change of basis, but not the linear map itself.**\n\n### General tensor transformations\n\nCompare the vector transformation law\n$$\nx'_i = R_{ij} x_j\n$$\nand the matrix transformation law\n$$\nA'_{ij} = R_{ik}A_{kl}R^T_{lj} = R_{ik}R_{jl}A_{kl}.\n$$\nAside from the obvious - the fact that the first formula is slightly more sane and slightly less inclined towards an extended stay in the University's insane asylum (also known as its Math Department) than the second - how are these two transformation laws related? Tidying up the indices may reveal slightly more. For vectors, now labeled $T_{i_1}$ instead of $x_i$, we have\n$$\nT_{i_1}' = R_{i_1 j_1} T_{j_1}\n$$\nand for matrices, now labeled $T_{i_1 i_2}$ and with $k$, $l$ becoming $j_1$ and $j_2$, we have\n$$\nT_{i_1 i_2}' = R_{i_1 j_1} R_{i_2 j_2} T_{j_1 j_2}.\n$$\nBesides swiftly arriving at the conclusion that mathematicians should've never been let out of their pink little playpens and that every parent should mathematician-proof their house and that chalk companies deserve to be internationally sanctioned for being responsible for this crime against humanity, we *also* swiftly arrive at our Final Destination (trademark, copyright):\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **General tensor transformation law.** A tensor $T_{i_1 i_2 ... i_p}$ of rank $p$ (with $p$ indices) is something that transforms like a tensor; that means that, under a matrix $R$ representing a change of basis from $\\mathbf{e_i}$ to $\\mathbf{e_i}'$, $T$ transforms to $T'$ component-by-component as follows:\n$$\nT'_{i_1 i_2 ... i_p} = (\\Pi_{k=1}^p R_{i_k j_k})T_{j_1 j_2 ... j_p} = R_{i_1 j_1} R_{i_2 j_2} ... R_{i_p j_p} T_{j_1 j_2 ... j_p}.\n$$\n\n> Note that $R$ is required to be orthogonal (and thus a transformation matrix between two orthonormal bases, marking a rotation, a reflection or both), and that second-rank tensors (analogous to matrices) can be represented as square matrices.\n\nA tensor of rank $p$ is also known as a $p$-tensor. A tensor of rank $0$ is also sometimes known as a real number. Real numbers are nice and warm and cuddly. Why can't we be studying them all the time?\n\n### What is not a tensor?\n\n$$\nT_{ij} = \\begin{bmatrix}\n6 & 9 & 4 \\\\\n2 & 0 & 6 \\\\\n9 & 4 & 2\n\\end{bmatrix}\n$$\nThis is a tensor. We know that because it's a square, and it has loads of numbers in it, and it even has that funny little thing at the bottom we call an index. That means it has to be a tensor, right? Right?\n\nWRONG, you stupid bonehead. It's not. Or at least, we don't know if it is.\n\nIf we say a vector is a tensor or a matrix is a tensor, what does that mean, exactly? We aren't saying that the list of numbers making up the vector is a tensor; we're saying that the **point** the vector describes is a tensor, because it doesn't change under a change of basis. We aren't saying that the array making up a matrix is a tensor either; we're saying that the **linear map** the matrix represents is a tensor, because that doesn't change under a change of basis.\n\nTherefore, it's crucial that we all repeat after me: just like how a vector represents a point and a matrix a linear map, a tensor is **not** an array of numbers, but the **multilinear map** that array of numbers represents. A tensor is **not** an array of numbers, but the **multilinear map** that array of numbers represents. A tensor is **not** an array of numbers, but the **multilinear map** that array of numbers represents. ~~I haven't talked to my family in six months please get me out of here~~\n\nTherefore, if a real number **doesn't** describe a quantity that's invariant under change of basis, or if a vector **doesn't** describe the same point, or if a matrix **doesn't** describe a linear map, then they aren't tensors. Is $A = 3$ a tensor? If $A$ means area, then it isn't - area depends on the basis used. Again, is \n$$\nT_{ij} = \\begin{bmatrix}\n6 & 9 & 4 \\\\\n2 & 0 & 6 \\\\\n9 & 4 & 2\n\\end{bmatrix}\n$$\na tensor? If it's a linear map, sure, but if the numbers mean \"number of ants crawling on each square of my $3\\times 3$ chocolate bar\" then probably not.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. One final note: both the Levi-Civita symbol $\\epsilon_{ijk}$ and the Kronecker delta $\\delta_{ij}$ are tensors. They have the special property of being a little baby and not changing their components even when everyone else is changing their basis. Call these tensors **invariant tensors** (more on this later!)\n","n":0.023}}},{"i":5,"$":{"0":{"v":"Tensors are Other Things","n":0.5},"1":{"v":"\n## The existential question\n\nAfter the better part of roughly fifteen hours spent in various Starbucks around the neighborhood wrangling, duking it out, and generally having an absolute blast with tensors, we arrive at the existential question at last: why are we doing all this? Besides transforming every undergrad math lecture hall into the world's least arousing sadomasochistic blood-dungeon devoted to the pleasure of pain - and being 99% of the reason why \"import tensorflow as tf\" takes at least five different autocorrect checks to spell right - why do any of this?\n\n> Coincidentally, \"tf\" was also my reaction to finding out that tensors were a thing for the first time.\n\nIt turns out that, putting aside my adamantium-clad prejudice and fire-forged hatred for tensors and their four-dimensional states of being for a millisecond, maybe they're occasionally somewhat useful in describing physical phenomena - especially ones that don't change when your frame of reference rotates. \n\nIn fact, tensors derive their name from the Latin root \"tensio\", meaning to stretch but itself derived from \"tendere\", meaning \"tender\", which is further derived from the Japanese \"tsundere\". This is a reference to ~~how I'd like to tenderize every last one of them with a human-sized meat mallet~~ a reference to how they were originally used to measure *tensile forces*: forces that stretch, compress, and shear an object. For the simplest case, a tiny three-dimensional cube, there were nine such forces to consider: three directly parallel to the axes - stretching forces in $x$, $y$ and $z$ directions - and three *pairs* of forces which act as *shearing* forces: they stretch the object along the angle mid-way between two coordinate axes.\n\n![alt text](./assets/images/image-46.png)\n\nAs shown by the above diagram, nine forces act on a microscopic cube: $\\sigma_{ii}$ for $i= 1, 2, 3$ representing forces purely acting along a certain axes, $\\sigma_{ij}$ for $i\\neq j$ representing shearing forces which act in-between axes $i$ and $j$ and make me think of that time I used a pair of garden shears to (REDACTED) (REDACTED). By the way, how much compressive force do I need to squeeze a 70-kilogram, roughly-cylindrical object into a meter-long bathtub filled with corrosive acid? I need to know by Friday.\n\nThis lends itself naturally to the following embedding of all the information we need to know:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **stress tensor** of a microscopic cube in $\\mathbb{R^3}$ is given by\n$$\n\\mathbf{\\sigma}= \\begin{bmatrix}\n\\sigma_{xx} & \\sigma_{xy} & \\sigma_{xz} \\\\\n\\sigma_{yx} & \\sigma_{yy} & \\sigma_{yz} \\\\\n\\sigma_{zx} & \\sigma_{zy} & \\sigma_{zz}\n\\end{bmatrix}.\n$$\nEach of the entries describe a force in a certain direction - that is, even though it's just a number, the entries of this matrix are vectors. The forces these vectors represent do not change no matter what basis you're looking at them through; thus, $\\sigma$ is a rank-$2$ tensor as we understand them. \n\n> It's worth noting that $\\sigma_{ij}$ in the above matrix can itself be expressed as the contraction of a tensor product $\\sigma_{ij} = C_{ijkl}e_{kl}$, where $C_{ijkl}$ is the **elasticity tensor** and $e_{kl}$ is the **strain tensor** describing how a small displacement in the $k$-direction stretches the object in the $l$-irection.\n\nThis leads us to a better question to ask. Instead of \"where do tensors pop up\", it now becomes even more natural to ask: why use tensors at all? Why represent these forces as a tensor, instead of adding them to equilibrium via Newton's Second Law? Why disguise the physical reality of these forces under a layer of mathematical abstraction in the form of tensors, or matrices, or even vectors?\n\nThe answer - as it so often is in math, with not just tensors, but unending swarms of other things as well - is that studying tensors as a whole, not necessarily this particular stress tensor but the properties of tensors as a mathematical object in general, leads us to the discovery of useful facts and theorems about that object: tensor decompositions, tensor integral theorems, and many more. \n\nWe then use these facts we've discovered through an additional veil of abstraction and apply them back onto the non-abstract physical scenario the tensor was originally designed for - for instance, maybe a tensor decomposition would yield something useful: not necessarily here, but in many other cases indeed so. Let's look at just a few of these other cases now!\n\n## Tensors in electric fields\n\nIf you weren't obsessively figuring out a way to beat the entirety of Plants vs. Zombies using only Puff-Shrooms and throwing love-letter paper-airplanes asking a certain someone out to prom from the back row that accidentally hit your teacher instead in all four years of high school physics, you should probably remember Ohm's Law.\n\n> Wait, my therapist tells me that apparently the paragraph I just wrote is something called \"projection\" and is threatening to stop our sessions if I don't cooperate. I'm now being physically coerced to tell you that by \"you\" I actually mean \"me\", and by \"me\" I mean \"who?\"\n\nOhm's Law states that the electric current $I$ flowing through a metallic conductor placed in an electric field is proportional to the potential difference $V$ throughout the conductor:\n$$\nI = \\frac{V}{R},\n$$\nwhere $R$ is a little something we call *resistance*.\n\nThere's a lot missing from this statement of Ohm's Law, censored and redacted and baby-ified for innocent eyes not yet wise to the wickedness of the world. For one, it assumes that the metal object in question is *isotropic*; it looks exactly the same in all directions, with the same resistance, density, thickness etc. throughout. But even more importantly, it doesn't generalize well to our current microscopic understanding of the electric field: the potential difference $V$ refers to a discrete difference in potential $\\phi$ between two points, not some continuous quantity at every single point. Rather, if we were to measure potential difference continuously at every point in the field, we would have\n$$\nV = \\nabla \\phi = \\mathbf{E}\n$$\nas $\\nabla \\phi$ represents the instantaneous change of potential in all directions at that point; call this the *electric field strength* $\\mathbf{E}$ at the point. As such, we now have\n$$\nI = \\frac{\\mathbf{E}}{R}\n$$\nbut given that $I$ is now a scalar field at every point and the material is **not** necessarily isotropic, meaning it may have different resistances in every direction, we rewrite Ohm's law in the form\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Tensor form of Ohm's law for non-isotropic conductors in an electric field.** For a **current density** scalar field $\\mathbf{J}$, an electric field $\\mathbf{E}$ and a **conductivity tensor** $\\sigma$, we have\n$$\n\\mathbf{J} = \\sigma \\mathbf{E}\n$$\nwhere $\\sigma$ is a $2$-tensor (a $3\\times 3$ matrix) which contains the resistance of the material in every pair of axes (parallel to each axis as well as in-between pairs of axes, as seen above with the stress tensor.)\n\nIf the material is indeed isotropic, then we have something much simpler. Isotropic materials lend themselves to isotropic tensors $\\sigma$; in three dimensions (and tensor rank $2$), the only possible choice is $\\delta_{ij}$, $i, j = 1, 2, 3$:\n$$\n\\sigma_{ij} = \\sigma \\delta_{ij}\n$$\nfor a constant $\\sigma$, indicating that the material has the same resistance in every axis and no resistance in-between the axes.\n\nIn two dimensions, though - think an infinitely thin electric wafer, like a potato chip or a really thin wire - we have something altogether a bit more interesting. \n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. In $\\mathbb{R^2}$, $\\delta_{ij}$ is not the only isotropic $2$-tensor; $\\epsilon_{ij}$ is also isotropic ($i, j = 1, 2$.)\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nBy the tensor transformation law:\n$$\n\\begin{aligned}\n\\epsilon_{ij}' &= \\epsilon_{kl}R_{ik}R_{jl} \\\\\n&= \\epsilon_{12}R_{i1}R_{j2} + \\epsilon_{21}R_{i2}R_{j1} \\\\\n&= R_{i1}R_{j2} - R_{i2}R_{j1} \\\\ \n\\end{aligned}\n$$\nI'm pretty sure that out of the million gazillion bajillion different ways you could prove this by, this is by far and away **the** stupidest one, but as long as it works:\n$$\n\\begin{cases}\ni = 1,\\ j = 2, \\epsilon_{ij} = 1, \\epsilon_{ij}' = R_{11}R_{22}-R_{12}R_{21} = \\det R = 1 \\\\\ni  = 2, j= 1, \\epsilon_{ij} = -1, \\epsilon_{ij}' = R_{12}R_{21} - R_{11}R_{22} = -\\det R = -1 \\\\\ni = j = 1, \\epsilon_{ij} = 0, \\epsilon_{ij}' = R_{11}R_{12} - R_{12}R_{11} = 0 \\\\\ni = j = 2, \\epsilon_{ij} = 0, \\epsilon_{ij}' = R_{21}R_{22} - R_{22}R_{21} = 0\n\n\\end{cases}\n$$\nwith $\\epsilon_{ij} = \\epsilon_{ij}'$ in all four cases.\n\n***\n\nAs such, isotropic $2$-tensors in $\\mathbb{R^2}$  can be any linear combination of the symmetric isotropic tensor $\\delta_{ij}$ and the antisymmetric isotropic tensor $\\epsilon_{ij}$; thus, the conductivity tensor $\\sigma$ is in the form\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Conductivity tensor for isotropic material in $\\mathbb{R^2}$.**\n$$\n\\sigma = \\sigma_{xx} \\delta_{ij} + \\sigma_{xy}\\epsilon_{ij} = \\begin{bmatrix}\n\\sigma_{xx} & \\sigma_{xy} \\\\\n-\\sigma_{xy} & \\sigma_{xx}\n\\end{bmatrix}\n$$\nfor constants $\\sigma_{xx}$ and $\\sigma_{xy}$ representing the *longitudinal conductivity* and the *Hall conductivity* respectively. The most interesting consequence of this is that, given an electric field vector $\\mathbf{E}$ that points only in the $x$-direction\n$$\n\\mathbf{E} = \\begin{bmatrix}\nx \\\\\n0\n\\end{bmatrix}\n$$\nthe current density in the two-dimensional object beecomes\n$$\n\\mathbf{J} = \\sigma \\mathbf{E} = \\begin{bmatrix}\n\\sigma_{xx} x \\\\\n-\\sigma_{xy} x\n\\end{bmatrix}\n$$\nimplying that there will also be an induced current in the $y$-direction as long as the Hall conductivity $\\sigma_{xy}$ is nonzero. \n\nWhat use did we have for tensors in all of this? Quite a lot, actually. Because we knew conductivity was a $2$-tensor; we immediately knew exactly what form it should take for an isotropic substance both in $\\mathbb{R^3}$ and in $\\mathbb{R^2}$; and because of that, we were able to infer that a Hall conductivity exists - the induction of an electric current in a direction perpendicular to the field through the *Hall effect*, something which is traditionally explained through electromagnetic induction and magnetic fields but was derived and manifested into existence purely through the properties of tensors.\n\n\n## The inertia tensor\n\nConsider a rigid body modeled by a collection of point masses $m_a$ at position $\\mathbf{x}_a$; suppose that the body is tied to some origin, or *pole*, and rotating about that origin with an angular velocity $\\omega$, defined as the rate of change of the angle of the object as it revolves. (Fun fact: these sentences describe what I have in my basement right now almost exactly.) \n\nSuppose that every point on the rigid body is moving with the same linear velocity $\\mathbf{v_a}$; in particular, linear velocity is given as the cross product of position and angular velocity, i.e.\n$$\n\\mathbf{v_a} = \\omega \\times \\mathbf{x_a}\n$$\nwhere, by convention, we define the angular velocity to be **pointing upwards**/**in the $z$-direction** if the object is rotating in the $xy$-plane, so as to 1) have angular velocity point towards an easily-pinned-down constant direction; and 2) given that $\\mathbf{v_a}$, the linear velocity, is provided by the dot product between radius and angular velocity, ensure that the linear velocity is constantly perpendicular to the radius (as is expected for circular motion) and that linear velocity, radius, and angular velocity form an orthonormal basis.\n\n![alt text](./assets/images/image-47.png)\n\nWe define the **total angular momentum** of the rigid body, a vector, as the sum of the angular momentums of all of its point masses, themselves defined as the cross product\n$$\n\\mathbf{L} = m_a(\\mathbf{x_a} \\times \\mathbf{v_a}) = m_a\\mathbf{x_a}\\times(\\mathbf{\\omega} \\times \\mathbf{x_a}) = m_a((\\mathbf{x_a \\cdot x_a})\\mathbf{\\omega} - (\\mathbf{x_a \\cdot \\omega})\\mathbf{x_a})\n$$\nby the vector triple product identity for point mass $a$. (We also have $\\mathbf{L} = \\mathbf{x_a} \\times (m_a\\mathbf{v_a})$, where the second term is recognizable as linear velocity; this is maximized when $\\mathbf{x}$ and $\\mathbf{v}$ are orthogonal, i.e. with circular motion.) The total angular momentum of the rigid body is thus given by\n$$\n\\mathbf{L} =\\sum_{a}m_a((\\mathbf{x_a \\cdot x_a})\\mathbf{\\omega} - (\\mathbf{x_a \\cdot \\omega})\\mathbf{x_a})\n$$\nfor a discrete collection of finite point charges. Summation notation reveals this to be equal to\n$$\n\\begin{aligned}\n\\mathbf{L}_i &=\\sum_{a}m_a((\\mathbf{x_a \\cdot x_a})\\mathbf{\\omega} - (\\mathbf{x_a \\cdot \\omega})\\mathbf{x_a})_i \\\\\n\n&= \\sum_{a}m_a(|\\mathbf{x_a}|^2\\mathbf{\\omega}_i - (\\mathbf{x_a})_j\\mathbf{\\omega}_j      (\\mathbf{x_a})_i) \\\\\n\n&= \\sum_{a}m_a(|\\mathbf{x_a}|^2\\delta_{ij}\\mathbf{\\omega}_j - (\\mathbf{x_a})_i     (\\mathbf{x_a})_j\\mathbf{\\omega}_j ) \\\\\n\n&= [\\sum_a m_a(|\\mathbf{x_a}|^2\\delta_{ij} - (\\mathbf{x_a})_i     (\\mathbf{x_a})_j)]\\omega_j \\\\\n\n&= I_{ij} \\omega_j\n\\end{aligned}\n$$\nwhere $I$ is a $3\\times 3$ matrix - indeed a tensor, because it is formed from a linear combination of tensors $\\delta_{ij}$ and $\\mathbf{x_a}$ - defined below:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **inertia tensor** $I$ is the $2$-tensor in $\\mathbb{R^3}$ with elements\n$$\nI_{ij} =\\sum_a m_a(|\\mathbf{x_a}|^2\\delta_{ij} - (\\mathbf{x_a})_i     (\\mathbf{x_a})_j)\n$$\n> for a rigid body containing point masses $m_a$ at position $\\mathbf{x_a}$.\n\nIf we have a continuous rigid body containing an infinite number of infinitesimal point masses, we instead have the following integral form for the inertia tensor:\n$$\nI_{ij} = \\int_V \\rho(\\mathbf{x})(|\\mathbf{x}|^2\\delta_{ij} - \\mathbf{x}_i \\mathbf{x}_j)\\ dV\n$$\nwith density $\\rho$ replacing discrete point mass $m_a$, continuous position $\\mathbf{x}$ replacing discrete position $\\mathbf{x_a}$, and $V$ being the region describing the rigid body.\n\nIn particular, we observe that the inertia tensor is symmetric; from [[Linear Algebra.Eigenvalues and Eigenvectors.Eigenvalues and Eigenvectors of Hermitian Matrices]], we know that the matrix $I$ representing the inertia vector is diagonalizable, yielding three mutually orthogonal principal axes equalling the eigenvectors of $I$. But what do these principal axes actually mean for the rigid body at hand? To understand this, we turn to a physical interpretation of the inertia tensor:\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The component $I_{ij}$ of the inertia tensor describes the **moment of inertia** of the rigid body, analogous to mass in linear scenarios: the greater $I_{ij}$ is, the more rotational \"force\" (**torque**) will be applied to the object about axis $j$ when a small angular acceleration or velocity is applied to it about axis $i$.\n\nThis can be demonstrated via the original equation relating angular momentum $\\mathbf{L}$, $I$ and $\\omega$:\n$$\nL_{x} = I_{xj}\\omega_{j}\\ (= I_{xx}\\omega_x + I_{xy}\\omega_y + I_{xz}\\omega_z)\n$$\ntaking the $x$-axis as an example in $\\mathbb{R^3}$. Just as the time-derivative of linear momentum is net linear force, the time-derivative of angular momentum about a certain axis is net angular force (torque) about the axis, denoted by $\\tau$; as such, we have\n$$\n\\tau_x = I_{xj}\\dot\\omega_j = I_{xj}\\mathbf{a}_{j}\n$$\nwhere $\\mathbf{a}$ is angular acceleration; this resembles Newton's Second Law $F= ma$. Thus the components of $I$ can be thought of as analogous to mass: $I_{ij}$ describes the amount of \"angular mass\", or moment of inertia, present along axis $j$ with respect to an  acceleration about axis $i$.\n\nAs such, if we can identify three eigenvectors of $I$ - the principal axes - then if we take these principal axes as our new coordinate system, $I$ will look like the diagonal matrix\n$$\nI = \\begin{bmatrix}\n\\lambda_1 & & \\\\\n& \\lambda_2 & \\\\\n& & \\lambda_3\n\n\\end{bmatrix}\n$$\nwith all components $I_{ij}$ with $i \\neq j$ equalling zero, meaning that all the \"mass\" of the object is concentrated purely along these axes - there are no mixed-axes inertias. But what kind of object does this describe? This leads us to the following example:\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Find the inertia tensor of a sphere of radius $R$ and constant density $\\rho_0$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>. \n\nWe utilize the integral form of the components $I_{ij}$ of the inertia tensor $I$:\n$$\nI_{ij} = \\int_V \\rho(\\mathbf{x})(|\\mathbf{x}|^2\\delta_{ij} - x_i x_j)\\ dV\n$$\nwith $V$ being a ball with radius $R$ centered about the origin and $\\rho$ being a constant $\\rho_0$. In particular, the latter term\n$$\nT_{ij} = -\\int_V \\rho(\\mathbf{x}) x_i x_j\\ dV\n$$\nis an invariant tensor (see example above); as such, by the Integral Invariance Theorem we have\n$$\nT_{ij} = \\alpha \\delta_{ij}\n$$\nfor some constant $\\alpha$; from the trace of $T_{ij}$ we also have\n$$\nT_{ii} = -\\int_V \\rho(\\mathbf{x}) x_i x_i\\ dV = -\\int_V \\rho(\\mathbf{x})|\\mathbf{x}|^2\\ dV = \\alpha\\delta_{ii}=3\\alpha\n$$\nand thus \n$$\n\\alpha = -\\frac{1}{3}\\int_V \\rho(\\mathbf{x})|\\mathbf{x}|^2\\ dV.\n$$\nIn total, we have\n$$\nI_{ij} = \\frac{2}{3}\\delta_{ij}\\int_V\\rho(\\mathbf{x})|\\mathbf{x}|^2\\ dV\n$$\nsumming everything together, and when $\\rho(\\mathbf{x})$ is a constant $\\rho_0$ we have\n$$\nI_{ij} = \\frac{2}{3}\\rho_0 \\delta_{ij}\\int_V |\\mathbf{x}|^2\\ dV\n$$\nwhich is easily evaluated via a change of variables to spherical coordinates.\n\n***\n\nFrom the above example, we make one important observation: the inertia tensor of a spherical object \n$$\nI_{ij} = \\frac{2}{3}\\rho_0 \\delta_{ij}\\int_V |\\mathbf{x}|^2\\ dV\n$$\nis a diagonal matrix, with $\\delta_{ij}$ being zero when $i\\neq j$; this tells us that a sphere has no **moment of inertia** along the mixed axes, meaning that there is no way you can give the sphere a small angular acceleration along one axis and have it spin on another axis - conforming to our expectations of reality. In general, we expect a rigid body that has **mass symmetry** about a certain axis, like a cylinder or a sphere, to have no mixed moments of inertia about that axis; if you spin it about that axis, it'll spin about that axis and that axis only instead of wobbling about wildly and toppling over like a freshly-castrated mule after overdosing on an entire barrel of anesthetics mixed with beer from the tap. \n\nAs such, what do the principal axes of a rigid body in 3D indicate? The axes of symmetry where the rigid body can be considered balanced, and a rotation about which produces no mixed effects upon the body on other axes.\n\n> <span style=\"background-color: #1eff12; color: black;\">**Quest complete**</span>!","n":0.019}}},{"i":6,"$":{"0":{"v":"Tensors are Multilinear Maps","n":0.5},"1":{"v":"## What are multilinear maps?\n\nSometimes you flip to a new chapter in your math textbook, stare at it, get angry, and then when the overwhelming tidal wave of rage and devastation finally recedes from your body all that's left is for you to wonder \"why?\" Why did people look at this:\n$$\n\\int\n$$\nand think to themselves, \"you know what I want? Two $\\int$s. **THREE** $\\int$s. Throw a $\\oint$ hula-hoop around it for good measure.\" \n\nThese people are exactly the type of people who look at linear maps, and go, \"I bet I can make this even more torturous and deranged.\" Thanks for nothing, Ricci-Curbastro and Levi-Civita; may your flaming purgatory in the bowels of hell last as long as both of your names combined.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **multilinear map** $f$ is an extension of linear maps to a domain of multiple vectors at once: it maps $n$ vectors $\\mathbf{v_1, ..., v_n}$, each belonging to (potentially distinct) vector spaces $V_1, V_2, ..., V_n$, to a vector $\\mathbf{w}$ in vector space $W$. Denote this as $f(\\mathbf{v_1, ..., v_n}) = \\mathbf{w}, f: V_1 \\times V_2 \\times ... \\times V_n \\to W$.\n\nThe \"multi\" part of \"multilinear\" comes from the fact that $f$ has linearity in each of the $n$ variables it accepts as its input. A regular linear map $A(\\mathbf{x})$ is linear because it satisfies the following condition:\n$$\nA(\\lambda \\mathbf{x} + \\mu \\mathbf{y}) = \\lambda A(\\mathbf{x}) + \\mu A(\\mathbf{y})\n$$\nAnalogously, for a multilinear map this linearity condition holds for each of $\\mathbf{v_1}, ..., \\mathbf{v_n}$:\n$$\n\\begin{aligned}\nf(\\mathbf{v_1, ..., \\lambda v_i + \\mu u_i, ..., v_n}) \\\\\n= \\lambda f(\\mathbf{v_1, ..., v_i, ... v_n}) + \\mu f(\\mathbf{v_1, ..., u_i, ..., v_n})\n\\end{aligned}\n$$\nfor **every one of** $i = 1, ..., n$. For instance, a **bi-linear map** $f: U \\times V \\to W$ for vector spaces $U, V, W$ has the property that, for four vectors $\\mathbf{a, b, c, d}$,\n$$\n\\begin{aligned}\nf(\\alpha \\mathbf{a} + \\beta \\mathbf{b}, \\gamma \\mathbf{c} + \\delta \\mathbf{d}) &= \\alpha f(\\mathbf{a}, \\gamma \\mathbf{c} + \\delta \\mathbf{d}) + \\beta f(\\mathbf{b}, \\gamma \\mathbf{c} + \\delta \\mathbf{d}) \\\\\n(&= \\alpha\\gamma(f(\\mathbf{a}, \\mathbf{c}) + ...))\n\\end{aligned}\n$$\n\n\n\nThis gives us the tools to construct a **coordinate-independent** definition of tensors, based on multilinear maps. Let's start small with a familiar example - a rank $2$ tensor, more affectionately known as a matrix (representation of a linear map) - and extrapolate from there. We know that such \"tensors\" - matrices - are linear maps which take a single vector $\\mathbf{x}$ and map it to another vector $\\mathbf{x}'$:\n$$\n\\mathbf{x}'_i = A_{ij}\\mathbf{x}_j \\text{ (summation notation)}\n$$  \nThis suggests to us the generalization\n$$\n\\mathbf{x}'_i = T_{i i_1 ... i_{p-1}}\\mathbf{x}^1_{i_1} \\mathbf{x}^2_{i_2} ... \\mathbf{x}^{p-1}_{i_{p-1}}\n$$\nfor a tensor of rank $p$, taking $n$ vectors $\\mathbf{x}^1, \\mathbf{x}^2, ..., \\mathbf{x}^n$ to return a single vector $\\mathbf{x}'$. Alternatively, multiplying both sides by $\\mathbf{x_i}'$\n$$\n(\\mathbf{x}'_i)^2 = T_{i i_1 ... i_{p-1}}\\mathbf{x}'_i\\mathbf{x}^1_{i_1} \\mathbf{x}^2_{i_2} ... \\mathbf{x}^{p-1}_{i_{p-1}}\n$$\nresults in a map from $p$ vectors to a real number.\n\n \n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A tensor $T_{i_1 i_2 ... i_p}$ of rank $p$ can be defined as a multilinear map $T: V_1 \\times V_2 \\times ... \\times V_n \\to V$ on $n$ vectors, denoted $\\mathbf{x}^1, \\mathbf{x}^2, ..., \\mathbf{x}^n$, which outputs a vector $\\mathbf{x}'$ with components\n\n$$\n\\mathbf{x}'_i = T_{i i_1 ... i_{p-1}}\\mathbf{x}^1_{i_1} \\mathbf{x}^2_{i_2} ... \\mathbf{x}^{p-1}_{i_{p-1}}. \\text{ (summation over each index)}\n$$\n\n> or, equivalently,\n$$\nx = T_{i_1 i_2 ... i_{p}}\\mathbf{x}^1_{i_1} \\mathbf{x}^2_{i_2} ... \\mathbf{x}^{p}_{i_{p}}\n$$\n> which maps $p$ vectors $\\mathbf{x_1, ..., x_p}$ to a real number $x$.\n\n\nWe note that the $n+1$ vector spaces $V_1, ... V_n$ and finally $V$ **need not be** the same vector space; as such, just as linear maps can spit out a vector from $\\mathbb{R^m}$ when you throw in a vector from $\\mathbb{R^n}$ if it has dimensions $m \\times n$, tensors can also have non-symmetrical dimensions. We focus on \"square\" (or cube, or hypercube, etc.; I don't exactly know the fifth-dimensional lingo the Generation $\\Omega$ hyperkids are using today) tensors for now.\n\nIn order to show that this is indeed equivalent to the previous definition of a tensor we have given, we proceed towards showing that 1) multilinear maps are independent of basis, and 2) multilinear maps transform like tensors. This necessitates the introduction of some new (and improved) tensor operators, so buckle up!\n\n## Tensor operations\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. Preservation of the tensor transformation law.\n\nThere is one commonality that unites all the following tensor operations: ~~they all suck~~ given two tensors $S$ and $T$, they will all output another tensor, be it $S + T$ (addition), $S \\otimes T$ (~~lamps in an electric circuit~~ tensor multiplication), or anything of the like - meaning that it preserves the tensor transformation law.\n\n### Linear operations on tensors\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Linear operations on two tensors $S$ and $T$ - i.e. addition, subtraction, and scalar multiplication by a scalar $\\alpha$ - are defined as long as $S$ and $T$ are of the same rank to be carried out component-by-component, i.e.\n$$\n(\\alpha S + \\beta T)_{i_1 i_2 ... i_p} = \\alpha S_{i_1 ... i_p} + \\beta T_{i_1 ... i_p}.\n$$\n\nIt's not difficult to show that this is still a tensor: under an orthogonal transformation matrix $R$, we have\n$$\n\\begin{aligned}\n(\\alpha S + \\beta T)'_{i_1 ... i_p} &= \\alpha S'_{i_1 ... i_p} + \\beta T'_{i_1 ... i_p} \\\\\n&= \\alpha S_{j_1 ... j_p} R_{i_1 j_1} R_{ i_2 j_2} ... R_{i_p j_p} + \\beta T_{j_1 ... j_p} R_{i_1 j_1 } R_{i_2 j_2} ... R_{i_p j_p} \\\\\n&= (\\alpha S + \\beta T)_{j_1 ... j_p}R_{i_1 j_1 } R_{i_2 j_2} ... R_{i_p j_p} \\\\\n\\end{aligned}\n$$\nwhich is indeed how a tensor is transformed by definition.\n\n### Tensor multiplication\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Given two tensors $S$ and $T$ of ranks $p$ and $q$ respectively, define the **tensor product** $S \\otimes T$ outputting a tensor of rank $p + q$ on a component-by-component basis as\n$$\n(S\\otimes T)_{i_1 i_2 ... i_p j_1 ... j_q} = S_{i_1 i_2 ... i_p} T_{j_1 ... j_q}\n$$\n\nWe make the important clarification that this **is** defined for tensors which aren't of the same \"dimensions\" (e.g. a $2\\times 2$ rank-$2$ tensor and a $1\\times 3$ rank-$1$ tensor).\n\nThis is **not** matrix multiplication, or the dot product, or anything we've seen before as we know it; it's something entirely new, defined between **any** two tensors. When we're just dealing with vectors and matrices, it's actually not all that hard to visualize. Consider the tensor product between\n$$\n\\ S = \\begin{bmatrix}\n4 \\\\\n2 \\\\\n0\n\\end{bmatrix},\\ T = \\begin{bmatrix}\n6 & 9 & 4 \\\\\n2 & 0 & 6 \\\\\n9 & 4 & 2\n\\end{bmatrix}\n$$\nwhere \n$$\n(S\\otimes T)_{1 i j}= S_1 T_{ij} = 4T_{ij}\n$$\nand as such $(S\\otimes T)_{1ij}$ is the matrix $4T$, $(S\\otimes T)_{2ij}$ is $2T$, and $(S\\otimes T)_{3ij}$ is $0T$. The tensor product can thus be thought of as taking a tensor $T$ (in this case, a matrix), layering $3$ copies of it into a new dimension, then multiplying each copy by $4$, $2$ and $0$ (or with the elements of another vector). \n\nAs such, for a vector ($1$-tensor) with $3$ rows and a matrix ($2$-tensor) with dimensions $3\\times 3$, the tensor product results in a $3\\times 3 \\times 3$ $3$-tensor; the dimensions add on top of each other, as does the rank. \n\nIn addition, we provide a \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> that the tensor product obeys the tensor transformation law.\n\nUnder a transformation matrix $R$:\n$$\n\\begin{aligned}\n(S\\otimes T)'_{i_1 i_2 ... i_p j_1 ... j_q} &= (S_{i_1 i_2 ... i_p})' (T_{j_1 j_2 ... j_q})' \\\\\n&= S_{k_1 k_2 ... k_p} R_{i_1 k_1} R_{i_2 k_2} ... R_{i_p k_p} T_{l_1 l_2 ... l_q} R_{j_1 l_1} R_{j_2 l_2} ... R_{j_q l_q}\\\\ \n&= S_{k_1... k_p}T_{l_1...l_q} R_{i_1 k_1} ... R_{i_p k_p} R_{j_1 l_1} ... R_{j_q l_q} \\\\\n&= (S\\otimes T)_{k_1 k_2 ... k_p l_1 ... l_q}R_{i_1 k_1} ... R_{i_p k_p} R_{j_1 l_1} ... R_{j_q l_q}\n\\end{aligned}\n\n$$\nwhich obeys the tensor transformation law.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>: for $n$ vectors $\\mathbf{x_1, x_2, ..., x_n}$, an $n$-tensor can be constructed from the tensor product $\\mathbf{x_1 \\otimes x_2 \\otimes ... \\otimes x_n}$, defined through\n$$\n(\\mathbf{x_1 \\otimes x_2 \\otimes ... \\otimes x_n})_{i_1 ... i_n} = (\\mathbf{x_1}_{i_1}) ... (\\mathbf{x_n})_{i_n}.\n$$\n\n### Tensor contraction\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Let $T$ be the $n$-tensor with components denoted $T_{i j p... q}$. Define a **contraction** over the pair of indices $i$, $j$ (which can be chosen arbitrarily as any pair of indices) as the tensor $S$ with components\n$$\nS_{p ... q} = \\delta_{ij} T_{ij p ... q}\\text{ (denoting summation over $i$ and $j$!)}\n$$\nFully written out, this is \n$$\nS_{p...q} = T_{11 p... q} + T_{22 p... q} + ... + T_{nn p... q}\n$$\nwhich eagle-eyed readers will notice is a generalization of the trace of a matrix, now understood as a contraction of a $2$-tensor:\n$$\nS = T_{11} + ... + T_{nn}.\n$$\nThis is, as always, still a valid tensor due to it being a sum of tensors.\n\n### Symmetric and anti-symmetric matrices\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A $n$-tensor $T_{ijp ...q}$ is **symmetric** about a pair of indices $i$ and $j$, again arbitrarily chosen, if $T_{ijp...q} = T_{jip...q}$ (swapping the indices); call $T$ **anti-symmetric** about the same pair of indices if instead $T_{ijp...q} = -T_{jip...q}$. \n\nIf a tensor is symmetric (or anti-symmetric) in **all** possible pairs of indices, call it **totally** symmetric (or anti-symmetric).\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. In $\\mathbb{R^3}$, there are no nonzero rank-$4$ or higher tensors that are totally anti-symmetric. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nSuppose for the sake of contradiction that $T_{i_1 i_2 ... i_p}$ is an anti-symmetric rank-$p$ tensor in $\\mathbb{R^3}$, with $p > 3$ and $i_j = 1, 2, 3$ for all $j = 1, 2, ..., p$. By the Pigeonhole Principle, we have $3$ numbers ($1, 2, 3$) and $p$ boxes to fit them into with $p > 3$; sooner or later there will be a repeated index. \n\nWithout loss of generality, let $i_j = i_k$. Thus\n$$\nT_{i_j i_k ... i_p} = T_{i_k i_j ... i_p}\n$$\nby $i_j = i_k$, and\n$$\nT_{i_j i_k ... i_p} = -T_{i_k i_j ... i_p}\n$$\nby antisymmetry, leading to \n$$\nT_{i_j i_k... i_p} = 0.\n$$\nThis is valid for **any** component of $T$, as by the Pigeonhole Principle (stated above) all components of $T$ will have at least one repeated index. Thus, all totally anti-symmetric tensors of rank $4$ or higher in $\\mathbb{R^3}$ are zero tensors. \n\n> Note that this proof can be extended naturally to rank-$p$ or higher tensors in $\\mathbb{R^n}$ with $p > n$.\n\n***\n\nArmed with all of this, we return to\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Tensors are multilinear maps: every multilinear map taking $n$ vectors in $\\mathbb{R^3}$ (or some other space) and outputting a real number can be represented by a tensor, and every tensor corresponds to a multilinear map which is invariant under change of basis (by definition of multilinear maps).\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nAs multilinear maps from the combined space of $n$ vectors to the reals are simply linear combinations of the components of these $n$ vectors, say $\\mathbf{x^1, ..., x^n}$, any multilinear map $f$ of this form can be expressed as \n$$\nf(\\mathbf{x^1, ..., x^n}) = \\sum_{i_1}\\sum_{i_2}...\\sum_{i_n}T_{i_1 ... i_n}\\mathbf{x^1}_{i_1} ... \\mathbf{x^n}_{i_n}\n$$\nwhich is the most general form, because it is an arbitrary linear combination of every possible product of the components of these $n$ vectors. Written in summation notation, this is identical to the definition of tensors as multilinear maps provided above.\n\nThe object $T_{i_1 ... i_n}$ transforms like a tensor because the entire multilinear map is independent of basis, by definition:\n\n$$\nT_{i_1 ... i_n}\\mathbf{x^1}_{i_1} ... \\mathbf{x^n}_{i_n} = (T_{i_1 ... i_n}\\mathbf{x^1}_{i_1} ... \\mathbf{x^n}_{i_n})'\n$$\nin an alternative basis marked by transformation matrix $R$, leading to\n$$\n\\begin{aligned}\nT_{i_1 ... i_n}\\mathbf{x^1}_{i_1} ... \\mathbf{x^n}_{i_n} &= (T_{i_1 ... i_n}\\mathbf{x^1}_{i_1} ... \\mathbf{x^n}_{i_n})' \\\\\nT_{i_1 ... i_n}\\mathbf{x^1}_{i_1} ... \\mathbf{x^n}_{i_n} &= T_{j_1 ... j_n}'\\mathbf{x^1}_{j_1}' ... \\mathbf{x^n}_{j_n}' \\\\\n\\end{aligned}\n$$\nswitching indices from $i$ to $j$ on the right-hand side.\nWe know that $R_{j_k i_k}\\mathbf{x}^k_{i_k} = (\\mathbf{x}^k_{j_k})'$ by tensor transformation laws, so multiplying by $R_{j_1 i_1} R_{j_2 i_2} ... R_{j_n i_n}$ on the left-hand side gives\n$$\n\\begin{aligned}\nT_{i_1 ... i_n}(R_{j_1 i_1}\\mathbf{x^1}_{i_1}) ... (R_{j_n i_n}\\mathbf{x^n}_{i_n}) &= T_{j_1 ... j_n}'\\mathbf{x^1}_{j_1}' ... \\mathbf{x^n}_{j_n}' (R_{j_1 i_1} R_{j_2 i_2} ... R_{j_n i_n})\\\\\nT_{i_1 ... i_n}(\\mathbf{x}^1_{j_1})' ... (\\mathbf{x}^n_{j_n}) &= T_{j_1 ... j_n}'(\\mathbf{x^1}_{j_1}') ... (\\mathbf{x^n}_{j_n}') (R_{j_1 i_1} R_{j_2 i_2} ... R_{j_n i_n})\n\\end{aligned}\n$$\nEliminating the $\\mathbf{x}'$ terms from both sides gives\n$$\nT_{i_1 ... i_n} = T_{j_1 ... j_n}'(R_{j_1 i_1} R_{j_2 i_2} ... R_{j_n i_n})\n$$\nwhich obeys the tensor transformation law.\n\n\n## Contracting products and the quotient rule\n\nLet's finish with two last comments on tensor products. \n\n### Contracting a product\n\nGiven two tensors $S$ and $T$ of ranks $p$ and $q$ respectively, it's occasionally useful to apply a contraction to the tensor product $S \\otimes T$, i.e. $(S \\otimes T)_{\\text{contracted}}$ has components\n$$\n\\delta_{ij}S_{ik_1 ... k_{p-1}}T_{jl_1 ... l_{q-1}} = S_{ik_1 ...k_{p-1}}T_{i l_1 ...l_{q-1}}\n$$\nwhich, when both tensors are vectors, wears a familiar guise:\n$$\n(S \\otimes T)_{\\text{contracted}} = S_{i} T_i = S \\cdot T\n$$\nresulting in a $0$-tensor, otherwise known as a number.\n\nTensor contraction is a little bit like tensor division. Consider the tensor product between $S$ of rank $(m+n)$ and $T$ of rank $n$, with components\n$$\nS_{i_1 i_2 ... i_m j_1 j_2 ... j_n}T_{j_1 j_2 ... j_n}\n$$\nwhich has $n$ free indices $i_1, ..., i_m$ and is summed over $j_1, ..., j_n$; this results in a tensor of rank $m+n - n = m$, still obeying the tensor transformation law. This leads naturally to\n\n### Tensor quotients\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Tensor quotient rule.** Suppose that for an indexed array (just a list of numbers; not necessarily a tensor) $T$ with $n+m$ free indices, and **all** tensors $S$ of rank $m$, the product contraction between $S$ and $T$ satisfies\n$$\nT_{i_1 i_2 ... i_n j_1 ... j_m} S_{j_1 ... j_m} = V_{i_1 i_2 ... i_n}\n\n$$\n> where $V$ is a tensor of rank $n$; then $T$ is a tensor. If $n = m = 1$, this says that $T$ (a rank-$2$ array, or a matrix/a linear map) mapping a $1$-tensor to another $1$-tensor (two vectors) is a tensor.\n \n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nBegin by decomposing $S$ into a sum of tensor products of vectors in the form $a_{j_1}...b_{j_m}$. (Such sums can always be found for every tensor because the space of tensors of rank $p$ is equivalent to the product of $p$ vector spaces, all of which are spanned by some basis.) As tensors are additive, we only need to consider the singular term $a_{j_1}...b_{j_m}$ and thus the equation\n$$\nT_{i_1 i_2 ... i_n j_1 ... j_m}a_{j_1}...b_{j_m} = V_{i_1 i_2 ... i_n}.\n$$\nMultiplying both sides by another $n$ tensors $c_{i_1} ... d_{i_n}$ gives\n$$\n(c_{i_1} ... d_{i_n})T_{i_1 i_2 ... i_n j_1 ... j_m}a_{j_1}...b_{j_m} =  (c_{i_1} ... d_{i_n})V_{i_1 i_2 ... i_n}\n$$\nwhere the right-hand side is a scalar. By definition, $T$ is thus a multi-linear map as it maps $n+m$ vectors $a, ... b, c, ..., d$ to a scalar; as such, it is a tensor. $\\square$","n":0.021}}},{"i":7,"$":{"0":{"v":"Tensors are Tensor Fields","n":0.5},"1":{"v":"\n## Invariant and isotropic tensors\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Call a tensor $T$ invariant under a given rotation $R$ if all of its components remain identical after transformation by $R$:\n\n$$\nT'_{i_1 i_2 ... i_p} = T_{j_1 j_2 ... j_p}R_{i_1 j_1} R_{i_2 j_2} ... R_{i_p j_p} = T_{i_1 i_2 ... i_p}.\n$$\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. If a tensor $T$ is invariant under **any** rotation $R$, it is called **isotropic**.\n\nAll scalars are isotropic; no vector is isotropic because they have a fixed direction, and thus will always transform under rotation. In particular, we claim that\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. There are only two nonzero isotropic rank $p = 1, 2,$ or $3$ tensors: a constant multiple of the totally symmetric rank $2$ Kronecker delta $\\delta_{ij}$, and a constant multiple of the totally antisymmetric rank $3$ Levi-Civita symbol. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nFirst we prove that both tensors are invariant. For the Kronecker delta, this follows from\n$$\n\\delta'_{ij} = \\delta_{mn} R_{im}R_{jn} =  R_{in}(R^T)_{nj} = \\delta_{ij}\n$$\nas $RR^T = I$; for the Levi-Civita symbol this follows from\n$$\n\\epsilon_{ijk}' = \\epsilon_{lmn}R_{il}R_{jm}R_{kn} = \\det R\\ \\epsilon_{ijk} = \\epsilon_{ijk}\n$$\nby definition of the determinant; note that if $R$ is a reflection instead of a rotation $\\epsilon$ is no longer invariant, as $\\det R = -1$.\n\nTo prove that the above two tensors are the only two possible invariant tensors, we consider ranks 1, 2 and 3 separately.\n\nFor rank $1$ tensors, consider the matrix $R$ of a rotation by $\\pi$ counterclockwise about the $z$-axis:\n$$\nR = \\begin{bmatrix}\n-1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\nSuppose that $T_i$ is a $1$-tensor. For it to be isotropic, we must have\n$$\nT_i' = R_{ij}T_j = T_i\n$$\nLooking through the rows, this suggests\n$$\n\\begin{cases}\n-T_1 = T_1 \\\\\n-T_2 = T_2 \\\\\nT_3 = T_3\n\\end{cases}\n$$\nwhich gives $T_1 = T_2 = 0$. If $-R$ is used instead of $R$, a similar argument is obtained for $T_3 = 0$; thus, any isotropic tensor of rank $1$ is a zero tensor.\n\nFor rank $2$ tensors, considering\n$$\nR = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n-1 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\nyields\n$$\nR_{ik}R_{jl}T_{kl} = T_{ij}\n$$\nwhich across the individual rows yields\n$$\nT_{13}  = R_{1k}R_{3l}T_{kl} = R_{3l}T_{2l} = T_{23} \n$$\nand\n$$\nT_{23} = R_{2k}R_{3l}T_{kl} = -R_{3l}T_{1l} = -T_{13}\n$$\nimplying $T_{13} = T_{23} = 0$; the same argument can be made for all non-diagonal elements, which all vanish to $0$.\n\nFor diagonal elements, e.g. $T_{11}$, we havee\n$$\nT_{11} = R_{1k}R_{1l}T_{kl} = R_{1l}T_{2l} = T_{22}\n$$\nwith similar arguments showing that all diagonal elements are equal.\n\nThus, all isotropic rank $2$ tensors have equal diagonal elements and non-diagonal elements equalling zero; this is in the form $\\alpha \\delta_{ij}$ for a constant $\\alpha$.\n\nFor rank $3$ tensors, considering both the above transformation matrices yields that $T_{ijk} = 0$ if any two indices are equal and $T_{ijk} = -T_{jik}$. This is in the form $\\beta \\epsilon_{ijk}$. $\\ \\square$\n\n### Rank $2$ tensor decomposition\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Any rank $2$ tensor (which is just a fancy name for a matrix) can be decomposed into a **traceless** rank $2$ tensor and two isotropic tensors $\\alpha \\delta_{ij}$ and $\\epsilon_{ijk} B_{k}$ for another tensor $B$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nBegin with the symmetric/anti-symmetric decomposition of a rank $2$ tensor $T$. For any $T$, we have\n$$\nT_{ij} = \\frac{T_{ji} + T_{ij}}{2} + \\frac{T_{ij} - T{ji}}{2}\n$$\nof which the first term is symmetric and the second is anti-symmetric; denote the first term $S_{ij}$ and the second $A_{ij}$. If the tensor $S$ has trace $Q$, then the tensor $S - \\frac{Q}{3}\\delta_{ij}$ has trace zero (\"traceless\") due to the additive properties of trace; thus we have\n$$\nS_{ij} = P_{ij} + \\frac{Q}{3}\\delta_{ij}\n$$\nfurther decomposing $S$ into a *traceless* tensor $P$ and an isotropic tensor $\\delta_{ij}$. Similarly, we can decompose any antisymmetric tensor $A_{ij}$ of rank $2$ into the product contraction\n$$\nA_{ij} = \\epsilon_{ijk}B_{k}\n$$\nfor some rank $1$ tensor $B$, as $A_{ji} = \\epsilon_{jik}B_{k} = - A_{ij}$. In total, this gives us\n$$\nT_{ij} = P_{ij} + \\frac{Q}{3}\\delta_{ij} + \\epsilon_{ijk}B_k.\n$$\nIn particular, $B_{k}$ is $\\frac{1}{2} \\epsilon_{ijk}A_{ij}$.\n\n\n## Tensor fields and their derivatives\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **tensor field** over $\\mathbb{R^3}$ assigns a tensor $T_{i_1 ... i_k}$ to every point $\\mathbf{x} \\in \\mathbb{R^3}$; such a tensor field can be written $T(\\mathbf{x}): \\mathbb{R^3} \\to \\mathbb{R^m}$, with $m$ the **number of components** (not the rank) of the tensor.\n\nDefining a (partial) derivative of a tensor field takes a bit of work. One possible interpretation is a component-by-component derivative: differentiating a tensor field just leaves us with the tensor containing the derivatives of all its components, much like differentiating a vector field. But with the tensor product, we can do something better.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the $N$th **tensor-valued** derivative of a rank-$m$ tensor field $T$ as the rank-$m+n$ tensor $X$ with components\n\n$$\nX_{i_1 ... i_n j_1 ... j_m} = \\frac{\\partial}{\\partial x_{i_1}} ...\\ \\frac{\\partial}{\\partial x_{i_n}} T_{j_1 ... j_m}\n$$\nwhere $x_{i_1}, ..., x_{i_n}$ are arbitrarily chosen variables in the domain of the tensor field.\n\nNote that this is defined on a **component-by-component** basis: the components of this new tensor field are every possible $n$th-derivatives of the original tensor component. \n\nWe note, very importantly, that this is a tensor product - specifically, a tensor product between the original tensor and $n$ different instances of the differential operator $\\nabla$, which makes a repeat appearance:\n$$\nX = \\nabla \\otimes \\nabla \\otimes ... \\otimes \\nabla \\otimes T\n$$\n\n In order to prove that it indeed outputs a tensor, we need $\\nabla$ itself to be a tensor; why is this so?\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The differential operator $\\nabla$ is a tensor; it is invariant with respect to changes of basis.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n$\\nabla$ is the vector operator given by\n$$\n\\begin{bmatrix}\n\\frac{\\partial}{\\partial x_1} \\\\\n\\frac{\\partial}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial x_n}\n\\end{bmatrix}\n$$\nawaiting a function to differentiate; suppose that we change basis via a matrix $R$ to an alternate set of variables $x_1', x_2', ..., x_n'$ such that, by the tensor transformation law,\n$$\nx_i' = R_{ij}x_j = R_{i1}x_1 + ... + R_{in}x_n\n$$\nleading to\n$$\n\\frac{\\partial x_i'}{\\partial x_p} = 0 + ... + R_{ip} + ... + 0 = R_{ip},\n$$\ntreating $x_j$ as a variable to be differentiated.  Similarly, we also have\n$$\nx_i = R_{ji}x_j' = R_{1i}x_1' + ... + R_{ni}x_n'\n$$\nleading to\n$$\n\\frac{\\partial x_i}{\\partial x_q'} = R_{qi},\\ \\frac{\\partial x_q}{\\partial x_i'} = R_{iq}.\n$$\nThus by the chain rule,\n$$\n(\\nabla')_i = (\\frac{\\partial}{\\partial x_i})' = \\frac{\\partial}{\\partial x_i'} =\\frac{\\partial}{\\partial x_q} \\frac{\\partial x_q}{\\partial x_i'} = \\frac{\\partial}{\\partial x_q} R_{iq} = \\nabla_q R_{iq}\n$$\nwhich follows the tensor transformation law. Thus $\\nabla$ is a tensor.\n\n***\n\nAs $\\nabla$ is a tensor, any rank-$2$ tensor product involving $\\nabla$ can be subject to our previous antisymmetric, symmetric and traceless tensor decomposition. For instance, we may have\n$$\nT = \\mathbf{F} \\otimes \\nabla,\\ T_{ij} = \\frac{\\partial}{\\partial x_j} F_i\n$$\nfor a vector field $\\mathbf{F}$; it follows that this rank-$2$ tensor can be decomposed into the isotropic component\n$$\n\\frac{\\text{Tr }T}{3}\\delta_{ij} = \\frac{\\partial_k F_k}{3}\\delta_{ij} = (\\frac{\\nabla \\cdot \\mathbf{F}}{3}) \\delta_{ij}\n$$\nthe traceless, symmetric component\n$$\nS_{ij} - \\frac{1}{3}\\nabla \\cdot \\mathbf{F}\n$$\nand the antisymmetric component\n$$\n\\epsilon_{ijk}B_k = \\epsilon_{ijk}(\\frac{1}{2}\\epsilon_{ijk}\\nabla_i F_j) = -\\frac{1}{2} \\epsilon_{ijk} (\\nabla \\times \\mathbf{F}).\n$$\n\n## Tensor integral theorems\n\n### Invariant integrals\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **The invariance theorem.** Suppose that $T$ is a tensor defined by the integral\n$$\nT_{ij...k} = \\int_V f(r)x_i x_j ... x_k\\ dV\n$$\n> for some scalar function $f$, position vector $\\mathbf{x}$, and **rotationally invariant** region $V$ (e.g. a sphere), where $r$ is the spherical radius $|\\mathbf{x}|$; then $T$ is isotropic. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nFirst, we prove that $T$ is indeed a tensor.\n$T$ transforms as follows:\n$$\n\\begin{aligned}\nT'_{ij...k} = \\int_V f(r)' x_i' x_j' ... x_k'\\ dV'\n\\end{aligned}\n$$\nof which $f(r)'$ remains $f(r)$ as it is a scalar (and also because radius is rotationally symmetric), the $x_i$s transform like tensors because they are represented by position vectors along the coordinate axes - i.e. $x_i' = R_{il}x_l$ for some rotation matrix $R$ - and $dV$ does not transform, as a rotation does not change the magnitude of the volume component ($\\det R = 1$.) Substituting gives\n$$\nT'_{ij...k} = \\int_V f(r)R_{i'i}x_i R_{j'j}x_j ... R_{k'k}x_k\\ dV = (R_{i'i}R_{j'j}...R_{k'k})T_{ij...k}\n$$\nwhich obeys the tensor transformation law.\n\nAs such, we have\n$$\nT'_{ij...k} = \\int_V f(r)x_i' x_j' ... x_k'\\ dV = T_{ij...k}\n$$\nbecause the integrand is a change of coordinates from $\\mathbf{x}$ to $\\mathbf{x}'$, and does not affect the value of the integral itself (in particular, the Jacobian is $\\det R = 1$.). In other words, if the integrand is a tensor, it should transform like a tensor - and that means that the integrand is invariant in actual value (though not necessarily in representation) under a rotation.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Find the value of a spherically symmetric integral over a single axis $x_i$ over a sphere $V$, given by the tensor\n$$\nT_i = \\int_V \\rho(r) x_i\\ dV.\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>. As $V$ is rotationally invariant, $T_i$ is an isotropic tensor; but all isotropic tensors of rank $1$ are zero tensors. Thus the integral is zero.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Find the value of a spherically symmetric integral over two axes $x_i$ and $x_j$ over a sphere $V$, given by the $2$-tensor\n$$\nT_{ij} = \\int_V \\rho(r) x_i x_j\\ dV.\n$$\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>. By the above results, $T_{ij}$ must be an isotropic tensor of rank $2$; the only tensor that fulfills these properties is a constant multiple of the Kronecker delta $\\delta_{ij}$. Thus we have\n$$\nT_{ij} = \\alpha \\delta_{ij}\n$$\n> for some $\\alpha \\in \\mathbb{R}$; as such, the trace of this tensor $T_{ii}$ is given by\n$$\nT_{ii} = \\alpha\\delta_{ii} = 3\\alpha = \\int_V\\rho(r) x_i x_i\\ dV = \\int_V \\rho(r)r^2\\ dV\n$$\n> where the right-hand integral evaluates to $4\\pi \\int_0^R \\rho(r) r^4\\ dr$. This leads us to\n$$\n\\alpha = \\frac{4\\pi}{3}\\int_0^R \\rho(r) r^4\\ dr\n$$\n> and thus\n$$\nT_{ij} = (\\frac{4\\pi}{3}\\int_0^R \\rho(r) r^4\\ dr)\\ \\delta_{ij}.\n$$\n\n### Tensor divergence theorem\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Tensor divergence theorem**. Let $T_{ij...k...l}(\\mathbf{x})$ be a tensor field; define the **divergence of $T$** with respect to some index $k$ to be the sum\n$$\n\\nabla_k \\cdot T_{ij...k...l}(\\mathbf{x}) = \\frac{\\partial}{\\partial x_k}T_{ij...k...l}(\\mathbf{x})\n$$\n> taking $k$ as the index of summation and all others as fixed. Then for a closed region $V$ in $\\mathbb{R^3}$ and its boundary $S$, we have\n$$\n\\int_V \\nabla_k \\cdot T_{ij...k...l}\\ dV = \\int_S T_{ij...k...l}\\ n_k\\ dS\n$$\n> where $\\mathbf{n}$ is the normal vector to boundary $S$. (Note that the right-hand side is summed over $k$; it is in some respects analogous to $\\mathbf{F \\cdot n} = F_k n_k$.)\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThis is a consequence of the (regular) Divergence Theorem. Consider a vector field $\\mathbf{v}$ formed from a contraction of the tensor $T$ with $n-1$ other constant vectors, i.e.\n$$\n\\mathbf{v}_k = T_{ij...k...l} \\mathbf{a}_i \\mathbf{b}_j ... \\mathbf{c}_l\n$$\nwith $\\mathbf{a, b, ..., c}$ being constant vectors. Applying the Divergence Theorem on $\\mathbf{v_k}$ gives\n$$\n\\begin{aligned}\n\\int_V \\nabla \\cdot v_k\\ dV &= \\int_S v_k\\cdot \\mathbf{dS} = \\int_S v_k n_k\\ dS \\text{ (divergence theorem)}  \\\\\n&= \\mathbf{a_i b_j ... c_l}\\int_V T_{ij...k...l}\\ n_k\\ dS \\\\ \n\\end{aligned}\n$$\nand directly expanding $v_k$ into the above product gives\n$$\n\\int_V \\nabla \\cdot v_k\\ dV = \\mathbf{a_i b_j ... c_l}\\int_V \\nabla_k \\cdot T_{ij...k...l}\\ dV\n$$\nleading to \n$$\n\\int_V \\nabla_k \\cdot T_{ij...k...l}\\ dV = \\int_S T_{ij...k...l}\\ n_k\\ dS\n$$\nas $\\mathbf{a, b, ..., c}$ are all constant.\n\n\n\n","n":0.024}}},{"i":8,"$":{"0":{"v":"Surface Integrals","n":0.707},"1":{"v":"\n> Vector Calculus is the repeatable two-stage process of thinking \"I don't think this can get any worse\", then seeing that it could, in fact, get worse, over and over and over again. Case in point: line integrals, and now this. What's next, volume integrals? Integral theorems??\n","n":0.146}}},{"i":9,"$":{"0":{"v":"Evaluating Surface Integrals","n":0.577},"1":{"v":"## Generalizing the line integral\n\nSince we're studying all these different types of integrals, and each one is a generalization or specification or manifestation or personification of the last, it's occasionally useful to understand exactly *how* they're all connected - and what parts they're generalizing off of one another.\n\nA surface integral is a generalization of two types of integrals: an area integral, whose idea of integrating along infinitesimal areas $dA$ is generalized to two-dimensional surfaces in three dimensions, rather than in the two-dimensional plane, and a line integral, whose idea of taking directional slices along a curve is generalized to taking directional slices along a surface.\n\nLet's take a look at how each of these generalizations operate. \n\n## Areas along a parametric surface\n\n![alt text](./assets/images/image-21.png)\n\nFor an area integral, the area differential $dA = dx\\ dy$ represented the process of splitting a region into infinitesimally small rectangular areas, each with length $dx$ and with $dy$. For a surface integral over a parametric surface $S = \\mathbf{x}(u,v)$, we deal not with rectangular areas, but with *surface areas* along the $u$- and $v$-directions - much like how, over a curve $C$, we deal with arc length $ds$ instead of the differential $dx$ along a straight line.\n\nThere is zero guarantee that $u$ and $v$ are orthogonal; given a small increment along the $u$-axis $\\delta u$ and along the $v$-axis $\\delta v$, we can only guarantee that, at infinitesimal scales, the infinite sum of the parallelograms formed by the vector\n$$\n\\mathbf{x}(u+\\delta u, v) - \\mathbf{x}(u, v)\n$$\nand the vector\n$$\n\\mathbf{x}(u, v+\\delta v) - \\mathbf{x}(u,v)\n$$\ncan approximate the surface area of $S$. The above two expressions look exactly like partial derivatives:\n$$\n\\mathbf{x}(u+\\delta u, v) - \\mathbf{x}(u, v) = \\frac{\\partial \\mathbf{x}}{\\partial u} \\ \\delta u\n$$\nand, correspondingly,\n$$\n\\mathbf{x}(u, v+\\delta v) - \\mathbf{x}(u, v) = \\frac{\\partial \\mathbf{x}}{\\partial v} \\ \\delta v.\n$$\nThe area of the parallelogram formed by two vectors is the magnitude of the cross-product of the two vectors, and thus our expression for the surface area differential $dS$ is\n$$\n| \\frac{\\partial \\mathbf{x}}{\\partial u} \\ \\delta u \\times \\frac{\\partial \\mathbf{x}}{\\partial v} \\ \\delta v|\n$$\nwhere $\\delta u$ and $\\delta v$ are scalars, and thus the above expression equals\n$$\ndS = |\\frac{\\partial \\mathbf{x}}{\\partial u} \\times \\frac{\\partial \\mathbf{x}}{\\partial v}|\\ du\\ dv\n$$\nwhere the cross product equals our expression for the normal vector to the surface!\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **scalar area differential** for a surface $S$ parameterized as $\\mathbf{x}(u,v)$ as\n$$\ndS = |\\frac{\\partial \\mathbf{x}}{\\partial u} \\times \\frac{\\partial \\mathbf{x}}{\\partial v}|\\ du\\ dv\n$$\n> as opposed to the vector area differential (encountered later).\n\n### Reparameterization invariance\n\nLike the uniqueness of the arc length parameterization for line integrals, the above definition for the surface area differential is independent of the parameterization $(u,v)$ chosen for a surface.\n\nSuppose we wish to re-parameterize the surface via alternative parameters $\\bar{u}, \\bar{v}$, assuming that each can be expressed as (scalar) functions of $u$ and $v$ (and vice versa). By the chain rule, we have\n$$\n\\frac{\\partial \\mathbf{x}}{\\partial \\bar{u}} = \\frac{\\partial \\mathbf{x}}{\\partial u} \\frac{\\partial u}{\\partial \\bar{u}} + \\frac{\\partial \\mathbf{x}}{\\partial v} \\frac{\\partial v}{\\partial \\bar{u}}\n$$\nand\n$$\n\\frac{\\partial \\mathbf{x}}{\\partial \\bar{v}} = \\frac{\\partial \\mathbf{x}}{\\partial u} \\frac{\\partial u}{\\partial \\bar{v}} + \\frac{\\partial \\mathbf{x}}{\\partial v} \\frac{\\partial v}{\\partial \\bar{v}}\n$$\nleading to \n$$\n\\begin{aligned}\n|\\frac{\\partial \\mathbf{x}}{\\partial \\bar{u}} \\times \\frac{\\partial \\mathbf{x}}{\\partial \\bar{v}}| &= (\\frac{\\partial \\mathbf{x}}{\\partial u} \\frac{\\partial u}{\\partial \\bar{u}} + \\frac{\\partial \\mathbf{x}}{\\partial v} \\frac{\\partial v}{\\partial \\bar{u}}) \\times (\\frac{\\partial \\mathbf{x}}{\\partial u} \\frac{\\partial u}{\\partial \\bar{v}} + \\frac{\\partial \\mathbf{x}}{\\partial v} \\frac{\\partial v}{\\partial \\bar{v}}) \\\\\n&= 0 + (\\frac{\\partial u}{\\partial \\bar{u}} \\frac{\\partial v}{\\partial \\bar{v}})|\\frac{\\partial\\mathbf{x}}{\\partial u}\\times \\frac{\\partial\\mathbf{x}}{\\partial v}| - (\\frac{\\partial u}{\\partial \\bar{v}} \\frac{\\partial v}{\\partial \\bar{u}})\\ |\\frac{\\partial\\mathbf{x}}{\\partial u}\\times \\frac{\\partial\\mathbf{x}}{\\partial v}| + 0 \\\\\n&= \\frac{\\partial(u,v)}{\\partial(\\bar{u},\\bar{v})}|\\frac{\\partial\\mathbf{x}}{\\partial u}\\times \\frac{\\partial\\mathbf{x}}{\\partial v}| \\\\\n&= \\frac{du\\ dv}{d\\bar{u}\\ d\\bar{v}}\\ |\\frac{\\partial\\mathbf{x}}{\\partial u}\\times \\frac{\\partial\\mathbf{x}}{\\partial v}|\n\\end{aligned}\n$$\nwhere the last two lines are due to the Jacobian, leading to\n$$\n|\\frac{\\partial \\mathbf{x}}{\\partial \\bar{u}} \\times \\frac{\\partial \\mathbf{x}}{\\partial \\bar{v}}|\\ d\\bar{u}\\ d\\bar{v} = |\\frac{\\partial\\mathbf{x}}{\\partial u}\\times \\frac{\\partial\\mathbf{x}}{\\partial v}|\\ du\\ dv.\n$$\n\n## Scalar fields\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>.\nFor a scalar field $\\phi(\\mathbf{x})$ and a parameterized surface $S = \\mathbf{x}(u,v)$ defined over a region $D = \\{(u,v): u_a < u < u_b,\\ v_a < v < v_b\\}$, define the **scalar surface integral** as the area integral\n$$\n\\int_{v_a}^{v_b} \\int_{u_a}^{u_b} \\phi(\\mathbf{x}(u,v))\\ |\\frac{\\partial\\mathbf{x}}{\\partial u} \\times \\frac{\\partial \\mathbf{x}}{\\partial v}|\\ du\\ dv\n$$\n> regardless of the parameterization $(u,v)$ considered. \n\nNote that taking $\\phi = 1$ simply gives the surface area of the surface.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Find the surface area of a sphere of radius $a$. \n\nI sure love it when calculus textbooks demonstrate the unadulterated power of calculus by overcomplicating statements in my fifth-grade textbook and irrevocably lifting the veil of innocence from my untarnished childhood forever.\n\nApply a parameterization in, big surprise, spherical coordinates: \n$$\n\\mathbf{x(\\theta, \\phi)} = (x,y,z) = \\begin{bmatrix}\na\\sin \\phi \\cos \\theta \\\\\na \\sin \\phi \\sin \\theta\\\\\na \\cos \\phi\n\\end{bmatrix},\\ 0 \\leq \\phi \\leq \\pi,\\ 0 \\leq \\theta \\leq 2\\pi,\n$$\nleading to \n$$\ndS = |\\frac{\\partial\\mathbf{x}}{\\partial \\theta} \\times \\frac{\\partial \\mathbf{x}}{\\partial \\phi}| = |\\begin{bmatrix}\n-a\\sin\\phi\\sin\\theta \\\\\na\\sin\\phi\\cos\\theta \\\\\n0\n\\end{bmatrix}\\times \\begin{bmatrix}\na\\cos\\phi\\cos\\theta \\\\\na\\cos\\phi\\sin\\theta \\\\\n-a\\sin\\phi\n\\end{bmatrix}| = a^2\\sin \\phi\\ d\\theta\\ d\\phi\n$$\n(Source: trust me bro/sis/gender-neutral-term-of-vitriolic-affection.) \n\nThe surface integral that leads to the surface area can be formulated as\n$$\n\\begin{aligned}\n\\int_{0}^{\\pi} \\int_{0}^{2\\pi}a^2\\sin\\phi\\ d\\theta\\ d\\phi \\\\\n= 2\\pi a^2 \\int_{0}^{\\pi}\\sin\\phi\\ d\\phi \\\\\n= 4\\pi a^2\n\\end{aligned}\n$$\nI can't believe it took these calculus nerds two semesters in Cambridge to figure out what I knew ever since fourth grade.\n\n## Vector fields\n\nAs promised, we now define the vector-valued analogue to the scalar area differential $dS$:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **vector area differential**, denoted $d\\mathbf{S}$, for a surface $S$ parameterized by $\\mathbf{x}(u,v)$ as\n$$\nd\\mathbf{S} = (\\frac{\\partial \\mathbf{x}}{\\partial u}\\times \\frac{\\partial \\mathbf{x}}{\\partial v})\\ du\\ dv\n$$\nwith the cross-product term no longer behind bars, having served its sentence for first-degree murder of $k\\mathbf{x}$ in the presence of $\\mathbf{x}$. In contrast to $dS$, $d\\mathbf{S}$ is a vector with direction pointing in the normal to the surface and magnitude $dS$; it thus encodes both surface area and surface direction within it, and that fact allows us to calculate\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Flux**. Suppose that the vector field $\\mathbf{F(x)}$ denotes the **velocity field** of a fluid, i.e. its direction and magnitude of flow at every point; we define the **flux** of the fluid about a surface $S$ as the amount of fluid crossing the surface $S$ per unit time.\n\nThis definition of flux is made calculable through the surface integral by two relevant facts:\n\n1. The direction pointing out of the surface at a point is given by $d\\mathbf{S}$ (pointing in the direction of the normal vector to the surface);\n2. The flow of a fluid $\\mathbf{F}$ in the direction $\\mathbf{n}$ is given by the dot product $\\mathbf{F\\cdot n}$.\n\nAll of this leads to the following formulation for flux, given a surface $S$, a velocity field $\\mathbf{F(x)}$, and a region $D$ in the $u-v$ plane over which the surface lies:\n$$\n\\int_D \\mathbf{F\\cdot dS}\n$$\nWe can, of course, speak of flux even if the underlying vector field $\\mathbf{F}$ does not describe fluid flow; *electric flux* and *magnetic flux* - the electric and magnetic fields passing through a surface, respectively - are good examples in this vein.\n\n","n":0.03}}},{"i":10,"$":{"0":{"v":"Revisiting Surfaces","n":0.707},"1":{"v":"## Review (See [[Vector Calculus.Curves and Surfaces.Surfaces]])\n\nA few points of importance before we journey forth towards surface integrals.\n\n### Defining surfaces\n\nA surface can be described in all of the following ways:\n- **Explicitly**, in the form $z = f(x,y)$, as a two-dimensional surface in three-dimensional space $\\mathbb{R}^3$.\n- **Implicitly**, in the form $F(x,y,z) = 0$, as a level set $w = 0$ to the function $F(x,y,z)=w$.\n- **Parametrically**, as a vector-valued parametric function with two degrees of freedom $\\mathbf{x} = \\psi(u,v)$ for parameters $u$, $v$ that maps points in $\\mathbb{R}^2$ to $\\mathbb{R}^3$.\n\n### Normal vectors to surfaces\n\n- For implicit surfaces $F(x,y,z)=0$, $\\nabla F$ provides the direction of the normal to the function at every point (as level curves $F = 0$ are curves where $F$ does not change, implying the directional derivative is zero, whereas the gradient is the direction that maximizes the directional derivative)\n    - By convention, we normalize the normal vector via $\\pm \\frac{\\nabla F}{|\\nabla F|}$ with sign determined by *orientation* (see below).\n- For parametric surfaces $\\mathbf{x = \\psi}(u,v)$, **define** the normal vector to the surface at point $(u,v)$ as the cross product\n$$\n\\frac{\\partial \\psi}{\\partial u}\\times \\frac{\\partial \\psi}{\\partial v}\n$$\nwhere $\\psi_u$ provides the tangent vector to the surface in the $u$-direction, while $\\psi_v$ provides the tangent vector in the $v$-direction; the normal vector is thus in a direction parallel to two tangent vectors to the surface. \n- Note that in $\\mathbb{R}^3$, the cross product is inherently signed (right-handed sets of vectors result in positive cross-products); however, the same is not true in higher-dimensional space.\n\n\n### Boundaries to surfaces\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **boundary** to a surface is the piecewise-smooth curve that lies on its \"edge\"; taken entirely unrigorously, that means a point where there's some direction where you'll fall off the surface if you take an infinitesimal step. (In contrast, an **interior point** in the surface is where you can walk infinitesimally in any direction and still stay on the surface.\n\nExamples of boundaries to surfaces: the edge of a swimming pool, the perimeter of a circle, and, if certain illuminaries are to be believed, Antarctica. \n\nSome surfaces have clear boundaries; the boundary to a disk is the circle that forms the perimeter of that disk. Other surfaces have no such boundaries; a hollow sphere is technically a surface, but its boundary is itself.\n\nFor a surface denoted $S$, one common notation for the boundary of the surface is $\\partial S$; this arises from the following alternate definition for the boundary. \n\nConsider two solids $V_r$ and $V_{r+\\epsilon}$ in $\\mathbb{R}^3$; $V_r$ is the solid bounded by a surface $S$ (e.g. a hollow spheere bounds a sphere), while $V_{r+\\epsilon}$ is the solid that is just *infinitesimally bigger* than $V_r$, e.g. through inflation via syringe pump injection of 0.1 $\\text{mol}$ of helium gas. Then $V_{r+\\epsilon}$ with $V_r$ removed - denoted $V_{r+\\epsilon}\\backslash V_r$ - yields the **edge** of $V_r$, and thus the boundary of $S$:\n$$\n\\partial S = \\lim_{\\epsilon \\to 0}\\frac{1}{\\epsilon}(V_{r+\\epsilon} \\backslash V_{r})\n$$\ne.g. a disk expanding ever-so-slightly minus the original disk yields a circle. In essence, $\\partial S$ is our way of saying that the edge of a surface - the part which moves when the surface is inflated - is its boundary.\n\n> According to the 60$ full-color textbook which I pirated off some shady Onion website and which has thus become my Holy Grail, there is a \"deep\" and \"beautiful\" reason, aside from the one given at present, for why this notation carries its meaning as the boundary to a surface. I'm guessing this is mathematician-speak for either one of two things: \n\n1. There *actually* is something deep and beautiful, but you won't get to know unless you take the optional *Infinite-Order Unorientability of Non-Riemannian Surfaces in Twnety-Dimensional Hyperspace* elective in the five-quadrillionth-femtosecond of Part XIV of your PhD in Differential Geometry, or\n2. They're embarassed to admit that the notation is only used today because some French nerd called Pierre Cumsock le Partiale pretard je Logriouxoueuaux in the 1700s invented the symbol and it's obtained too much structural staying-power to be contested ever since.\n\n> Or, on explaining the commutative identity of multiplication to third-graders: \"you may rightfully ask why we use the word 'commutative' to describe the binary operation $\\cdot$ in $\\mathbb{R}^n$, if there are no non-commutative operations in fields identical to $\\mathbb{R}^n$ up to isomorphism. There is a deep and beautiful reason behind this which will only be revealed in later courses.\"\n\nA final note on the power of denoting boundaries as $\\partial S$: note that the circumference of a circle of radius $r$, and thus area $\\pi r^2$, is \n$$\n\\frac{d}{dr}\\ \\pi r^2 = 2\\pi r\n$$\nand the same with a sphere of radius $r$, a hypersphere, etc. Coincidence? I think not.\n\nThe boundary of the boundary of a surface is the boundary of a curve, which is nothing. This concept can be succinctly expressed as $\\partial^2 S = 0$, and even more succinctly expressed as \"a really bad idea.\"\n\n### Orientability\n\nBasically: orientable surfaces are where you can start at one point on the surface, say \"this direction is up\", walk the entire way around the surface, and have the same direction be \"up\" the entire time. Unorientable surfaces, in contrast, are surfaces where you decide one direction is up at the beginning, walk all the way around, then suddenly discover that up has become down and fall off the surface in shock and horror as a result. \n\nThe choice of \"up\" - i.e. which direction is **outside** the surface, and which direction is **inside** - determines whether we append a positive or negative sign to the normal vector at some point on the surface. If this choice is consistent as we walk smoothly through the surface, call the surface \"orientable\"; otherwise, call it ~~all sorts of demeaning names and Oriental slurs~~ \"unorientable\".\n\nA sphere is orientable, while a M$\\ddot{o}$bius strip is not.\n\n> M$\\ddot{o}$bius","n":0.032}}},{"i":11,"$":{"0":{"v":"Poisson and Laplace Equations","n":0.5},"1":{"v":"\n> Poisson made groundbreaking strides in the investigation of potential fields in physics, asking \"what if $\\nabla^2 V = f$?\". Laplace made the equally groundbreaking stride of being a gigantic wuss and pretending like $f$ didn't exist. Naturally, the operator $\\nabla^2$ is now known far and wide as the Laplacian. \n\n\n\n","n":0.14}}},{"i":12,"$":{"0":{"v":"Solving the Laplace Equation","n":0.5},"1":{"v":"## Properties of the Laplace equation\n\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **The Laplace equation** is defined as $\\nabla^2\\Phi = 0$ for a scalar field $\\Phi$, where $\\nabla^2$ is the operator known as the **Laplacian**.\n\nTime for a few quick properties of this partial differential equation which should help us out a tiny bit!\n\n> <span style=\"background-color: #ffb812; color: black;\">Property 1</span>. **Linearity**. If $\\Phi$ and $\\Psi$ are both solutions to the Laplace equation, then any linear combination of the two $\\lambda \\Phi + \\mu \\Psi$ for scalars $\\lambda, \\mu$ also solves the Laplace equation.\n\nThis is swiftly verified via the linearity of the Laplacian operator itself. \n\nExpressed in differential form, the Laplace equation ppears as follows:\n$$\n\\nabla^2 \\Phi = \\frac{\\partial^2 \\Phi}{\\partial x_1^2} + \\frac{\\partial^2 \\Phi}{\\partial x_2^2} + ... + \\frac{\\partial^2 \\Phi}{\\partial x_n^2} = 0\n$$\nfor a scalar field $\\Phi$ in $\\mathbb{R^n}$. \n\nIt's natural to broadly conceptualize solutions to $\\Phi$ into one of either two types: types where the solution is **symmetric** - $\\frac{\\partial^2 \\Phi}{\\partial x_i^2}$ is the same in every single direction $x_i$, and thus equals zero - and types where the solution is not, and the function has different second partial derivatives along the axes and symmetry is lost.  \n\nThis leads us to the following property:\n\n### Radial symmetry and isotropic solutions\n\n> <span style=\"background-color: #ffb812; color: black;\">Property 2</span>. **Radial symmetry**. If $\\Phi$ is solely a function of the radius $r$ from the origin, e.g. in polar or spherical coordinates - $\\Phi = \\Phi(r)$ - then $\\Phi$ is symmetrical about all the coordinate axes; thus, if we can guarantee that $\\Phi$ has second derivative zero about one of the axes, it satisfies the Laplace equation by default.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Call such a solution $\\Phi = \\Phi(r)$ to $\\nabla^2 \\Phi = 0$ **isotropic**: it has no preferred direction.\n\nUsing the expression for $\\nabla^2$ in spherical coordinates (or, for the 2D analogue, polar coordinates) leads to $\\Phi$ satisfying\n\n$$\n\\frac{1}{r^2}\\frac{\\partial}{\\partial r}(r^2\\frac{\\partial \\Phi}{\\partial r}) = 0\n$$\nin which the other two terms involving the other partial derivatives $\\partial_\\theta$ and $\\partial_\\phi$ are eliminated because $\\Phi$ is conveniently only a function of $r$. This is now an ordinary differential equation - one we can easily solve! \n$$\n\\begin{aligned}\n\\frac{1}{r^2}\\frac{\\partial}{\\partial r}(r^2\\frac{\\partial \\Phi}{\\partial r}) &= 0 \\\\\n\\frac{\\partial}{\\partial r}(r^2\\frac{\\partial \\Phi}{\\partial r}) &= 0 \\\\\nr^2 \\frac{\\partial \\Phi}{\\partial r} &= \\beta \\\\\n\\frac{\\partial \\Phi}{\\partial r} &= \\beta r^{-2} \\\\\n\\Phi &= \\alpha - \\beta r^{-1}\n\\end{aligned}\n$$\nfor arbitrary constants $\\alpha$ and $\\beta$. Note that, in particular, solutions of the form $\\Phi = \\frac{\\beta}{r}$, e.g. $\\Phi = -\\frac{GM}{r}$, represent the gravitational and electrostatic fields we all know and love (with $r=0$ representing the point inside the point mass or point charge, where potential is not defined).\n\n> <span style=\"background-color: #ffb812; color: black;\">Caution</span>! This solution is clearly undefined at $r=0$, and does not solve the Laplace equation at that point.\n\nIf instead we want $\\Phi$ to be radially symmetric in cylindrical coordinates - $\\Phi = \\Phi(r)$ with $r = \\sqrt{x^2+y^2}$ and no $z$-direction - then $\\Phi$ is independent of $z$ and thus has zero partial derivative in that direction. As such, we need only consider\n$$\n\\begin{aligned}\n\\nabla^2 \\Phi = \\frac{1}{r}\\frac{\\partial}{\\partial r}(r\\frac{\\partial \\Phi}{\\partial r})&= 0 \\\\\nr\\frac{\\partial \\Phi}{\\partial r} &= \\beta \\\\\n\\Phi &= \\alpha + \\beta \\log r \n\\end{aligned}\n$$\nfor our solution. This solution is invariant along the $z$-axis and changes only in the $xy$-plane; as such, it is also a solution to the Laplace equation in $\\mathbb{R^2}$. Importantly, while the spherically symmetric solution $\\Phi = \\alpha - \\beta r^{-1}$ in $\\mathbb{R^3}$ decays to $\\alpha$ asymptotically, this solution grows to infinity as $r \\to \\infty$.\n\n\n### Monopoles and dipoles\n\n> Always two, there are. No more, no less.<br/>\n-- Master Yoda\n\nWe move on to one final property of solutions to the Laplace equation which allow us to generate more isotropic solutions from a single isotropic solution:\n\n> <span style=\"background-color: #ffb812; color: black;\">Property 3</span>. If $\\Phi$ is an isotropic solution to $\\nabla^2 \\Phi = 0$, then the directional derivative of $\\Phi$ in the direction of any vector $\\mathbf{d}$, $\\nabla_\\mathbf{d}\\Phi = \\mathbf{d} \\cdot \\nabla \\Phi$, is also a solution to the Laplace equation.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nUsing summation convention, we have\n$$\n\\nabla_\\mathbf{d} \\Phi = d_i \\partial_i \\Phi\n$$\nand as such\n$$\n\\nabla^2 (\\nabla_\\mathbf{d} \\Phi) = \\nabla^2(d_i \\partial_i \\Phi) =  \\sum_{j=1}^n \\partial^2_j (d_i \\partial_i \\Phi)\n$$\nand if indeed $\\Phi$ is an isotropic solution to the Laplace equation, then in every direction $\\partial^2_i \\Phi = 0$; as $\\Phi$ can be assumed to be smooth and continuous, swapping the order of the partial derivatives results in\n$$\n\\sum_{j=1}^n \\partial^2_j (d_i \\partial_i \\Phi) = \\sum_{j=1}^n d_i \\partial_i (\\partial^2_j \\Phi) = 0.\n$$\n\nIf we were to construct such directional-derivative-bound solutions, our basic building block would be:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Denote the isotropic solution to Laplace's equation in $\\mathbb{R^3}$ as the **fundamental solution**, or the **unit monopole**, if it has the property that the flux of its gradient out of a sphere of any radius is equal to one.\n\nThe flux of the gradient $\\nabla \\Phi$ has the convenient physical interpretation of being either gravitational flux (for $\\Phi$ equalling gravitational potential, where $\\nabla \\Phi$ is gravitational force) or electric flux.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The fundamental solution in $\\mathbb{R^3}$ takes the form $\\Phi_m = -1/4\\pi r$, and the fundamental solution in $\\mathbb{R^2}$ takes the form $f = \\log r/2\\pi$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>\n\nFor the general isotropic solution $\\Phi = \\alpha - \\beta r^{-1}$, consider the flux integral\n$$\n\\oiint_{S} \\nabla \\Phi \\cdot \\mathbf{d S}\n$$\nover an arbitrary sphere $S$ centered at the origin. Using the expression of gradient in spherical coordinates, we have\n$$\n\\nabla \\Phi = \\frac{\\partial }{\\partial r}(\\alpha - \\beta r^{-1})\\hat{\\mathbf{r}} = (\\beta r^{-2})\\hat{\\mathbf{r}}\n$$\nwhich is always parallel with the normal vector to the sphere at every point. Thus, the flux integral simplifies to the sum of $\\beta r^{-2}$ throughout the surface of the sphere, where it is constant; supposing that the sphere has radius $\\rho$ and thus surface area $4\\pi \\rho^2$, we have\n$$\n\\oiint_S = 4\\pi \\rho^2 \\cdot \\beta \\rho^{-2} = 4\\pi\\beta\n$$\nindependent of radius $\\rho$, yielding $\\beta = 1/4\\pi$ and thus the fundamental solution $\\Phi_m = -1/4\\pi r$. The $\\mathbb{R^2}$ case follows similarly.\n\n***\n\nThe fundamental solution is also known as the unit monopole because it's a hypothetical representation of a single magnetic pole (north or south), existing independent of its counterpart and freely suspended in space; despite every mathematical indication towards their existence and despite scientists' best attempts at smashing three billion dollars' worth of funding to bits and pieces in the Large Hadron Collider, monopoles have never been found outside of physicists' wet dreams and in-between the arms of your mom at the epileptic strip club down the street.\n\n\nGiven this fundamental solution, we define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **dipole** of strength $\\mathbf{d}$ - a pair of magnetic poles with opposite polarity, i.e. north and south, separated by a close distance and with large but opposite strengths - can be modelled by the Laplace equation solution $\\nabla_\\mathbf{d} \\Phi_m$, where $\\Phi_m$ is the fundamental solution as above.\n","n":0.03}}},{"i":13,"$":{"0":{"v":"Integral Solutions","n":0.707},"1":{"v":"## Point sources and the Delta function\n\nAs mentioned before, an important caveat to the isotropic solution for the Laplace equation in $\\mathbb{R^3}$ - which takes the form $\\psi(r) = \\alpha - \\beta r^{-1}$ - is the fact that it diverges at the origin, where $r = 0$; at that point, it can no longer be construed as a valid solution to the Laplace equation. In that case, then, 1) what does $r=0$ mean in a physical sense, and 2) what equation does the fundamental solution solve at that point?\n\nThe answer to the first question is slightly easier. A gravitational field around an infinitely small point mass, for instance, would satisfy the Laplace equation everywhere except for the point where the mass is located; at every other point, the mass density of the point mass is zero. As such, $r = 0$ in the context of a solution to the Laplace equation can be interpreted as gravitational potential near a point mass - all the density concentrated in a single point, zero density everywhere else, and infinite potential/energy required to bring another mass to that point as a result.\n\nTo see what equation our fundamental solution solves at $r = 0$, consider the volume integral in a spherical region $V$ of radius $R$ around the origin\n\n$$\n\\int_V \\nabla^2 \\psi\\ dV = \\oiint_S \\nabla \\psi \\cdot \\mathbf{dS} = (\\nabla\\psi(R))4\\pi R^2 \n$$\nby the Divergence Theorem; yet we know that, by definition of the fundamental solution, $\\nabla^2 \\psi = 0$ at every point aside from the origin. This would imply that the Poisson equation satisfied by $\\psi$, $\\nabla^2 \\psi = \\rho$, has the properties:\n$$\n\\begin{cases}\n0 \\text{ everywhere where $\\mathbf{x \\neq 0}$,} \\\\\n\\int_V \\rho\\ dV = 4\\pi R^2 (\\nabla\\psi(R))\n\\end{cases}\n$$\nand when $\\psi(r)$ is our fundamental solution $1/4\\pi r$, we have $\\nabla \\psi = -1/4\\pi r^2$ and $\\int_V \\rho\\ dV = -1$.\nThese are properties only the Delta function would have. In three dimensions, the Delta function $\\delta^3(\\mathbf{x})$ is easily generalized by definition to be\n$$\n\\begin{cases}\n\\delta^3(\\mathbf{x}) = 0, \\mathbf{\\ x\\neq 0}\\\\\n\\int_V \\delta^3(\\mathbf{x})\\ dV = 1\\text{ for any region $V$ enclosing the origin}.\n\\end{cases}\n$$\nAs such, the *fundamental solution* $\\psi(r) = 1/4\\pi r$ obtained previously as a solution to the Laplace equation everywhere except the origin is, in reality, a solution of the more general Poisson equation\n$$\n\\nabla^2 \\psi = -\\delta^3(\\mathbf{x})\n$$\nthroughout a domain that now encompasses the entirety of $\\mathbb{R^3}$; in general, we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Poisson equation for a point source**. The isotropic solution to the Laplace equaiton\n$\\psi(r) = \\lambda/4\\pi r$ solves the Poisson equation\n$$\n\\nabla^2 \\psi = -\\lambda\\delta^3(\\mathbf{x}).\n$$\nIn particular, the Delta function's presence represents a **point source** for the potential: one that exists only at a single point and nowhere else.\n\n## An integral solution for the Poisson equation\n\nWe understand from the above that the isotropic solution\n$$\n\\psi(r) = \\lambda/4\\pi r\n$$\nto the Laplace equation models a point source of strength $\\lambda$ placed at the origin; if instead the point source is placed at a point $\\mathbf{x'}$, then by extension we have\n$$\n\\psi(\\mathbf{x}) = \\frac{\\lambda}{4\\pi |\\mathbf{x - x'}|}.\n$$\nNow suppose that instead of a single point source, we have many, many different point sources scattered about a closed region $V$; each has a different strength $\\lambda$ - enough to justify our use of a function $\\rho$ to represent the strength of the point source placed at point $\\mathbf{x'}$: \n$$\n\\lambda = \\rho(\\mathbf{x'}).\n$$\nEventually, when we have an infinite number of point sources that cover $V$ in its entirety and $\\rho$ becomes a smooth, continuous function, the point sources collect together to become an object bounded by $V$ with density $\\rho(\\mathbf{x'})$ at each point:\n\n$$\n\\psi(\\mathbf{x}) = \\int_V \\frac{\\rho(\\mathbf{x'})}{4\\pi |\\mathbf{x - x'}|}\\ dV.\n$$\n\nwhere, importantly, the variable being integrated through in the volume integral is $\\mathbf{x'}$, **not** $\\mathbf{x}$. This ultimately leads to one final claim:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The amalgamated object $\\psi(\\mathbf{x})$, represented by the above volume integral of point sources, is a solution to the Poisson equation $\\nabla^2 \\psi = -\\rho.$\n\nThe surface-level dark-magic trickery of what we're doing here belies a simple explanation for how this might have occurred: the Poisson equation $\\nabla^2 \\psi = -\\rho(\\mathbf{x})$ describes a potential function $\\psi$ which arises from a source of density $\\rho$ at points $\\mathbf{x} \\in \\mathbb{R^3}$. Therefore, if at each **infinitesimal point** some function $\\psi$ has a point source placed there with strength $\\rho$, and if it smells like the potential function of $\\rho$ and talks like the potential function of $\\rho$, then it **is** the potential function of $\\rho$.\n\nNot rigorous enough yet? Then\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nDirect substitution gives\n$$\n\\begin{aligned}\n\\nabla^2 \\psi(\\mathbf{x}) &= \\int_V \\nabla^2\\frac{\\rho(\\mathbf{x'})}{4\\pi |\\mathbf{x - x'}|}\\ dV \\\\\n&= \\int_V \\rho(\\mathbf{x'})\\frac{1}{4\\pi}\\nabla^2 \\frac{1}{|\\mathbf{x-x'}|}\\ dV\n\\end{aligned}\n$$\nWhere we know that $\\frac{1}{\\mathbf{x-x'}} = \\frac{1}{r}$, as an isotropic solution to the Laplace equation, also satisfies the Poisson equation involving a delta function\n$$\n\\nabla^2 \\frac{1}{\\mathbf{|x-x'|}} = -4\\pi \\delta^3(\\mathbf{x-x'})\n$$\ntranslated from the origin to $\\mathbf{x'}$. What we are left with is simply\n$$\n\\begin{aligned}\n\\nabla^2 \\psi(\\mathbf{x}) &=\\int_V -\\rho(\\mathbf{x'})\\frac{1}{4\\pi}4\\pi \\delta^3(\\mathbf{x-x'})\\ dV \\\\\n&= \\int_V -\\rho(\\mathbf{x'})\\delta^3(\\mathbf{x-x'})\\ dV \\\\\n&= p(\\mathbf{x})\n\\end{aligned}\n$$\nintegrating over $\\mathbf{x'}$, by the properties of the Delta function.","n":0.035}}},{"i":14,"$":{"0":{"v":"Existence and Uniqueness","n":0.577},"1":{"v":"\n## Boundary conditions and Poisson's Equation\n\nSuppose we want to solve the Poisson equation\n$$\n\\nabla^2 \\Phi = \\rho(\\mathbf{x})\n$$\nfor some source function $\\rho$, and within a region of space $V$ with boundary $\\partial V$. In particular, we specify one of two boundary conditions over $\\partial V$: either\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **The Dirichlet condition**, for which $\\Phi(\\mathbf{x})$ is directly specified by a function $\\psi(\\mathbf{x})$ for some $\\psi$ on all points on $\\partial V$: $\\Psi(\\mathbf{x}) = \\psi(\\mathbf{x}), \\mathbf{x} \\in \\partial V$.\n\nor, alternatively,\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **The Neumann condition**, for which the directional derivative in the direction of the normal $\\mathbf{n}$ to the surface $\\partial V$ is specified by a function $\\psi(\\mathbf{x})$: $\\mathbf{\\Phi(x)\\cdot n} = \\psi(\\mathbf{x}), \\mathbf{x}\\in\\partial V$. \n\nFor this broad formulation of boundary-value problems concerning Poisson's equation, we claim an equally broad statement of uniqueness:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The solution $\\Phi$ to the Poisson equation $\\nabla^2\\Phi = \\rho$ over a bounded region $V$, where either the Dirichlet or the Neumann condition is specified over the boundary $\\partial V$, is unique as long as it exists.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nCrucial to proving this statement is the following lemma, known as Green's first identity, named (much like Green's theorem and Green's second identity) after the Green Goblin:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. For scalar fields $\\phi$ and $\\psi$ in $\\mathbb{R^3}$, we have\n$$\n\\int_V \\phi \\nabla^2 \\psi\\ dV = \\int_{\\partial V}\\phi\\nabla\\psi\\cdot\\mathbf{dS} - \\int_V \\nabla \\phi \\cdot \\nabla \\psi\\ dV.\n$$\n> This can be understood as a multivariate analogue to integration by parts:\n$$\n\\int u\\ dv = uv - \\int v\\ du\n$$\n> in which we have $u = \\phi$ and $v = \\nabla \\psi$, and the surface integral can be understood as \"reducing the degree of integration\" by $1$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThis lemma follows directly from the application of the Divergence Theorem on the surface integral:\n\n$$\n\\int_{\\partial V}\\phi \\nabla \\psi \\cdot \\mathbf{dS} = \\int_{\\partial V} \\nabla\\cdot(\\phi \\nabla \\psi) dV\n$$\nresulting in the other two terms. Correspondingly, we also have the result\n$$\n\\int_V \\phi\\nabla^2 \\psi - \\psi \\nabla^2 \\phi\\ dV = \\int_{\\partial V}(\\phi \\nabla \\psi - \\psi \\nabla \\phi) \\cdot \\mathbf{dS};\n$$\nthis is known as *Green's second identity*, no relation to the first.\n\n***\n\nAfter the preceding contractually-obligated commercial break, we now return to the Poisson equation. Suppose, for the sake of contradiction, that both $\\Phi$ and $\\Psi$ are scalar fields that satisfy a given boundary condition and solve the Poisson equation over **any arbitrary** region $V$. Then we have\n\n$$\n\\nabla^2 \\Phi - \\nabla^2 \\Psi = \\nabla^2 (\\Phi - \\Psi) = \\rho - \\rho = 0\n$$\nwith $\\Phi - \\Psi$ satisfying the Laplace equation in the same region; denote $\\Phi - \\Psi$ as $f$. By Green's first identity, we have\n$$\n\\int_V f\\nabla^2 f\\ dV = \\int_V 0\\ dV = 0\n$$\non the left-hand side, taking $\\phi = \\psi = f$, and on the right-hand side\n$$\n\\int_{\\partial V} f\\nabla f \\cdot \\mathbf{dS} - \\int_V (\\nabla f) \\cdot (\\nabla f)\\ dV\n$$\nresulting in \n$$\n\\int_{\\partial V} f\\nabla f \\cdot \\mathbf{dS} = \\int_V (\\nabla f) \\cdot (\\nabla f)\\ dV = \\int_V |\\nabla f|^2\\ dV\n$$\nwhich we note is always non-negative.\n\nNow we make use of the boundary conditions. If both $\\Phi$ and $\\Psi$ satisfy either the Dirichlet or Neumann boundary conditions, then either $\\Phi - \\Psi = f = 0$ (Dirichlet; both equal the same function on the boundary) or $\\nabla(\\Phi - \\Psi) \\cdot \\mathbf{n} = \\nabla f \\cdot \\mathbf{n} = 0$ (Neumann; the gradient of both fields have the same dot product with the normal to the boundary). \n\nAs such, the left-hand side above is zero in both cases; for the Dirichlet condition $f = 0$ directly, and for the Neumann condition $\\nabla f \\cdot \\mathbf{dS} = \\nabla f \\cdot \\mathbf{n}\\ dS = 0$. However, this surface integral - which is always zero - always equals the integral of a non-negative quantity $|\\nabla f|^2$ over **any arbitrary** volume; as any volume can be chosen, this clearly reaches a contradiction. $\\square$\n\n***\n\nWe have two small amendments to make to the above theorem before proceeding:\n\n1. The above proof is just as viable for boundary conditions that are a mix of Neumann and Dirichlet conditions, as well as for regions where there are different boundary conditions on different parts of the boundary.\n\n2. Uniqueness does not guarantee existence even though existence guarantees uniqueness. For instance, if $\\Phi$ satisfies $\\nabla^2 \\Phi = \\rho$ in a region $V$ with Neumann condition $\\nabla \\Phi \\cdot \\mathbf{n} = g(\\mathbf{x})$ for some $g$, then \n$$\n\\oiint_{\\partial V} \\nabla \\Phi \\cdot \\mathbf{n}\\ dS = \\oiint_{\\partial V} \\nabla \\Phi \\cdot \\mathbf{dS} = \\int_V \\nabla^2\\Phi\\ dV\n$$\nby the Divergence Theorem, which mandates that $\\oiint_{\\partial V} g\\ dS = \\int_V \\rho\\ dV$.\n\n## Harmonic functions and their properties\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Solutions to the Laplace equation $\\nabla^2 \\psi = 0$ are called **harmonic functions**. Not to be confused with the type of harmonic function that pops up in a musicology PhD's doctoral thesis analyzing the first bars of Rick Astley's hit single, \"Never Gonna Give You Up\".\n\nHarmonic functions pop up everywhere in the real world. As stated, they describe the potential function of something when that particular \"something\" has no source - when it is equally diffused throughout the region, like heat or fluid flow or \"the smell of that guy who hasn't showered in a week showing up to the lecture hall\".\n\nThis section will introduce just two of the countless fascinating properties of harmonic functions. (In reality the *only* two fascinating properties, but we don't need to know that.)\n\n> <span style=\"background-color: #ffb812; color: black;\">Property 1</span>. The **mean-value property**. The value of a harmonic function $\\psi(\\mathbf{x})$ at a point $\\mathbf{x = a}$ within a region that encompasses a sphere of radius $R$ centered at $\\mathbf{a}$ is equal to the mean value of $\\psi$ on that sphere $S$:\n\n$$\n\\psi(\\mathbf{a}) = \\bar{\\psi}(R) = \\frac{1}{4\\pi R^2} \\int_S \\psi(\\mathbf{x})\\ dS\n$$\n\n> for any value of $R$ within the region.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nUsing spherical coordinates $(r, \\theta, \\phi)$ centered at $\\mathbf{a}$, the sphere $S$ can be represented in parametric form as\n$$\n(r\\sin \\theta \\cos \\phi, r \\sin \\theta \\sin \\phi, r\\cos \\phi)\n$$\n and the surface-area component $dS$ as $r^2 \\sin \\theta\\ d\\theta\\ d\\phi$ by the cross product. As such, we have\n$$\n\\begin{aligned}\n\\bar{\\psi}(r) = \\frac{1}{4\\pi r^2}\\int_{S}\\psi(\\mathbf{x})\\ dS = \\frac{1}{4\\pi}\\int_0^{\\pi}\\int_0^{2\\pi}\\psi(r,\\theta,\\phi)\\sin \\theta\\ d\\theta\\ d\\phi   \n\n\\end{aligned}\n$$\nNote here that $\\psi(r, \\theta, \\phi)$ is solely a function of $r$, and thus satisfies $\\nabla\\psi(r) = \\frac{\\partial \\psi}{\\partial r} \\hat{\\mathbf{r}}$ and $\\mathbf{dS} \\cdot \\nabla\\psi(r) = \\nabla\\psi(r)\\ dS$ as the normal to the surface is parallel with the gradient, which has $r$-component only.\n\nOur task is now to prove that this is a constant with respect to $r$, and that its value is $\\psi(\\mathbf{a})$. Begin with \n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial r}\\bar{\\psi}(r) &= \\frac{1}{4\\pi}\\int_0^{\\pi}\\int_0^{2\\pi}\\frac{\\partial}{\\partial r}\\psi(r)\\ dS \\\\\n&= \\frac{1}{4\\pi}\\int_S \\nabla\\psi \\cdot \\mathbf{dS} \\\\\n&= \\frac{1}{4\\pi}\\int_V \\nabla^2 \\psi\\ dV =0\n\\end{aligned}\n$$\nby the Divergence Theorem and the definition of a harmonic $\\nabla^2 \\psi = 0$. Thus $\\bar \\psi(r)$ is constant; in addition, when $r \\to 0$ the sphere being considered encompasses the point $\\mathbf{a}$ and $\\mathbf{a}$ only, and so $\\bar\\psi(r)$ takes the value $\\psi(\\mathbf{a})$ at that point. $\\square$\n\n\n\n> <span style=\"background-color: #ffb812; color: black;\">Property 2</span>. The **extreme-value property**. Harmonic functions satisfying the Laplace equation within a region $V$ have no local maxima or minima within $V$; any maxima or minima must be on the boundary of the region.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nIf $\\nabla^2 \\psi = 0$, then $\\nabla \\cdot (\\nabla \\psi) = 0$ for every point in $\\psi$. Assume for the sake of contradiction that a maximum or minimum point $\\mathbf{x}$ exists in $V$; by definition, such a point satisfies either\n$$\n\\psi(\\mathbf{x + \\epsilon}) > \\psi (\\mathbf{x})\n$$\nor\n$$\n\\psi(\\mathbf{x + \\epsilon}) < \\psi (\\mathbf{x})\n$$\nfor all such infinitesimal vectors $\\epsilon$ with $|\\epsilon| \\to 0$. Suppose that $\\mathbf{x}$ is a maximum point, with $\\psi(\\mathbf{x + \\epsilon}) < \\psi (\\mathbf{x})$. Then by the **mean-value property** above, we have\n\n$$\n\\psi(\\mathbf{x}) = \\text{mean value of points surrounding $\\mathbf{x}$}\n$$\nwhich is smaller than $\\psi(\\mathbf{x})$ by definition. This leads to a contradiction.","n":0.028}}},{"i":15,"$":{"0":{"v":"Physical Origins and Significance","n":0.5},"1":{"v":"## The Fundamental Forces\n\nWe begin, as we always do, with the Four Horsemen of Physics: the four fundamental forces - gravitational, electromagnetic, and the strong and weak nuclear forces. We have, in ascending order of annoyance,\n\n1. The gravitational force, acting between two point particles with masses $m$ and $M$ separated by a distance $r$. When these two masses are static, this force obeys an *inverse-square* law:\n$$\n\\mathbf{F(r)} = -\\frac{GMm}{r^2}\\mathbf{\\hat{r}}\n$$\ndenoting the vector between them as $\\mathbf{r}$, its corresponding unit vector as $\\mathbf{\\hat{r}}$, and its magnitude as $r$.\n\n1. The electromagnetic force, acting between two point particles with charges $Q$ and $q$ separated by a distance $r$. This force satisfies an *inverse-square law* too, but it's a *different* one:\n$$\n\\mathbf{F(r)} = \\frac{Qq}{4\\pi \\epsilon_0 r^2}\\hat{\\mathbf{r}}\n$$\n<h6>(and also Maxwell's equations maybe?)</h6><br/>\n\n1. The strong nuclear force. Um, that one involves, like, neutrons and protons and morons and stuff, and it holds them together or something. It's also really strong. (Totally.)\n2. The weak nuclear force. This one <h6>(*mumble*) radioactive decay (*mumble*) (*mumble*) (*more incoherent mumbling*) atoms and electrons.</h6> \n\nAnd that's all I got, folks! \n\n## Inverse-square unification\n\nWhy do the formulas for the gravitational and electromagnetic forces both roughly follow an inverse-square law, differing only by a constant, a sign (which signifies gravitational attraction and electrostatic repulsion), and the name of the wrinkled old bastard now enshrined upon it - that is, within a static frame of reference where both objects aren't moving? \n\nCertainly, any surface-level similarities dissolve away completely when the objects begin to move; but the fact that an inverse-square law codifies both forces suggests some shared origin at play. That shared origin has everything to do with the underlying force fields at work: the gravitational and electric fields, respectively. \n\nSuppose that two objects - one with mass $M$ and charge $Q$, the other with mass $m$ and charge $q$, with $M >> m$ and $Q >> q$ - are stationary with distance vector $\\mathbf{r}$ in space. Given such a disparity in mass and charge, the effects enacted by the weakly-charged object on the strongly-charged object are negligible. \n\nWe are thus able to derive the *gravitational field strength* and *electric field strength* of the strongly-charged object at a point $\\mathbf{x}$ occupied by the weakly-charged object as\n$$\n\\mathbf{g(x)} = \\frac{\\mathbf{F_{grav}(x)}}{m} = -\\frac{GM}{r^2}\\mathbf{\\hat{r}}\n$$\nand\n$$\n\\mathbf{E(x)} = \\frac{\\mathbf{F_{electric}(x)}}{q} = \\frac{Q}{4\\pi\\epsilon_0 r^2}\\mathbf{\\hat{r}}\n$$\nwith distance vector $\\mathbf{r}$ between the two objects, respectively. This doesn't tell us anything new, necessarily; they're just consequences of the inverse-square laws we had at the beginning. But what *could* tell us something new is whether or not we can derive these field equations from common principles. Specifically, that common principle is:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **The Gaussian principle**. The total field strength emanating from a point in space - be the field gravitational or electric - is proportional to the quantity of mass or charge present at that point.\n\nThis statement is fairly intuitive: after all, it makes sense that the more the mass, the more the field strength/attraction (and vice versa).\n\n> Betcha you thought I was gonna make a \"your mom\" joke there! Well, I'm far too classy to do that. I'm saving that for our later section on poles.\n\nIt also helps that the notion of \"field strength emanating from a point\" lends itself nicely to the concept of divergence, in mathematical language: \n\n$$\n\\nabla \\cdot \\mathbf{g} \\propto \\rho(\\mathbf{x}) \n$$\nwhere $\\rho$ denotes (mass) density at point $\\mathbf{x}$ in $\\mathbb{R^3}$ of the object of mass $M$, and similarly\n$$\n\\nabla \\cdot \\mathbf{E} \\propto \\rho_e(\\mathbf{x})\n$$\nfor electric field strength $\\mathbf{E}$ and *charge density* $\\rho_e$ of the object. Experimental results verify that the constant of porportionality is $-4\\pi G$ with $G$ the gravitational constant and $\\pi$ the circle constant for the gravitational field, with the negative sign signifying attraction, and $1/\\epsilon_0$ for that other one. (I don't have a favorite.) Thus, purely out of the fundamental intuition that field strength is proportional to density, we have:\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Gauss' Law**.\n\n$$\n\\begin{cases}\n\\nabla \\cdot \\mathbf{g} = -4\\pi G \\rho(\\mathbf{x}) \\\\\n\\nabla \\cdot \\mathbf{E} = \\rho_e(\\mathbf{x})/\\epsilon_0.\n\\end{cases}\n$$\nThis is specifically known as Gauss' Law in *differential form*, not to be confused with the other 493 Gauss' Laws, or the 1,928 Gauss' Theorems, or the middle name of my second-born child.\n\nHappily, this statement involves an expression of divergence; and all our Spidey-Senses tingle as we buckle up for a surface integral of epic proportions for both the above fields:\n\n$$\n\\int_V \\nabla \\cdot \\mathbf{g}\\ dV = \\oiint_{dV} \\mathbf{g} \\cdot \\mathbf {dS}\n$$\nwhere finally the worldrending powers of $\\oiint$ are unleashed, and \n$$\n\\int_V \\nabla \\cdot \\mathbf{E}\\ dV = \\oiint_{dV} \\mathbf{E} \\cdot \\mathbf {dS}.\n$$\nThe simplest such region in $\\mathbb{R^3}$ and its corresponding surface in the context of these fields is a ball and its boundary sphere. For such an object, the direction of both fields at a point $\\mathbf{x}$ - $\\mathbf{g}$ and $\\mathbf{E}$ - are proportional to the vector $\\mathbf{r}$ between the origin point of the field and $\\mathbf{x}$; as such, on the surface of the sphere, $\\mathbf{g}$ and $\\mathbf{E}$ are everywhere normals to the sphere. \n\nAs such, the left-hand side volume integrals evaluate to \n$$\n\\int_V \\nabla \\cdot \\mathbf{g}\\ dV = \\int_V -4\\pi G \\rho(\\mathbf{x})\\ dV = -4\\pi G \\int_V\\rho(\\mathbf{x})\\ dV = -4\\pi G M\n$$\nas the volume integral of density is mass, with the volume in question being centered around the object with mass $M$, *whether or not it is a point mass* - the object can be any size or shape. This also extends to the electric field, whose volume integral evaluates similarly to\n$$\n\\int_V \\nabla \\cdot \\mathbf{E}\\ dV = \\frac{Q}{\\epsilon_0}\n$$\nas the volume integral of charge density is charge. \n\nSimultaneously, by the statement of the Divergence Theorem above we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The **integral form** of Gauss' Law.\n$$\n\\begin{cases}\n\\oiint_{dV} \\mathbf{g} \\cdot \\mathbf {dS} = -4\\pi G M \\\\\n\\oiint_{dV} \\mathbf{E} \\cdot \\mathbf {dS}.= \\frac{Q}{\\epsilon_0}.\n\\end{cases}\n$$\nCall this the *integral form* of Gauss' Law. As a consequence of the Divergence Theorem, we know that we could've started with either the integral or the differential forms and ended up with the other; and, even more pleasingly, the integral form also makes just as much sense to us (intuitively speaking) as the differential form does. In plainer terms, it says simply this:\n\n> The amount of gravitational or electric field flowing out of an object is proportional to the amount of mass or charge present in that object. \n\nCompare this with the differential form of Gauss' Law for a single point. Evaluating the surface integrals directly is also fairly simple: as stated, $\\mathbf{g}$ and $\\mathbf{E}$ are everywhere parallel to the normal surface-area component $\\mathbf{dS}$, leading to\n$$\n\\oiint_{dV} \\mathbf{g\\cdot dS} = (\\text{surface area of sphere})\\ g(\\mathbf{r}) = 4\\pi r^2(\\mathbf{g(r)}) = -4\\pi GM\n$$\nyielding\n$$\ng = -\\frac{GM}{r^2}\n$$\nrecovering Newton's Law of Gravitation, and similarly\n$$\nE = \\frac{Q}{4 \\epsilon_0 \\pi r^2}\n$$\nrecovering Coulomb's Law for a spherical region in space. (Gauss' laws in differential form also form the first of Maxwell's equations.)\n\nAll this physicist mumbo-jumbo tells us just two important things:\n\n1. Both the electric and gravitational forces satisfy inverse-square laws because they originate from the same physical principle: that field strength is proportional to density.\n\n2. Through Gauss' Law and applications of the Divergence Theorem, gravitational and electric field strengths aren't only applicable to point masses; in fact, they're applicable to spheres (as above) and indeed objects of any shape or size (and in any dimension), as we'll soon see below - with the Poisson equation.\n\n> Incidentally, another consequence of Gauss' Law is that because the volume integral above doesn't care how the density is distributed inside the sphere, you could have an object that's entirely hollow on the inside and it would still look fine to the equations. This is yet another instance of math being a fantastic metaphor for how my life is going right now.\n\n## The Poisson and Laplace Equations\n\nSuppose that we now want to find, as before, the electric field strength $E$ (or gravitational field strength $g$) some distance $r$ away from an object with mass $M$ - but with the twist that the object may not be spherical; indeed, it could just as well be a square, one of Shrek's ears represented as a Gabriel's Horn, a $\\pi^{\\pi^{69420}}$-pixel-resolution Mandelbrot set, or a cylinder of length 5.5 inches and girth 3.1 inches encased in a M&Ms-tube filled with peanut butter, jelly, and mashed bananas. \n\nThe left-hand volume integral\n$$\n\\int_V \\nabla \\cdot \\mathbf{g}\\ dV\n$$\nproceeds exactly as expected; the issue lies with the right-hand side flux integral, where the dot product $\\mathbf{g\\cdot dS}$ is no longer guaranteed to be sympathetic to our ambitions - unlike such a dot product for a spherical object, where field lines were always normal to the surface. \n\nThe new identity entering the fray here originates from one place and one place only: conservatism. (This is a sentence no human being has ever uttered in the history of the universe, and will never utter again.) Physically, we know that both gravitational and electric forces are **conservative**, not because it likes to start conversations at the dinner table with \"I'm not racist, but...\" and eschew the virtues of supply-side economics, but because of two reasons:\n\n1. *Conservatism* for a force field means that conservation of energy is achieved no matter what path a particle takes between two points. If this weren't true, this would imply that if you climbed a mountain and then climbed back down, there would be a certain pair of paths where you would feel more like you just dunked an entire 12-pack of Red Bull than you started.\n\n2. From Gauss' law, both the gravitational and electric field strengths of a point charge are radially symmetrical; they can thus be written in the form\n$$\n\\mathbf{g} = g(r)\\mathbf{\\hat{r}}\n$$\nwith magnitude dependent only on radius $r$ and not direction. It can be verified that the curl of all radially symmetrical functions are zero:\n$$\n\\begin{aligned}\n\\nabla \\times \\mathbf{g} &= \\epsilon_{ijk}\\nabla_j(g(r)\\hat{\\mathbf{r}}_k) \\\\\n&= \\epsilon_{ijk}(\\frac{x_j x_k}{r}g'(r) + g(r)\\delta_{jk}) \\\\\n\\end{aligned}\n$$\nwhich sums to zero due to the antisymmetry of $\\epsilon_{ijk}$, noting that $r = \\sqrt{x_1^2 + x_2^2 + x_3^2}$. A field with zero curl is conservative by Stokes' theorem, and thus both of the above roads lead back to Rome. (We can check that the inverse-square formulation of the electric field satisfies these equations.)\n\nWe know that conservative fields can be written as the gradient of some potential $\\Phi$:\n$$\n\\mathbf{g} = \\nabla \\Phi,\\ \\mathbf{E} = \\nabla \\Psi.\n$$\nThis leads us to the differential form of Gauss' Law, where we wrote\n$$\n\\begin{cases}\n\\nabla \\cdot \\mathbf{g} = -4\\pi G \\rho(\\mathbf{x}) \\\\\n\\nabla \\cdot \\mathbf{E} = \\frac{\\rho_{e}(\\mathbf{x})}{\\epsilon_0}\n\n\\end{cases}\n$$\nwith the substitution of the potentials resulting in\n$$\n\\begin{cases}\n\\nabla^2 \\Phi = -4\\pi G \\rho(\\mathbf{x}) \\\\\n\\nabla^2 \\Psi = \\frac{\\rho_{e}(\\mathbf{x})}{\\epsilon_0}\n\\end{cases}\n$$\nand finally and at long last, to the\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Poisson equation**. For a scalar-field **source** $\\rho(\\mathbf{x})$ and a **potential field** $\\Phi$, the **Poisson equation** is the partial differential equation\n$$\n\\nabla^2 \\Phi = \\rho(\\mathbf{x})\n$$\n> for which the above two stated examples are the Poisson equations satisfied by the gravitational and electric potential fields, respectively. \n\nThe special case where the source field vanishes is called\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **The Laplace equation**, which is simply formulated as $\\nabla^2\\Phi = 0$.\n\n","n":0.024}}},{"i":16,"$":{"0":{"v":"Partial Differentiation","n":0.707},"1":{"v":"> Goddammit, we're back in the soup.\n","n":0.378}}},{"i":17,"$":{"0":{"v":"The Chain Rule","n":0.577},"1":{"v":"> So apparently derivatives are fractions now.\n\n### The univariate chain rule, revisited\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **composite function** (of one variable) is a function of the form $f(x)$ where $x$ is itself a function of another variable $t$: $x = g(t)$. \n\nThe chain rule in 1D aims to answer the question: if $f$ depends on $x$ and $x$ depends on $t$, how does $f$ change when $t$ change? - i.e. what is $\\frac{df}{dt}$? The two ways of notating the result are either\n$$\n\\frac{df}{dt} = f'(g(t))g'(t)\n$$\nor\n$$\n\\frac{df}{dt} = \\frac{df}{dx}\\frac{dx}{dt}\n$$\nbut they speak the same meaning: the rate of change of $f$ with respect to $t$ is the rate of change of $f$ with respect to $x$, multiplied by the rate of change of $x$ with respect to $t$. \n\n> <span style=\"background-color: #12ffd7; color: black;\">Caution.</span> You will use the second notation, and if you use the first notation, I will personally call 911 and notify the Cringe Police of England that it's cringe-o-clock on your sorry ass.\n\nYou may have also heard a variety of analogies to justify this statement, each more sensible and life-changing than the last: e.g. \"to turn the second gear you have to turn the first\", \"to eat a M&M you eat the chocolate shell before you eat the filling\", \"to microwave a pregnant mother you microwave the mother with the baby inside then microwave the baby\", etc., etc.\n\n> Is this a consequence of the divergence theorem?\n\nHow can this be generalized to more than one dimension?\n\n### The chain rule in 2D\n\nNow consider a function of two variables\n$$\nz = f(x,y)\n$$\nwhere each of $x, y$ are functions of $t$: \n$$\nf(x,y) = f(g(t), h(t)).\n$$\nHow does $f$ change with respect to $t$, i.e. what is $\\frac{df}{dt}$? The **multivariate chain rule** states that\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Multivariate chain rule, Case 1 (functions of one variable)**. For a (scalar function of two variables $z=f(x,y)=f(g(t),h(t))$, \n$$\n\\frac{df}{dt} = \\frac{\\partial f}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial f}{\\partial y}\\frac{dy}{dt}\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThink back to the linear approximation of $f$ near an arbitrary point $x, y$, where the **incremental** $\\Delta z = f(x+\\Delta x, y + \\Delta y) - f(x,y)$ equalled\n$$\n\\Delta z = f_x \\Delta x + f_y \\Delta y + \\epsilon_1 \\Delta x + \\epsilon_2 \\Delta y\n$$\nwith the error terms $\\epsilon_1, \\epsilon_2$ tending to $0$ as $\\Delta x, \\Delta y$ tend to 0, as long as $f$ is differentiable at that point. As $x = g(t)$ and $y = h(t)$ respectively, we write:\n$$\n\\begin{cases}\n\\Delta x = g(t+\\Delta t) - g(t) \\\\\n\\Delta y = h(t+\\Delta t) - h(t)\n\\end{cases}\n$$\nTo obtain $\\frac{df}{dt}$, we divide $\\Delta z$ by $\\Delta t$\n$$\n\\frac{\\Delta z}{\\Delta t} = f_x \\frac{\\Delta x}{\\Delta t} + f_y \\frac{\\Delta y}{\\Delta t} + \\epsilon_1 \\frac{\\Delta x}{\\Delta t} + \\epsilon_2 \\frac{\\Delta y}{\\Delta t}\n$$\nand suppose that $\\Delta t$ tends to $0$, so that $\\frac{\\Delta z}{\\Delta t} \\to \\frac{dz}{dt}$. Then $\\Delta x \\to g(t) - g(t) = 0$ and similarly $\\Delta y \\to h(t) - h(t) = 0$; as such $\\epsilon_1$ and $\\epsilon_2$ both tend to $0$. Therefore\n$$\n\\frac{\\Delta z}{\\Delta t} = f_x \\frac{\\Delta x}{\\Delta t} + f_y \\frac{\\Delta y}{\\Delta t}\n$$\nwithout the error terms, and as\n$$\n\\Delta t \\to 0, \\begin{cases}\n\\frac{\\Delta z}{\\Delta t} = \\frac{dz}{dt} \\\\\n\\frac{\\Delta y}{\\Delta t} = \\frac{dy}{dt} \\\\\n\\frac{\\Delta x}{\\Delta t} = \\frac{ds}{dt}\n\\end{cases}\n$$\nwe finally have\n$$\n\\frac{df}{dt} = \\frac{\\partial f}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial f}{\\partial y}\\frac{dy}{dt}\n$$\nas desired. Note that $\\frac{df}{dt}$ instead of $\\frac{\\partial f}{\\partial t}$ is used because $f$ is, in reality, a univariate function of $t$.\n\n****\n\nWhat if each of $x$, $y$ are now themselves multivariate functions, e.g. $x = g(s,t)$, $y = h(s,t)$? Then we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Multivariate chain rule, Case 2 (functions of multiple variables)**. If $z = f(x,y)$ instead equals $f(g(s,t), h(s,t))$ for some functions of two variables $g$ and $h$, then\n$$\n\\begin{cases}\n\\frac{\\partial z}{\\partial s} = \\frac{\\partial z}{\\partial x} \\frac{\\partial x}{\\partial s} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial s} \\\\\n\\frac{\\partial z}{\\partial t} = \\frac{\\partial z}{\\partial x} \\frac{\\partial x}{\\partial t} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial t}\n\\end{cases}\n$$\nnow using $\\partial$ instead of $d$ because $z$ depends on more than one variable.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> is similar to above.\n\nThis is a classic example of what is known as \"proof by handwaving\". of course, the above can also be generalized to functions of any number of variables:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Generalized multivariate chain rule**. Suppose that $\\mathbf{f(x) = f}(x_1,x_2,...,x_m)$ is a function of $m$ variables $f: \\mathbb{R^m \\to R^n}$, with \n$$\n\\begin{cases}\nx_1 = u_1(t_1, t_2, ..., t_l) \\\\\nx_2 = u_2(t_1, t_2, ..., t_l) \\\\ \n\\vdots \\\\\nx_m = u_m(t_1, t_2, ..., t_l),\n\\end{cases}\n$$\n> i.e. each of the $x_i$ are a function $u_i$ of $l$ variables $t_1, t_2, ..., t_l$. Then for all $i = 1, 2, ..., l$, we have\n$$\n\\frac{\\partial f}{\\partial t_i} = \\sum_{j=1}^m \\frac{\\partial f}{\\partial x_j}\\frac{\\partial x_j}{\\partial t_i}\n$$\n(Note that this is a notational simplification because it doesn't display the variables that are being held constant - e.g. if you wanted to be really, really scrupulous you could write $(\\frac{\\partial f}{\\partial x_1})_{x_2,x_3,...,x_m}$ to demonstrate that $x_1$ is the variable differentiated with respect to and the other variables are constant!)\n\n> <span style=\"background-color: #ffb812; color: black;\">Corollary</span>. The above equation can be written in matrix form as\n$$\nS = TU,\n$$\n> in which $S$ is the $n \\times l$ matrix of partial derivative components of $f$ w.r.t. $t_1, t_2, ..., t_l$:\n$$\nS = \\{(\\frac{\\partial f}{\\partial t_j})_i\\} = \\begin{bmatrix}\n(\\frac{\\partial f}{\\partial t_1})_1 &  (\\frac{\\partial f}{\\partial t_2})_1 & ... & (\\frac{\\partial f}{\\partial t_l})_1 \\\\\n(\\frac{\\partial f}{\\partial t_1})_2 & (\\frac{\\partial f}{\\partial t_2})_2 & ... & (\\frac{\\partial f}{\\partial t_l})_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n(\\frac{\\partial f}{\\partial t_1})_n & (\\frac{\\partial f}{\\partial t_2})_n & ... & (\\frac{\\partial f}{\\partial t_l})_n\n\\end{bmatrix}\n$$\n> $T$ is the $n \\times m$ matrix recalled from the previous section as the matrix of the partial derivative components of $f$ w.r.t. $x_1, x_2, ..., x_m$:\n$$\nT = \\{(\\frac{\\partial f}{\\partial x_j})_i\\} = \\begin{bmatrix}\n(\\frac{\\partial f}{\\partial x_1})_1 &  (\\frac{\\partial f}{\\partial x_2})_1 & ... & (\\frac{\\partial f}{\\partial x_m})_1 \\\\\n(\\frac{\\partial f}{\\partial x_1})_2 & (\\frac{\\partial f}{\\partial x_2})_2 & ... & (\\frac{\\partial f}{\\partial x_m})_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n(\\frac{\\partial f}{\\partial x_1})_n & (\\frac{\\partial f}{\\partial x_2})_n & ... & (\\frac{\\partial f}{\\partial x_m})_n\n\\end{bmatrix}\n$$\n\n> and $U$ is the $m \\times l$ matrix of partial derivatives of $x_1, x_2, ..., x_m$ w.r.t. $t_1, t_2, ..., t_l$:\n\n$$\nU = \\{(\\frac{\\partial x_i}{\\partial t_j})\\} = \\begin{bmatrix}\n\\frac{\\partial x_1}{\\partial t_1} & \\frac{\\partial x_1}{\\partial t_2} & ... & \\frac{\\partial x_1}{\\partial t_l} \\\\\n\\frac{\\partial x_2}{\\partial t_1} & \\frac{\\partial x_2}{\\partial t_2} & ... & \\frac{\\partial x_2}{\\partial t_l} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial x_m}{\\partial t_1} & \\frac{\\partial x_m}{\\partial t_2} & ... & \\frac{\\partial x_m}{\\partial t_l}\n\\end{bmatrix}\n$$\n\nCall these matrix the *Jacobian* matrices of $\\mathbf{f(x)}$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span> of the chain rule applied to a change of coordinates (Cartesian to polar).\n\nSuppose that $z = f(x,y)$ describes the height of a point $(x,y)$ on your bedsheets this morning after I \"befriended\" your mom last night. Because your mom emits both the gravitational and magnetic fields characteristic of an Earth-sized planet, we have decided to convert $(x,y)$ to polar coordinates $(x(\\rho, \\theta), y(\\rho,\\theta))$ (they are more applicable to problems which revolve around a globe). What is the rate of change of $z$ w.r.t. $\\rho$ and $\\theta$?\n\nA notational clarification is required here:\n\n> <span style=\"background-color: #03cafc; color: black;\">Notation</span>. If $f(x,y) = f(x(\\rho, \\theta), y(\\rho, \\theta))$ (or any such combination of variables), $f$ can be written as a function of $\\rho$ and $\\theta$. Thus write $f(x,y) = F(\\rho, \\theta)$.\n\nIn this case, we have $(x,y) = (\\rho \\cos \\theta, \\rho \\sin \\theta)$ by polar coordinates. By the chain rule we have\n$$\n\\begin{aligned}\n(\\frac{\\partial f}{\\partial \\rho})_{\\theta} &= (\\frac{\\partial f}{\\partial x})_y(\\frac{\\partial x}{\\partial \\rho})_\\theta + (\\frac{\\partial f}{\\partial y})_x(\\frac{\\partial y}{\\partial \\rho})_\\theta \\\\\n&= (\\frac{\\partial f}{\\partial x})_y\\cos \\theta + (\\frac{\\partial f}{\\partial y})_x\\sin \\theta \\\\\n\\end{aligned}\n$$\nbeing careful to note which variables are kept constant, and\n$$\n\\begin{aligned}\n(\\frac{\\partial f}{\\partial \\theta})_{\\rho} &= (\\frac{\\partial f}{\\partial x})_y\\frac{\\partial x}{\\partial \\theta} + (\\frac{\\partial f}{\\partial y})_x\\frac{\\partial y}{\\partial \\theta} \\\\\n&= -(\\frac{\\partial f}{\\partial x})_y\\rho \\sin \\theta + (\\frac{\\partial f}{\\partial y})_x\\rho \\cos \\theta \\\\\n&= - y (\\frac{\\partial f}{\\partial x})_y + x (\\frac{\\partial f}{\\partial y})_x\n\\end{aligned}\n$$\nAs $\\rho = \\sqrt{x^2 + y^2}$ and $\\theta = \\tan^{-1}(\\frac{y}{x})$ are both functions of $x$ and $y$, we can also obtain expressions for $\\frac{\\partial f}{\\partial x}_y$ and $\\frac{\\partial f}{\\partial y}_x$ from the chain rule based on $\\rho$ and $\\theta$, e.g.\n$$\n(\\frac{\\partial f}{\\partial x})_y = (\\frac{\\partial f}{\\partial \\rho})_{\\theta} (\\frac{\\partial \\rho}{\\partial x})_y  + (\\frac{\\partial f}{\\partial \\theta})_{\\rho} (\\frac{\\partial \\theta}{\\partial x})_y\n$$\nnoting crucially that $(\\frac{\\partial \\rho}{\\partial x})_y$ **keeps $y$ constant, not $\\theta$**.\n\n","n":0.027}}},{"i":18,"$":{"0":{"v":"Review","n":1},"1":{"v":"> It doesn't make sense, but keep going.\n\n### Vector notation\n\n> <span style=\"background-color: #12ffd7; color: black;\">**Declaration**</span>. The author of these notes will henceforce refer to ourselves by the royal \"we\".\n\nWe play a little fast and loose with vector notation because of the flamboyance of youth. For example, the vector\n$$\n\\begin{bmatrix}\ne_1 \\\\ e_2 \\\\ e_3\n\\end{bmatrix}\n$$\nin $\\mathbb{R}^3$ will be notated $(e_1,e_2,e_3)$ whenever morally, ethically, and legally justifiable; doing this relieves me of my burning, visceral hatred for LaTeX and the societal construct of \\begin{bmatrix} just a little more every time.\n\n> Is this a consequence of the divergence theorem?\n\n**Orthonormal bases** are not to be confused with *orzonormal bases*, which are Italian death camps the disembodied ghost of Mussolini sends you to one femtosecond after committing the cardinal sin of breaking pasta in half, and *oh-so-normal bases*, which are how the mathematical community at large refers to houses, parties, and all regular human settlements.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Orthonormal bases** (and normal bases, and ~~orthogonal bases~~, and perpendicular bases, and inorganic bases, and the strong base I just poured over the corpse in my basement) are a group of vectors which are pairwise normal and have magnitude $1$ each. \n\nIn other words, $\\mathbf{e} = \\{\\mathbf{e_1,e_2,...,e_n}\\}$ is an orthonormal base if and only if\n$$\n\\mathbf{e_i \\cdot e_j} = \\delta_{ij}\n$$\nwhere $\\delta_{ij}$ is the *Kronecker delta symbol*: $\\delta_{ij} = 1$ if $i = j$, $0$ otherwise. Now say your teary goodbyes to \\mathbf{} and bold text, because you're never gonna see it again.\n\n> Is this a consequence of the divergence theorem?\n\nSike! Mathematicians denote vectors with bold text $(\\mathbf{e})$ or with a funny little arrow plus bold text $(\\vec{\\mathbf{e}})$, because it's probably the only place in their lives where they've learned to be bold. I speak from personal experience. \n\nTo confirm that a orthonormal basis $(e_x, e_y, e_z)$ of $\\mathbb{R^3}$ is **right-handed**, you will perform the following Rite of Honors. Take your right hand - your sword hand - and erect your thumb skyward in preparation:\n\n![alt text](./assets/images/image-4.png)\n\nIf your index finger is jabbed towards $e_x$, your middle finger towards $e_y$ (and, God willing, no person of significant import), and your thumb towards $e_z$, you shall have confirmed the right-handedness of $(e_x,e_y,e_z)$, draped yourself in an eternal cloak of dignity, and readied yourself for the formation of the Arch of Honors. ~~Now take your thumb and stick it up your ass sideways.~~ \n\n> We hereby induct the above demonstration into the distinguished ranks of \"things you can do or say in both the math classroom and in bed\", alongside \"this is really hard\", \"it's so infinitesimal I can't even see it\", and moaning really loudly.\n\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Right-handedness and left-handedness**.\n\nSlightly more formally, a basis in $\\mathbb{R}^3$ $(e_1,e_2,e_3)$ is **right-handed** if for $i, j, k$ being some permutation of $1, 2, 3$, we have \n$$\ne_i \\times e_j = \\epsilon_{ijk} e_k\n$$\nwhere $\\epsilon$ is the **alternating tensor**; e.g. $e_1 \\times e_2 = e_3$. To test for right-handedness without doing pesky cross products, simply form the Arch of Honors. Similarly, a basis is left-handed if\n$$\ne_i \\times e_j = -\\epsilon_{ijk} e_k\n$$\nor if an Arch of Honors can be thrusted and plunged in the direction of the basis vectors by the left hand. Note that $e_{ij}e_{ik}$ or some such term with a repeating index ($i$) in the suffixes denotes summation notation, except when it doesn't and interpreting it as so is an alarm bell for a clinical psychopathy diagnosis.\n\n### Basic properties of vector addition and multiplication\n\nIgnored for the sake of brevity. For more information, consult either your kindergarten teacher, your mommy, your second mommy, or this helpful video on vector calculus: \nhttps://www.youtube.com/watch?v=dQw4w9WgXcQ\n\n## Polar coordinates in $\\mathbb{R^3}$\n\n### Cylindrical polar coordinates\n\n![alt text](./assets/images/image-5.png)\n\nA polar point $(r,\\theta,z)$ is where:\n1. $z$ remains $z$ in Cartesian coordinates,\n2. $(r,\\theta)$ are the equivalent polar coordinates of the point $(x,y)$ in two dimensions: \n- $r$ is the distance of $(x,y)$ from the origin (in the two-dimensional $xy$-plane), and \n- $\\theta$ is the angle, measured counterclockwise, from the $x$-axis. \n\nAs such we have\n$$\n(r,\\theta,z) = (r \\cos \\theta, r\\sin \\theta, z)\n$$\nin Cartesian coordinates. True to the name, a cylinder ~~made from organic material (5.1 inches in length, 4.5 inches in girth) stuck in a mini M&Ms tube filled with butter and microwaved mashed banana~~ of radius $k$ is denoted by $r = k$ in cylindrical coordinates.\n\n### Spherical polar coordinates\n\n![alt text](./assets/images/image-6.png)\n\nA spherical point $(r, \\theta, \\phi)$ is where:\n- $r$ is the *three-dimensional* Cartesian distance of the point from the origin;\n- $\\theta$ is the \"latitude\" angle, i.e. the angle formed by the point and the $z$-axis measured clockwise;\n- $\\phi$ is the \"longitude\" angle, i.e. the angle $\\theta$ in cylindrical coordinates.\n\nAs shown by the helpful image above, \n$(r, \\theta, \\phi) = (r\\sin \\theta \\cos \\phi, r\\sin \\theta \\sin \\phi, r\\cos \\phi)$ in Cartesian coordinates. True to form, $r = k$ forms a sphere of radius $k$.\n\nUnfortunately, based on the current literature empirical evidence, $r = k$ in Cartesian coordinates does not summon forth the bodiless apparition of Rene Descartes of length and girth $k$ to the material realm.\n\n> Is this a consequence of the divergence theorem?\n\n## Determinants\n\n> End me now.\n\nFor centuries, pro-Determinant cults (known as \"math classes\") have secretly paraded around in the following names:\n- The Black Death\n- It That Lurks In The Darkness\n- Satan's Hemmorhoids\n- The Antichrist in Human Clothing \n\nThey teach their followers these secret Satanical axioms, disregarding the ruin and doom they bring forth to the world:\n\n$$\n\\det A = \\epsilon_{j_1 j_2 ... j_n}A_{j_1 1} A_{j_2 2} ... A_{j_n n}\n$$\n$$\n\\det(AB) = \\det A \\det B\n$$\n$$\n\\det(A^{-1}) = \\frac{1}{\\det A}\n$$\n> This is not a consequence of the divergence theorem.","n":0.033}}},{"i":19,"$":{"0":{"v":"Partial Derivatives","n":0.707},"1":{"v":"\n\n> Why do mathematicians want to investigate functions of more than two variables when the paper they use is two-dimensional? Are they stupid?\n\n## Defining partial derivatives\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Partial derivatives** of a multivariate function $f(\\mathbf{x}) = f(x_1,x_2, ..., x_n)$ are derivatives **with respect to** a single variable $x_i$ holding **every other variable constant**, but you're a smart cookie, so you knew that already. More precisely, denote this derivative as\n$$\n(\\frac{\\partial f}{\\partial x_i})_{x_1, x_2, ..., \\bar{x_i}, ..., x_n}\n$$\n> where $\\partial x_i$ indicates differentiation with respect to $x_i$, with the little curly $d = \\partial$ (joining the ranks of things that are objectively and indisputably better when curly, of which include hair, pigs' tails, and McDonalds' french fries), and the subscript $x_1, x_2, ..., \\bar{x_i}, x_n$ represents the variables kept constant (in this case, every variable except $x_i$). \n\nPartial derivatives thus represent the rate of change of a multivariate function with respect to one variable being changed only. \nThe above derivative can be more rigorously defined as the limit\n$$\n\\lim_{h \\to 0}\\frac{f(x_1, x_2, ..., x_i + h, ..., x_n) - f(x_1, x_2, ..., x_i, ..., x_n)}{h}\n$$\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. There are several alternative ways to denote partial derivatives, including $f_{x_i}$, $D_{x_i}f$, $\\partial_{x_i}f$, $\\partial_i f$, and the sound of a calculus student quietly shuffling off to the corner of the classroom for a half-hour cry session in the middle of doing their homework. \n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. Of these alternative notations, one is acceptable, one is slightly less (but still somewhat) acceptable, and the rest are eligible for the death penalty in 126 countries, crimes against humanity charges via Clause 26.8 of the Geneva Convention, and an automatic full score on the Dark Triad quiz (similar to wearing socks before pants, not liking pizza crusts, cutting a brownie from the center out, and being a Linux user).\n\nWe will assume you aren't exactly Mr. Norman Newbie or Ms. Amanda Amateur when it comes to calculus, and also that if you want to hear offensively terrible jokes screamed at you with all the force and maturity of a puberty-aged teenager during a raging mood swing you can go follow Elon Musk's twitter account, so let's get down to more serious business:\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. FInd $\\frac{\\partial}{\\partial x} x$.\n\n$$\n1.\n$$\n\nFeel free to take a deep breath after that arduous intellectual workout. \n\nAs is evident from the above example, partial derivatives are far more difficult than derivatives in one variable, and so in my professional opinion, we have to approach them with a substantially more well-thought-out plan of attack, and with far more finess and expertise:\n1. Steal a bunch of paper from your nearest professor's desk, preferably without being caught (even better if toilet paper).\n1. Sharpen three Faber-Castell 2B acrylic pencils$^{TM}$ in different colors, or two, or five (one for each variable)\n2. Write down your function with each variable being carefully color-coded.\n3. Go scream and cry in a corner somewhere for 30 minutes.\n4. Repeat step 3.\n5. Sharpen the pencils again.\n6. Stab yourself with the sharpened pencils because it will be a better experience than spending even another nanosecond on this partial derivative business.\n\n## Directional derivatives\n\nDirectional derivatives, as it turns out, **have** been given a thorough treatment in previous courses: they measure the rate of change of a multivariate function when going in the direction of a certain unit vector $\\hat{u}$ (*if you're going up a mountain at a 10-degree-angle, or at a 45-degree-angle, or going from this rock to that rock, how fast are you climbing the mountain?*). \n\nMore formally, we can define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. With respect to a unit vector $\\hat{u} \\in \\mathbb{R}^n$, define the **directional derivative** of a multivariate function $\\mathbf{f(x)}: \\mathbb{R^n \\to R^m}$ to be\n$$\n\\lim_{h \\to 0}\\frac{f(\\mathbf{x+}h\\hat{u}) - f(\\mathbf{x})}{h}\n$$\n> if the limit exists, i.e. the rate of change of $f$ when going up a tiny little step in the direction $\\hat{u}$. It can also be measured by\n$$\n\\nabla f \\cdot \\hat{u}\n$$\n> where $\\nabla$ denotes the gradient (more on this later)!\n\n## Differentiability\n\nCan a function be locally approximated by a linear function? In the univariate case, we call a function *differentiable* if the answer to that question is yes - its tangent line at a point best approximates it linearly at that point. In the multivariate case, this idea is expanded to a *tangent plane* (or a tangent cube, which is quite simply one of the most fantastic combinations of words I've ever encountered); however, we will restrict our three-dimensional minds to the simple case of a two-variable function $f(x,y) = z$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> Suppose that at a point $(x_0, y_0, z_0)$, $T_x, T_y$ are the tangent lines to $f$ at that point with respect to the $x$ and $y$-directions respectively (keeping the other direction constant). Then the **tangent plane** to $f$ at $(x_0,y_0)$ is the plane that is formed from the intersection of $T_x$ and $T_y$.\n\nEssentially: the tangent plane as defined above contains the tangent line in all possible directions (and not just the $x$-axis and the $y$-axis!) As the gradients to the tangent lines in the direction of each variable $T_x$ and $T_y$ are given by $f_x(x_0,y_0)$ and $f_y(x_0,y_0)$, we have\n$$\nz - z_0 = f_x(x_0, y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0)\n$$\nas the tangent plane passes through $(x_0,y_0,z_0)$. The tangent line is the **best possible** linear approximation to the function $f(x,y)$ at the point $(x_0,y_0,z_0)$; both it and its first derivative equal $f$'s at that point, and the more we zoom in, the more $f$ resembles its tangent plane.\n\nSome functions, however, are naughty and do not behave like their tangent planes imply. Take, from the last section, the example\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>.\n\n$$\nf(x,y) = \\begin{cases}\n\\frac{2xy}{x^2+y^2}, x \\neq 0\\\\\n0, x = 0\n\\end{cases}\n$$\nwhere the partial derivatives $f_x$ and $f_y$ both exist at $(x,y) = (0,0)$, and are both zero. Previously, we verified that the limit of this function does not exist at the point $(0,0)$ because approaching it from different directions give different results; this would imply that the tangent plane does not well-approximate its behavior at $(0,0)$ because it has different behaviors in every direction. Indeed,\n$$\nz = 0\n$$\nis the tangent plane at $(0,0)$; yet if we approach $(0,0)$ along the line $y = x$ we instead have $f(x,y) \\to \\frac{1}{2}$. Call such poorly-behaved functions in multiple variables the same thing we call poorly-behaved functions in one variable: ~~children in need of capital punishment~~ **nondifferentiable functions**.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Differentiable functions** in multiple variables (in this case, two) at a certain point $(x_0,y_0)$ are functions whose tangent plane well-approximates them at that point.\n\nIt is lucky indeed that this definition gives rise to another even more elaborate and contrived definition, so let us offer it. Suppose that the **increment** $\\Delta z$ of $z = f(x,y)$ near $(x_0, y_0)$ is given by\n$$\n\\Delta z = f(x_0 + \\Delta x, y_0 + \\Delta y) - f(x_0, y_0)\n$$\nfor very small $\\Delta x$ and $\\Delta y$. Then if $f(x_0,y_0)$ is **well-approximated** by its tangent plane at that point, we say that\n$$\n\\Delta z = f_x(x_0, y_0) \\Delta x + f_y(x_0, y_0)\\Delta y + \\epsilon_x \\Delta x + \\epsilon_y \\Delta y\n$$\nwhere $\\epsilon_x, \\epsilon_y \\to 0$ as $\\Delta x, \\Delta y \\to 0$. Essentially, for very small changes to $x$ and $y$, $f(x,y)=z$ beehaves exactly like the tangent plane.\n\nThis definition can be generalized to functions of any domain and range:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Suppose that $\\mathbf{f(x)} = \\mathbf{f}(x_1, x_2, ..., x_m)$ is a function $\\mathbb{R^m \\to R^n}$, and let $\\Delta \\mathbf{f}$ be defined as \n$$\n\\mathbf{f(x+\\Delta x) - f(x)}\n$$\n> as above. Then if $\\Delta\\mathbf{f}$ can be approximated linearly by\n$$\n\\Delta \\mathbf{f} = T \\Delta \\mathbf{x} \n$$\n> when $\\Delta \\mathbf{x}$ is small (i.e. the error term $\\epsilon(\\Delta \\mathbf{x})$ tends to zero), where $\\Delta \\mathbf{x}$ is\n$$\n\\begin{bmatrix}\n\\Delta x_1 \\\\\n\\Delta x_2 \\\\\n\\vdots \\\\\n\\Delta x_m\n\\end{bmatrix}\n$$\n> and $T$ is the $n \\times m$ matrix\n$$\n\\begin{bmatrix}\n(\\frac{\\partial f}{\\partial x_1})_1 &  (\\frac{\\partial f}{\\partial x_2})_1 & ... & (\\frac{\\partial f}{\\partial x_m})_1 \\\\\n(\\frac{\\partial f}{\\partial x_1})_2 & (\\frac{\\partial f}{\\partial x_2})_2 & ... & (\\frac{\\partial f}{\\partial x_m})_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n(\\frac{\\partial f}{\\partial x_1})_n & (\\frac{\\partial f}{\\partial x_2})_n & ... & (\\frac{\\partial f}{\\partial x_m})_n\n\\end{bmatrix}\n$$\n> which is independent of $\\Delta \\mathbf{x}$, then $\\mathbf{f(x)}$ is **differentiable** at $\\mathbf{x} = (x_1, x_2, ..., x_m)$.\n\nThe above matrix looks extremely messy, but closer inspection reveals that \n$$\n\\Delta \\mathbf{f} = T \\Delta \\mathbf{x} \n$$\nresults in\n$$\n\\Delta \\mathbf{f} = \n\\begin{bmatrix}\n(\\frac{\\partial f}{\\partial x_i})_1\\Delta x_i \\\\\n(\\frac{\\partial f}{\\partial x_i})_2 \\Delta x_i \\\\\n\\vdots \\\\\n(\\frac{\\partial f}{\\partial x_i})_n \\Delta x_i \\\\\n\\end{bmatrix} = \\vec{(\\frac{\\partial f}{\\partial x_i})}\\Delta x_i\n$$\nusing summation notation, i.e. $\\Delta \\mathbf{f}$ is still the sum of the products of $\\frac{\\partial f}{\\partial x_i}$ and $\\Delta x_i$ except for the fact that each partial derivative is now a vector instead of a scalar.\n\nAn extremely effective **sufficient condition** for differentiability, eliminating the need to dance with limits and frolick in the blood-crazed fields of $\\epsilon-\\delta$, is\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. If the partial derivatives of $\\mathbf{f(x)}: \\mathbb{R^m \\to R^n}$ exist near a point $\\mathbf{x} = (x_1, x_2, ..., x_m)$ and are continuous at that point, then $\\mathbf{f(x)}$ is differentiable at that point.\n","n":0.026}}},{"i":20,"$":{"0":{"v":"Limits and Continuity","n":0.577},"1":{"v":"> Course notes for AP Calculus KMS.\n\n## Scalar functions of one variable\n\nSuppose $f(x)$ is a real function of $x \\in R$ (as opposed to a fake function, which is three real functions stacked together in a trench coat.)\n\n### Limits\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Limits**. $l$ is the limit of $f(x)$ as $x \\to a$ if \n$$\n\\lim_{x\\to a} f(x) = l\n$$\nWow. Who's writing these notes, Sherlock Holmes?\n\n> <span style=\"background-color: #ffb812; color: black;\">Remarks</span>.\n\n1. Denote the **left limit** at $x=a$ as $\\lim_{x \\to a^-}f(x)$; it is the value $f(x)$ approaches at $x=a$ **from the left** of $a$ ($x = a - \\epsilon$ for a very small $\\epsilon$)\n2. Denote the **right limit** at $x=a$ as $\\lim_{x \\to a^+}f(x)$; it is the value $f(x)$ approaches at $x=a$ **from the right** of $a$ ($x = a + \\epsilon$ for a very small $\\epsilon$)\n3. The left limit may not equal the right limit. For example, $\\arctan x$ at $x = 0$. For another example, the speed limit of driving up Mount Everest versus the speed limit of driving down Mount Everest.\n4. If the left limit does not equal the right limit, the limit $\\lim_{x\\to a} f(x) = l$ **does not exist**.\n\n### Continuity\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. $f(x)$ is **continuous** at a point $a \\in \\mathbb{R}$ if and only if $\\lim_{x\\to a}f(x) = f(a)$, i.e. both that the limit exists and that the limit equals the function at that point.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. This definition of continuity excludes piecewise functions like\n\n$$\nf(x) = \\begin{cases}\n1, x\\neq a \\\\\n0, x = a\n\\end{cases}\n$$\n> where the limit exists at $x = a$, but the limit ($\\lim_{x\\to a} f(x) = 1$) does not equal the function itself ($f(a) = 0$) at that point.\n\n### Differentiability\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. $f(x)$ is **differentiable** at $x=a$ with derivative $f'(a)$ if\n$$\n\\lim_{x\\to a}\\frac{f(x) - f(a)}{x-a} = f'(a).\n$$\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. If $f(x)$ is differentiable at $x=a$, it is continuous at $x=a$. This can be seen by multiplying $x-a$ in the limit on both sides.\n\n### Taylor's theorem\n\nTaylor's theorem ranks just barely in the top ten of Taylor's most significant contributions towards humanity, right behind \"Firework\" and, admittedly, far ahead of \"Shake It Off\".\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Taylor's theorem**. For a function $f$ differentiable $N+1$ times, \n$$\nf(a+h) = f(a) + hf'(a) + \\frac{h^2}{2!}f''(a) + ... + \\frac{h^n}{n!}f^{(n)}(a) + O(h^{n+1})\n$$\n> or\n$$\nf(a+h) = \\sum_{k=0}^N \\frac{h^k}{k!}f^{(k)}(a) + R_N (h)\n$$\n> where $R_N(h)$ is the $N+1$th-degree polynomial\n$$\nR_N(h)=\\frac{h^{N+1}}{(N+1)!}f^{(N+1)}(\\xi),\n$$\n> with $a < \\xi < a+h$.  In terms of big-O notation, we can also write\n$$\nf(a+h) = \\sum_{k=0}^N \\frac{h^k}{k!} f^{(k)}(a) + O(h^{N+1}) \n$$\n> for very small $h$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Corollary 1.</span>.\nSetting $a = h$ and $h = x-h$ and in the above equation yields\n$$\nf(x) = \\sum_{k=0}^N \\frac{(x-h)^k}{k!}f^{(k)}(h) + O((x-h)^{N+1}).\n$$\n> If $f$ is infinitely differentiable, we instead have\n$$\nf(x) = \\sum_{k=0}^{\\infty} \\frac{(x-h)^k}{k!}f^{(k)}(h)\n$$\n> Call this the **Taylor expansion of f(x)** about the point $x=h$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Corollary 2</span>. \nIf instead we have $a = 0$ and $h = x$, we obtain the **Maclaurin series** of $f(x)$ (a Taylor expansion about $x=0$):\n\n$$\nf(x) = \\sum_{k=0}^{\\infty} \\frac{x^k}{k!}f^{(k)}(0)\n$$\n> for an infinitely differentiable $f(x)$. These expansions are only valid for a certain **radius of convergence** about the point they are centered upon; this notion will be expanded on later.\n\n\n\n## Functions of several variables\n\nFunctions of several variables take vectors as either their input or output; i.e. they map from $\\mathbb{R}^n$ to $\\mathbb{R}^m$, where $n$ and $m$ can be any natural number. For instance,\n$$\nf(x,y) = k,\\ k \\in \\mathbb{R}\n$$\nmaps a vector $(x,y) \\in \\mathbb{R^2}$ to $\\mathbb{R}$; alternatively,\n$$\nf(x) = (x, x^2)\n$$\nmaps a real number $x \\in \\mathbb{R}$ to a vector in $\\mathbb{R^2}$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Call the subset of $\\mathbb{R}^n$ $f$ is well-defined on the **domain** of $f$.\n\n### Limits\n\nWith truly astounding mathematical insight and ingenuity, we define the vector or scalar $\\mathbf{l}$ in\n$$\n\\lim_{\\mathbf{x\\to a}}f(\\mathbf{x}) = \\mathbf{l}\n$$\nto be the limit of $f$ as $\\mathbf{x}$ approaches $\\mathbf{a}$, **if and only if** such a limit is independent of the way $\\mathbf{x}$ approaches $\\mathbf{a}$. This is analogous to a limit existing in 2D if and only if it is the same when approahced from left and right; because higher dimensions have more directions than just left and right, we stipulate that the limit must now be the same when approached from every possible direction. \n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Consider the function $f(x,y)$ mapping from $\\mathbb{R^2 \\to R}$\n$$\nf(x,y) = \\begin{cases}\n\\frac{2xy}{x^2+y^2}, x \\neq 0\\\\\n0, x = 0\n\\end{cases}\n$$\n> Suppose we are moving on a line parametrized by $(x,y) = (\\rho \\cos \\theta, \\rho \\sin \\theta)$ for a constant $\\theta$ (not a constant $\\rho$!). Then\n$$\nf(x,y) = \\frac{2\\rho^2 \\sin \\theta \\cos \\theta}{\\rho^2} = \\sin 2\\theta \n$$\n> for all points except $\\rho = 0$. As a result we write\n$$\n\\lim_{\\rho \\to 0}f(x,y) = \\lim_{(x,y)\\to \\mathbf{0}}f(x,y) = \\sin 2\\theta\n$$\n> which need not equal zero. As such, $f(x,y)$ has no well-defined limit as $(x,y) \\to \\mathbf{0}$.\n\nAn alternate *epsilon-delta* definition is as follows:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Epsilon-delta definition of limits).  The limit of $f(\\mathbf{x})$ as $\\mathbf{x\\to a}$, denoted\n$$\n\\lim_{\\mathbf{x\\to a}}f(\\mathbf{x})\n$$\n> is defined to be $L$ if for every $\\epsilon > 0$ there is a corresponding $\\delta$ such that for every point $\\mathbf{x}_0 \\neq \\mathbf{a}$ within the domain of $f$ and a distance $D$ away from $\\mathbf{a}$, where $0 < D < \\delta$, \n$$\n|f(\\mathbf{x_0}) - L| < \\epsilon.\n$$\n\nThis infamous statement needs more than just a bit of clarification, so here goes! \n\nThe limit of $f$ as $\\mathbf{x} \\to \\mathbf{a}$ is the value $f$ approaches as as it gets infinitely close to (but does not fully equal) $\\mathbf{a}$. $\\mathbf{x_0}$ is a point within the domain of $f$; $D$ is the distance between $\\mathbf{x_0}$ and the point $\\mathbf{a}$, always larger than zero (meaning that $\\mathbf{x_0\\neq a}$) but smaller than a certain $\\delta$. \n\nThus, the above definition states that for **any** $\\epsilon > 0$ (meaning infinitesimally small), we are able to find a $\\delta > 0$ such that every point $x_0$ a distance $0 < D < \\delta$ from $\\mathbf{a}$ satisfies\n$$\n|f(\\mathbf{x_0}) - L| < \\epsilon,\n$$\ni.e. $f(\\mathbf{x_0})$ can get infinitely close (as $\\epsilon$ can be any value) to $f(\\mathbf{a})$, from any direction, and not actually equal it.\n\n\n### Continuity\n\nAs before, consider $f(\\mathbf{x})$ continuous at $\\mathbf{a}$ if\n$$\n\\lim_{\\mathbf{x} \\to \\mathbf{a}} f(\\mathbf{x}) = f(\\mathbf{a})\n$$\nNoting that\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. Suppose that $\\mathbf{a} \\in \\mathbb{R^n} = (a_1, a_2, ..., a_n)$. Then it is **not sufficient** for \n$$\n\\lim_{x_i \\to a_i} f(a_1, a_2, ..., x_i, ..., a_n) = f(a_1, ..., a_n); \n$$\n> for all $i = 1,...,n$; i.e. it is not sufficient for the limit to exist when every variable except $x_i$ is held constant and $x_i$ approaches $a_i$. This is because, as mentioned above, the limit must be the same **from every direction** (and not just the direction of the variables $x_1, ..., x_n$!)\n\n## Derivatives of vector-valued functions\n\nLet $\\mathbf{f}(x)$ be a **vector-valued** function, i.e. it takes a scalar $x\\in \\mathbb{R}$ and maps it to a vector in $\\mathbb{R^n}$. Define the **derivative** of $\\mathbf{f}(x)$ at $x=a$ as\n$$\n\\mathbf{f}'(a) = \\lim_{x \\to a}\\frac{\\mathbf{f}(x) - \\mathbf{f}(a)}{x - a}\n$$\nwhere we can likewise say that $\\mathbf{f}$ is differentiable at $x=a$ if this limit exists. If we write\n$$\n\\mathbf{f}(a) = \n\\begin{bmatrix}\n\\mathbf{f}_1(a) \\\\ \\mathbf{f}_2 (a) \\\\ \\vdots \\\\ \\mathbf{f}_n (a)\n\\end{bmatrix}\n$$\nnotwithstanding my eternal and inextinguishable vitriol towards \\begin{bmatrix}, i.e. representing $\\mathbf{f}$ as its vector components, then it follows that\n$$\n\\mathbf{f}'(a) = \\begin{bmatrix}\n\\mathbf{f}'_1(a) \\\\ \\mathbf{f}'_2 (a) \\\\ \\vdots \\\\ \\mathbf{f}'_n (a)\n\\end{bmatrix}\n$$\nwhere each of the $\\mathbf{f}_k(a)$ are univariate functions ($\\mathbb{R\\to R}$).\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. The velocity of a particle with time-varying **position** given by $\\mathbf{x}(t)$ at time $t$ is defined to be \n$$\n\\mathbf{u}(t) = \\frac{d\\mathbf{x}}{dt}.\n$$\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Let $\\mathbf{u}(t)$ be a vector-valued function, and $g(t) = \\mathbf{u}(t) \\cdot \\mathbf{u}(t)$, i.e. the magnitude of $\\mathbf{u}$. We thus have\n$$\n\\frac{dg}{dt} = \\frac{d}{dt}u_i^2\n$$\n> by the summation convetion, and\n$$\n\\frac{d}{dt}u_i^2 = 2u_i \\frac{d u_i}{dt} = 2\\mathbf{u}\\cdot \\frac{d\\mathbf{u}}{dt}\n$$\n> rewriting in vector form. If we further specify that $\\mathbf{u}(t)\\cdot \\mathbf{u}(t) = |\\mathbf{u}(t)| = 1$, then $\\frac{dg}{dt} = 0$, implying\n$$\n\\frac{dg}{dt} = 2 \\mathbf{u}\\cdot \\frac{d\\mathbf{u}}{dt} = 0\n$$\n> and that $\\mathbf{u}$ is perpendicular to its time derivative.\n\n\n","n":0.028}}},{"i":21,"$":{"0":{"v":"Higher-Order Partial Derivatives","n":0.577},"1":{"v":"> What does a mathematician and the order of the differential equation they're studying have in common? They're both high.\n\n## Higher-order partial derivatives\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>.\n**Higher-order partial derivatives** are partial derivatives of lower-order partial derivatives. For instance, a **second-order** partial derivative for a function $f(x, y)$ could be\n\n$$\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial x}\\frac{\\partial }{\\partial y}f(x,y);\n$$\n> Denote the above as $f_{xy}$, and more generally, any sequence of partial derivatives, in order, as $f_{x_ix_j ...}$ for variables $x_1, ..., x_m$. For a function of two variables $f(x,y)$, if the second-order derivative $f_{x_i x_j}$ satisfies $x_i \\neq x_j$, then the derivative is called a **mixed** second-order partial derivative.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The mixed second-order partial derivatives of $f(x,y)$, $f_{xy}$ and $f_{yx}$, are equal at a point $(x_0, y_0)$ **if and only if** at least one of $f_{xy}$ and $f_{yx}$ are continuous at $(x_0, y_0)$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Trivial.\n\nThis is a consequence of the Divergence Theorem.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Change of variables for second-order partial derivatives.\n\n\nSuppose that, after I \"befriended\" your mom last night, your dad comes home with the milk and intimately acquaints me with the barrel of his shotgun. Now my escape velocity as I run to a point $(x,y)$ in your backyard is given by $z = f(x,y)$. We want to convert this to pole coordinates $x = \\rho \\cos \\theta$, $y = \\rho \\sin \\theta$. As previously shown, our first-order partial derivatives were\n\n$$\n\\begin{aligned}\n(\\frac{\\partial f}{\\partial \\rho})_{\\theta} &= (\\frac{\\partial f}{\\partial x})_y(\\frac{\\partial x}{\\partial \\rho})_\\theta + (\\frac{\\partial f}{\\partial y})_x(\\frac{\\partial y}{\\partial \\rho})_\\theta \\\\\n&= (\\frac{\\partial f}{\\partial x})_y\\cos \\theta + (\\frac{\\partial f}{\\partial y})_x\\sin \\theta \\\\\n\\end{aligned}\n$$\nand\n$$\n\\begin{aligned}\n(\\frac{\\partial f}{\\partial \\theta})_{\\rho} &= (\\frac{\\partial f}{\\partial x})_y\\frac{\\partial x}{\\partial \\theta} + (\\frac{\\partial f}{\\partial y})_x\\frac{\\partial y}{\\partial \\theta} \\\\\n&= -(\\frac{\\partial f}{\\partial x})_y\\rho \\sin \\theta + (\\frac{\\partial f}{\\partial y})_x\\rho \\cos \\theta \\\\\n&= - y (\\frac{\\partial f}{\\partial x})_y + x (\\frac{\\partial f}{\\partial y})_x\n\\end{aligned}\n$$\nso what are the second-order partial derivatives - for instance, $\\frac{\\partial^2 f}{\\partial \\rho\\ \\partial \\theta}$? We have\n$$\n\\begin{aligned}\n\\frac{\\partial^2 f}{\\partial \\rho\\ \\partial \\theta} &= \\frac{\\partial}{\\partial \\rho}(\\frac{\\partial f}{\\partial \\theta})_{\\rho} \\\\\n&= \\frac{\\partial}{\\partial \\rho} (- y (\\frac{\\partial f}{\\partial x})_y + x (\\frac{\\partial f}{\\partial y})_x) \\\\\n\\end{aligned}\n$$\nwhere the operator $\\frac{\\partial}{\\partial \\rho}$ can itself be rewritten\n$$\n\\frac{\\partial}{\\partial \\rho} = (\\frac{\\partial}{\\partial x})_y\\cos \\theta + (\\frac{\\partial}{\\partial y})_x\\sin \\theta \n$$\nallowing the above to be written completely in terms of $x$ and $y$.\n\n## Taylor's theorem (multivariate case)\n\nRecall **Taylor's theorem** in the single-variable case (to reiterate, only barely one of Taylor's ten greatest contributions to mankind, above \"Cruel Summer\" and far below \"Exile\"):\n\n$$\nf(x+h) = f(x) + \\sum_{k=1}^N \\frac{h^k}{k!} f^{(k)}(x) + R_N(h)\n$$\nfor a $N+1$-time differentiable function $x$, with \n$$\nR_N(h) = \\frac{h^{k+1}}{(k+1)!}f^{(k+1)}(\\xi)\n$$\nfor $x < \\xi < x+h$. The generalized version of this theorem to multiple variables states:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Taylor's theorem for multivariate functions**. Let $f$ be a function $\\mathbb{R}^m \\to \\mathbb{R}$, and let $\\mathbf{a}$ be a point in the domain of $f$. Suppose that the partial derivatives of $f$ up to the $N+1$th-order are all continuous at $\\mathbf{a}$. Then for some $\\mathbf{h} \\in \\mathbb{R}^m$,\n$$\nf(\\mathbf{a+h}) = f(\\mathbf{a}) + \\sum_{k=1}^N  \\frac{\\mathbf{(h \\cdot \\nabla})^k}{k!}f(\\mathbf{a}) + R_N(\\mathbf{h})\n$$\nwhere\n$$\nR_N(\\mathbf{h}) = \\frac{\\mathbf{(h \\cdot \\nabla)}^{N+1}}{(N+1)!}f(\\mathbf{a + \\theta h})\n$$\nfor $0 < \\theta < 1$, and the operator $\\mathbf{(h \\cdot \\nabla)}$ applied to $f$ denotes the sum\n$$\n\\mathbf{(h \\cdot \\nabla)}f = \\mathbf{h}_i \\frac{\\partial f}{\\partial x_i}\n$$\nusing summation convention.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. By the summation convention, the operator\n$$\n\\mathbf{(h \\cdot \\nabla)}^{k}\n$$\n> for $k \\in \\mathbb{N}$ can be rewritten\n$$\n(\\mathbf{h}_{j_1} \\frac{\\partial }{\\partial x_{j_1}})(\\mathbf{h}_{j_2} \\frac{\\partial }{\\partial x_{j_2}})...(\\mathbf{h}_{j_k} \\frac{\\partial }{\\partial x_{j_k}})\n$$\n> equalling\n$$\n\\mathbf{h}_{j_1} \\mathbf{h}_{j_2}...\\mathbf{h}_{j_n}\\frac{\\partial}{\\partial x_{j_1}}\\frac{\\partial}{\\partial x_{j_2}}...\\frac{\\partial}{\\partial x_{j_n}}.\n$$\nwhere the order of the partial derivatives does not matter, by means of partial derivatives commuting if they are continuous at $\\mathbf{a}$.\n\nTaylor's theorem also holds for vector-valued functions, where the above statement is applied *component-wise*:\n\n$$\nf_i(\\mathbf{a+h}) = f_i(\\mathbf{a}) + \\sum_{k=1}^N  \\frac{\\mathbf{(h \\cdot \\nabla})^k}{k!}f_i(\\mathbf{a}) + R_N(\\mathbf{h})_i\n$$\n\nfor $i = 1, 2, ..., n$. \n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Suppose that \n$$\nf(\\mathbf{x}): \\mathbb{R^3 \\to R} = \\frac{1}{|\\mathbf{x}|} = \\frac{1}{\\sqrt{x_1^2 + x_2^2 + x_3^2}}. \n$$\n\n> Let $|\\mathbf{x}| = r.$ Then for some $\\mathbf{h} \\in \\mathbb{R}^3$, we have\n$$\n\\begin{aligned}\n(\\mathbf{h \\cdot \\nabla})f &= h_1f_{x_1} + h_2f_{x_2} + h_3 f_{x_3} \\\\\n&= -h_1\\frac{x_1}{r^3} -h_2\\frac{x_2}{r^3}-h_3\\frac{x_3}{r^3}\n\\end{aligned}\n$$\nand so on.\n","n":0.039}}},{"i":22,"$":{"0":{"v":"Gradients","n":1},"1":{"v":"> Gradients are denoted by $\\nabla$, pronounced \"del\" or \"nabla\" because they look like an ancient harp-like Hebrew instrument with the same shape. This is astonishing, as the list of $\\nabla$-shaped objects has length $N >> 1$ and some of them roll off the tongue much better, like \"Huell Babineaux's pointy head hanging upside-down on a yoga mat\" and \"Danny DeVito's humongous dumptruck\". \n\n## The Gradient Vector\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **gradient** of a multivariate scalar function $f(x_1, x_2, ..., x_m): \\mathbb{R^m \\to R}$, denoted $\\nabla f$ and pronounced ~~REDACTED~~, is defined as the vector\n$$\n\\nabla f = \\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\\n\\frac{\\partial f}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_n}\n\\end{bmatrix}\n$$\n> i.e. a vector whose components are the partial derivatives of $f$ in sequence. \n\n> <span style=\"background-color: #ffb812; color: black;\">Properties</span>. From the properties of (partial) differentiation, and if $f$ and $g$ are both functions mapping from $\\mathbb{R^m \\to R}$.n:\n1. $\\nabla(\\lambda f + \\mu g) = \\lambda \\nabla f + \\mu \\nabla g$;\n2. $\\nabla(fg) = f\\nabla g + g \\nabla f.$\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The directional derivative of $f$ in the direction of the unit vector $\\hat{u}$ is given by the dot product between the gradient vector and $\\hat{u}$, $(\\nabla f) \\cdot \\hat{u}$. Simultaneously, the **direction of steepest ascent** - the direction where the function increases the most - is given by the gradient vector itself.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nRecall that the directional derivative of $f$ in the direction of $\\hat{u}$ is the change of $f$ when taking an infinitesimally small step in the direction $\\hat{u}$. Thus if \n$$\n\\hat{u} = \\begin{bmatrix}\nu_1 \\\\ u_2 \\\\ \\vdots \\\\ u_m\n\\end{bmatrix}\n$$\ni.e. $u_1$ steps in direction $x_1$, $u_2$ in $x_2$, etc., and the rate of change in each of these directions are given by $\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}$, etc., we have\n$$\nf'(\\mathbf{x}; \\hat{u}) = u_1 \\frac{\\partial f}{\\partial x_1} + u_2 \\frac{\\partial f}{\\partial x_2} + ... + u_m \\frac{\\partial f}{\\partial x_m} = \\nabla{f} \\cdot \\hat{u}.\n$$\nWhat is the maximum value of such a directional derivative, i.e. what direction does $f$ increase the quickest in? $\\nabla{f}\\cdot \\hat{u}$ has magnitude at most $\\nabla{f}$ as $\\hat{u}$ is a unit vector, and this maximum is achieved when $\\nabla{f}$ and $\\hat{u}$ are in the same direction (the cosine of the angle in-between them is $1$). Thus, when you travel in the direction of the gradient vector, you ascend the function $f$ quickest. \n\n","n":0.05}}},{"i":23,"$":{"0":{"v":"Multiple Integrals","n":0.707},"1":{"v":"\n> This is getting out of hand. Now there are two of them!<br/>\n-- **Darth $\\int$idious**","n":0.258}}},{"i":24,"$":{"0":{"v":"Volume Integrals","n":0.707},"1":{"v":"> Why don't they just throw it into a bathtub and see how much the water goes \\*fooosh\\*? Are they stupid?\n\n## Generalizing the area integral\n\nWe know the drill. Just as a single integral is formed over a single Riemann sum, and an area (double) integral is formed over a double Riemann sum, a volume integral - just to remind you, we're now at three - is formed over a triple Riemann sum, based off the **volume differential**\n$$\ndV = dx\\ dy\\ dz.\n$$\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **volume integral** over a region $V$ in 3D space of a scalar function $\\phi(\\mathbf{x}): \\mathbb{R^3 \\to R}$, denoted $\\int_V \\phi\\ dV$, is the limiting value of the sum\n$$\n\\lim_{\\delta V \\to 0}\\sum_{i=1}^n \\phi(\\mathbf{x}_i)\\ \\delta V = \\lim_{\\delta x,\\ \\delta y,\\ \\delta z \\to 0}\\sum_{i=1}^l \\sum_{j=1}^m \\sum_{k=1}^n \\phi(\\mathbf{x}_{ijk})\\ \\delta x\\ \\delta y\\ \\delta z\n$$\n> if it exists, where each $dV = \\delta x\\ \\delta y\\ \\delta z$ can be interpreted as an infinitesimal cuboid in space, of which there are $lmn$ in total.\n\nThis may also be denoted $\\int\\int\\int dV$, which to my mind looks seven gazillion times cooler, but - to quote the text I'm basing these notes upon -\n> \"Some texts prefer... a conservation of integral signs and so write area integrals as $\\int \\int dA$ and volume integrals as $\\int \\int \\int dV$. The authors of these texts aren't string theorists and have never had to perform an integral in ten dimensions.\"\n\nThis is where the \"eldritch horror\" side of math really shines through. \n\nGiven the above formulation, we can also re-interpret the volume integral as the following iterated integral:\n$$\n\\int_{z_a}^{z_b}\\int_{y_a}^{y_b}\\int_{x_a}^{x_b}\\phi(x,y,z)\\ dx\\ dy\\ dz\n$$\nwhere the order of integration is still irrelevant as long as $\\phi$ satisfies the usual suspects (smoothness, continuity, springs back when poked, elastic and stretchy, good gluten formation etc. etc.) \n\nAs with non-rectangular regions in 2D, volume integrals may be carried out over non-cuboid regions through allowing $x_a = x_a(y,z), x_b=x_b(y,z)$ to be functions of $y$ and $z$ and $y_a = y_a(z), y_b = y_b(z)$ to be functions of $z$, keeping $z_a$ and $z_b$ constant:\n$$\n\\int_{z_a}^{z_b}\\int_{y_a(z)}^{y_b(z)}\\int_{x_a(y,z)}^{x_b(y,z)}\\phi(x,y,z)\\ dx\\ dy\\ dz\n$$\nAlternatively, as the order of integration typically does not matter, we may perform integration with respect to $z$ first and thus have\n$$\n\\int_{y_a}^{y_b} \\int_{x_a(y)}^{x_b(y)} (\\int_{z_a(x,y)}^{z_b(x,y)} \\phi\\ dz) \\ dA\n$$\nas $dx\\ dy = dA$.\n\n## Change of variables for volume integrals\n\nSuppose now that $(x,y,z) \\to (u(x,y,z),v(x,y,z),w(x,y,z))$ is a change of variables from $x, y, z$ to $u, v, w$, where all of $u, v, w$ are smooth, invertible functions of $x, y, z$; in particular, denote their inverses as\n$$\n\\begin{cases}\nx = x(u,v,w) \\\\\ny = y(u,v,w) \\\\\nz = z(u,v,w)\n\\end{cases}\n$$\nUsing a similar derivation process to the two-dimensional Jacobian, we define the three-dimensional Jacobian as the matrix determinant\n$$\nJ(u,v,w) = \\frac{\\partial(x,y,z)}{\\partial(u,v,w)} = \\begin{vmatrix}\nx_u & x_v & x_w \\\\\ny_u & y_v & y_w \\\\\nz_u & z_v & z_w\n\\end{vmatrix}\n$$\nrepresenting the area scaling factor for the parallepiped formed by $\\delta u, \\delta v, \\delta w$ instead of the parallelogram formed by $\\delta u$ and $\\delta v$. Analogous to the two-dimensional case, we thus have\n$$\n\\int_V \\phi(x,y,z)\\ dV = \\int_V \\phi(u,v,w)\\ |J(u,v,w)|\\ du \\ dv \\ dw\n$$\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Find the Jacobian for: \n1. Spherical polar coordinates, given by $(x,y,z) = (r\\sin \\theta \\cos \\phi, r\\sin \\theta \\sin \\phi, r\\cos \\theta)$;\n2. Cylindrical polar coordinates, given by $(x,y,z) = (r \\cos \\phi, r \\sin \\phi, z)$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\nFor spherical polar coordinates:\n$$\n\\begin{aligned}\nJ(r, \\theta, \\phi) &= \\begin{vmatrix}\n(r\\sin\\theta \\cos \\phi)_r & (r\\sin\\theta \\cos \\phi)_\\theta & (r\\sin\\theta \\cos \\phi)_\\phi \\\\\n(r\\sin\\theta \\sin \\phi)_r & (r\\sin\\theta \\sin \\phi)_\\theta & (r\\sin\\theta \\sin \\phi)_\\phi \\\\\n(r\\cos \\theta)_r & (r\\cos \\theta)_\\theta & (r\\cos \\theta)_\\phi\n\\end{vmatrix} \\\\\n&= \\begin{vmatrix}\n\\sin \\theta \\cos \\phi & r\\cos \\theta \\cos \\phi & - r\\sin\\theta\\sin\\phi \\\\\n\\sin \\theta \\sin \\phi & r \\cos \\theta \\sin \\phi & r\\sin\\theta\\cos\\phi \\\\\n\\cos \\theta & -r\\sin \\theta & 0\n\\end{vmatrix}\n\\end{aligned}\n$$\nUsing the last row to expand the determinant gives\n$$\n\\begin{aligned}\nJ(r,\\theta,\\phi)&= \\cos \\theta \\begin{vmatrix}\nr\\cos \\theta \\cos \\phi & - r\\sin\\theta\\sin\\phi \\\\\nr \\cos \\theta \\sin \\phi & r\\sin\\theta\\cos\\phi \n\\end{vmatrix} + r \\sin \\theta \\begin{vmatrix}\n\\sin \\theta \\cos\\phi & -r\\sin\\theta\\sin\\phi \\\\\n\\sin \\theta \\sin \\phi & r \\sin \\theta \\cos \\phi\n\\end{vmatrix} \\\\\n&= \\cos\\theta(r^2\\sin\\theta\\cos\\theta\\cos^2\\phi + r^2\\sin\\theta\\cos\\theta\\sin^2\\phi)\\\\\n& + r\\sin\\theta(r\\sin^2\\theta\\cos^2\\phi + r\\sin^2\\theta\\sin^2\\phi) \\\\\n&=r^2\\sin\\theta\\cos^2\\theta + r^2\\sin\\theta\\sin^2\\theta \\\\\n&= r^2 \\sin \\theta.\n\n\\end{aligned}\n$$\n\nFor cylindrical polar coordinates:\n\n$$\n\\begin{aligned}\nJ(r, \\phi, z) &= \\begin{vmatrix}\n(r\\cos\\phi)_r & (r\\cos\\phi)_\\phi & (r\\cos\\phi)_z \\\\\n(r\\sin\\phi)_r & (r\\sin\\phi)_\\phi & (r\\sin\\phi)_z \\\\\nz_r & z_\\phi & z_z\n\\end{vmatrix} \\\\\n&= \\begin{vmatrix}\n\\cos\\phi & -r\\sin\\phi & 0 \\\\\n\\sin\\phi & r\\cos\\phi & 0 \\\\\n0 & 0 & 1\n\\end{vmatrix}\n\n\\end{aligned}\n$$\nUsing the last row to expand the determinant gives\n$$\nJ(r,\\phi,z) = r\\cos^2\\phi + r\\sin^2\\phi = r\n$$\nas with polar coordinates in 2D.\n\n\n## Wrapping up\n\nThree last concerns before we wave *sayonara* and leave the world of multiple integrals behind forever. That is, until next Monday.\n\n1. **Finding volume with volume integrals** (incredible). Our puny three-dimensionaloid minds know not how to interpret $\\int_D \\phi \\ dV$ as volume or area under a curve, but $\\int_D \\ dV$ represents the sum of the infinitesimal cubes making up the solid $D$ and thus converges to its volume.\n\n2. **Generalizing volume integrals to higher dimensions**. Three points of concern here:\n\n    - By Fubibinini's Theorem, the order of integration is still irrelevant given the usual terms and conditions.\n    - If expressing an integral over some $n$-dimensional region as $n$ iterated integrals, the innermost integral can have bounds $a, b$ that are functions of the other $n-1$ variables, the second-innermost integral can have bounds that are functions of the other $n-2$ variables, etc., etc., until the outermost integral can only have constant bounds. \n    - The Jacobian $J(y_1,y_2,..y_n)$ for a change of variables $x_1, ..., x_n \\to y_1, ..., y_n$ is given by the matrix determinant $|\\frac{\\partial x_i}{{\\partial y_j}}|$.\n\n3. **Other applications of volume integrals.**\n\n    1. **Finding the total of a quantity (e.g. electric charge) over a solid object.**\n\n        > <span style=\"background-color: #03cafc; color: black;\">Example</span>. **Electric charge on a hemisphere**. Let $H$ be a hemisphere of radius $R$, centered about the origin. Suppose that there is a charge at every point on the sphere, with magnitude $f(z)$ at each point dependent on the $z$-coordinate only: $f(z)=f_0\\frac{z}{R}$ for some constant $f_0$. What is the total charge on the hemisphere?\n\n        > <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\n        Let's use spherical coordinates for this one! A hemisphere with radius $R$ about the origin in spherical coordinates can be very concisely described by\n        $$\n        r = R,\\ 0\\leq\\theta\\leq \\frac{\\pi}{2}\n        $$\n        as the hemisphere only encompasses the top half of the $xyz$-space. The total charge over the entire volume of the sphere can be formulated as the volume integral\n        $$\n        \\int_H f(z)\\ dV\n        $$\n        which, given a change of coordinates to spherical coordinates, equals\n        $$\n        \\begin{aligned}\n        \\int_{0}^{2\\pi}\\int_{0}^{\\frac{\\pi}{2}}\\int_{0}^{R}f_0\\frac{r\\cos\\theta}{R}\\ r^2\\sin\\theta\\ dr\\ d\\theta\\ d\\phi \\\\\n        = \\frac{f_0}{R}\\int_{0}^{2\\pi}\\int_{0}^{\\frac{\\pi}{2}}\\sin\\theta\\cos\\theta\\ [\\frac{r^4}{4}]^R_0\\ d\\theta\\ d\\phi \\\\\n        = \\frac{f_0R^3}{4}\\int_{0}^{2\\pi}\\int_{0}^{\\frac{\\pi}{2}}\\sin\\theta\\cos\\theta\\  d\\theta\\ d\\phi \\\\\n        = \\frac{f_0R^3}{4}\\int_{0}^{2\\pi}[\\frac{\\sin^2\\theta}{2}]^{\\frac{\\pi}{2}}_0 \\ d\\phi \\\\\n        = \\frac{f_0 R^3 \\pi}{4}.\n        \\end{aligned}\n        $$\n\n    2. **Finding the center of mass of a solid object.**\n\n        > <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Center of mass**. Suppose that $D$ describes a solid object in space, and $\\rho(\\mathbf{x})$ describes the density of the object at point $\\mathbf{x}=(x,y,z)$. The center of mass of the object is thus given by\n        $$\n        \\mathbf{X} = \\frac{1}{M}\\int_V \\mathbf{x}\\rho(\\mathbf{x})\\ dV\n        $$        \n        > where $M$ is the total mass of the object, given by the integral of its density over volume:\n        $$\n        M = \\int_V \\rho(\\mathbf{x})\\ dV.\n        $$\n        > Note that the center of mass is a **vector-valued integral**, which is simply an integral evaluated component-by-component but otherwise normally.\n\n        > <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\n        The integral expression of the center of mass is motivated by the **priciple of moments**, which states that if an object is thought of as being comprised of $n$ point-masses with position $\\mathbf{x_1, x_2, ..., x_n}$ and masses $m_1, m_2, ..., m_n$, then the object is in equilibrium about a pivot point $\\mathbf{X}$ (imagine balancing the object on the tip of your finger at this point) if the **moments** of the point masses sum to zero:\n\n        $$\n        \\sum_{i=1}^n m_i(\\mathbf{X-x_i}) = 0 \\iff \\mathbf{X} = \\frac{\\sum_{i=1}^n\\mathbf{m_i x_i}}{\\sum_{i=1}^n m_i}\n        $$\n\n        which, as $n \\to \\infty$, converges to the above integral for $\\mathbf{X}$.","n":0.028}}},{"i":25,"$":{"0":{"v":"Change of Variables","n":0.577},"1":{"v":"## The Jacobian\n~~...were a group of revolutionaries who presided over the French Revolution's reins during its Reign of Terror from 1795 to 1800.~~ \n\nJust as integrals of the type\n$$\n\\int f(x) f'(x)\\ dx\n$$\ncan be solved via a substitution $u = f(x)$, $du = f'(x)$, can we choose to substitute in a set of two alternate variables $(u,v)$ in place of $(x,y)$ and derive consequent expressions for $du$ and $dv$?\n\nThe answer is yes - though such expressions are a great deal more complicated than the single-integral case. We claim the following\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. For a change of variables $(x,y) \\to (u,v)$ in which $u$ and $v$ are continuous and invertible functions of $x$ and $y$, an area integral over a region $D$ can be written as\n$$\n\\int_D \\phi(x,y) \\ dx \\ dy = \\int_D \\phi(u,v)\\ |J(u,v)| \\ du \\ dv\n$$\n> where the **Jacobian** $J(u,v)$ is defined as the matrix determinant\n$$\nJ(u,v) = \\begin{vmatrix}\n\\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\\n\\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n\\end{vmatrix} = \\begin{vmatrix}\nx_u & x_v \\\\\ny_u & y_v\n\\end{vmatrix}\n$$\n> with alternative notation\n$$\nJ(u,v) = \\frac{\\partial (x,y)}{\\partial (u,v)}.\n$$\nNote that we stipulated for $u$ and $v$ to be invertible; this means that for the transformation $u = u(x,y), v = v(x,y)$, we have the corresponding inverse transformation $x = f(u,v), y = g(u,v)$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> (insanely non-rigorous).\n\nThis proof is sketchier than an eyewitness courtrom sketch of the Zodiac Killer, but we forge ahead regardless. The crux of the transformation from $(x,y)$ to $(u,v)$ in the context of an area integral is that the area differential $dA$ must now be represented not as $dx\\ dy$, but some expression of $du$ and $dv$; thus, the crucial question is what that expression is.\n\nLet's denote an infinitesimal change in $u$ and $v$ as $\\delta u$ and $\\delta v$ respectively; at such infinitesimal scales, both can be assumed to be perfectly straight, but not necessarily orthogonal (unlike $dx$ and $dy$). The small region bounded by $\\delta u$ and $\\delta v$ is thus a parallelogram:\n\n![alt text](./assets/images/image-19.png)\n\nGiven that $x = f(u,v)$ and $y = g(u,v)$, the question is now how to express the product $\\delta x \\delta y$ in terms of $\\delta u$ and $\\delta v$. By Taylor's Theorem, we have\n\n$$\n\\begin{aligned}\n\\delta x &= f(u + \\delta u, v + \\delta v) - f(u,v) \\\\\n&= [f(u,v) + ((\\delta u, \\delta v)\\cdot \\nabla)f(u,v) + ...] - f(u,v) \\\\\n&= ((\\delta u, \\delta v)\\cdot \\nabla)f(u,v) \\\\\n&= (\\delta u, \\delta v) \\cdot (f_u, f_v) \\\\\n&= (\\delta u) f_u + (\\delta v) f_v\n\\end{aligned}\n$$\nwhere the $...$ in the Taylor expansion conceal second-order and higher-order terms which vanish for very small $\\delta v$ and $\\delta u$, and\n$$\n\\begin{aligned}\n\\delta y &= g(u + \\delta u, v + \\delta v) - g(u,v) \\\\\n&= [g(u,v) + ((\\delta u, \\delta v)\\cdot \\nabla)g(u,v) + ...] - g(u,v) \\\\\n&= ((\\delta u, \\delta v)\\cdot \\nabla)g(u,v) \\\\\n&= (\\delta u, \\delta v) \\cdot (g_u, g_v) \\\\\n&= (\\delta u) g_u + (\\delta v) g_v\n\\end{aligned}\n$$\nor, in matrix form,\n$$\n\\begin{bmatrix}\n\\delta x \\\\ \n\\delta y\n\\end{bmatrix} = \\begin{bmatrix}\nf_u & f_v \\\\\ng_u & g_v\n\\end{bmatrix}\n\\begin{bmatrix}\n\\delta u \\\\\n\\delta v\n\\end{bmatrix} = J(u,v)\\begin{bmatrix}\n\\delta u \\\\\n\\delta v\n\\end{bmatrix}\n$$\nthus indicating that the determinant $|J(u,v)|$ represents the (signed) area of the parallelogram formed by $\\delta u$ and $\\delta v$.\n\n## Polar coordinates in 2D\n\nTrigonometric substitution for single integrals are useful, but for double integrals, they are downright lifesaving. As it turns out, **many** regions - from circles to half-circles to quarter-circles to... well, mainly just circles, but anything involving angles - are better represented with polar coordinates $(r, \\theta)$ than with Cartesian coordinates $(x,y)$; for instance, the circle $x^2 + y^2 = 1$ is simply represented by $r=1$ in polar coordinates. \n\nThus let us define the change of coordinates\n$$\n\\begin{cases}\nr = \\sqrt{x^2 + y^2} \\\\\n\\theta = \\tan^{-1}(\\frac{y}{x})\n\\end{cases}\n$$\nor, conversely and more familiarly,\n$$\n\\begin{cases}\nx = r\\cos \\theta \\\\\ny = r\\sin \\theta.\n\\end{cases}\n$$\nUsing the Jacobian as above gives us a clue into how double integrals expressed in polar coordinates are evaluated. As formulated previously, we have\n$$\nJ(r,\\theta) = \\frac{\\partial (x,y)}{\\partial (r,\\theta)} = \\begin{vmatrix}\nx(r,\\theta)_r & x(r,\\theta)_\\theta \\\\\ny(r,\\theta)_r & y(r,\\theta)_\\theta \n\\end{vmatrix} = \n\\begin{vmatrix}\n\\cos \\theta & -r \\sin \\theta\\\\\n\\sin \\theta &  r\\cos\\theta\n\\end{vmatrix} = r!\n$$\n(No factorial intended.)\n\nThus we simply have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Change of coordinates from Cartesian to polar in double integration**.\n\n$$\n\\int_D f(x,y)\\ dx\\ dy = \\int_D f(r,\\theta) \\ r\\ dr\\ d\\theta\n$$\nwhich is quite agreeable, as far as mathematical results go. \n\nA further geometric argument may lend much-needed credence to this Jacobian willy-nilly nonsense:\n\n![alt text](./assets/images/image-20.png)\n\n> I have never glanced upon such curious symbols in my entire life. Perchance you meant to type in $r$ and $\\theta$ but slipped, sir or madam? \n\nSubdividing a region with respect to polar coordinates results in infinitesimal sectors, which - when small enough - can be treated as sectors of a circle with radius $r$ at that point. The arc length of such a sector is $r \\ d\\theta$, in radians; the \"rectangle\" pictured above thus has area $r\\ dr\\ d\\theta$.\n\n***\n\nThe following example is quite nice. By this point it should be well-known that me and Vector Calculus are mortal enemies and neither of us can live while the other survives, etc., etc., and good opinion from me towards a problem is rarely bestowed and therefore more worth the earning, so you can be assured that this example is very nice indeed:\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Find the value of $\\int_{-\\infty}^{\\infty} e^{-x^2}\\ dx.$\n\nThe famed **Gaussian integral**, at long last! **Normally** this question would be reserved for a Statistics course, though the expression is so ubiquitous it surely **rings a bell**; this is one **mean** integral, not least because it is non-elementary - the antiderivative of $e^{-x^2}$ cannot be described using elementary functions like polynomials, exponentials, trigonometry, and the like.\n\nSo what to do? We propose that there is another integral, now a double integral instead of a single integral, that embeds within it our desired value, but is far, far simpler to evaluate:\n$$\nI = \\int_{D_a} e^{-(x^2+y^2)}\\ dx\\ dy\n$$\nwhere $D_a$ is a circular disk of radius $a$ centered about the origin. As $a \\to \\infty$, the integral will range over all real numbers and the two-dimensional slice of the integrated volume when $y=0$ will be our desired answer. \n\nBut why is this a simplification? Let's think back to polar coordinates - what in particular attracts us to them? Perhaps the fact that $x^2 + y^2 = r^2$ lies chief among our clues: under transformation $(x,y) \\to (r,\\theta)$, we have\n\n$$\nI = \\int_{D_a}e^{-r^2}\\ r\\ dr\\ d\\theta = \\int_{0}^{2\\pi}\\int_0^a re^{-r^2}\\ dr\\ d\\theta\n$$\nwhere $\\theta$ ranges from $0$ to $2\\pi$ for a full circle, and $r$ ranges from $0$ to $a$ to describe that circle. The inner integral can now be solved via substitution, observing that $r^2$ differentiates into $r$:\n\n$$\n\\int_0^{a}re^{-r^2}\\ dr\\ d\\theta = -\\frac{1}{2}[e^{-r^2}]^{a}_0 = -\\frac{1}{2}[e^{-a^2}-1]\n$$\nwhich converges to $\\frac{1}{2}$ as $a\\to \\infty$. Thus we have, miraculously:\n$$\n\\lim_{a\\to\\infty}I = \\int_0^{2\\pi}\\frac{1}{2}\\ d\\theta = \\pi. \n$$\nOne last step: how do we get from here, the double integral of $e^{(-x^2+y^2)}$, to where we wanted to go - the single integral of just $e^{-x^2}$? Symmetry. Taking a closer look at $I$ reveals that\n$$\n\\begin{aligned}\nI &= \\int_{D_a} e^{-(x^2+y^2)}\\ dx\\ dy \\\\\n&= \\int_{D_a} e^{-x^2} e^{-y^2}\\ dx\\ dy \\\\\n&= \\int_{-\\infty}^{\\infty}e^{-y^2}(\\int_{-\\infty}^{\\infty}e^{-x^2}\\ dx)\\ dy\\\\\n\\end{aligned}\n$$\nas $e^{-y^2}$ can be treated as constant w.r.t. $x$, and thus\n$$\nI = (\\int_{-\\infty}^{\\infty}e^{-x^2}\\ dx)(\\int_{-\\infty}^{\\infty}e^{-y^2}\\ dy)\n$$\nbut the two terms in the above product should be identical because they are the same integral under a different variable, leading to\n$$\nI = (\\int_{-\\infty}^{\\infty}e^{-x^2}\\ dx)^2 = \\pi\n$$\nby the above, and finally, that\n$$\n\\int_{-\\infty}^{\\infty}e^{-x^2}\\ dx = \\sqrt{\\pi}\n$$\ntaking the positive root as the integrand is positive everywhere in $\\mathbb{R}$. \n\nThis is a pretty amazing result. I'm hard-pressed to say exactly what's so amazing about it - maybe I could wax poetic about how it near-miraculously connects $e$ to $\\pi$, or maybe it finally reveals the mysterious connection between the normal distribution and the presence of $\\frac{1}{\\sqrt{2\\pi}}$, but in cases like this, no words can more eloquently describe the elegance of such results than these: *holy fricking shit.*\n\n\n","n":0.028}}},{"i":26,"$":{"0":{"v":"Area Integrals","n":0.707},"1":{"v":"## Generalizing the integral\n\nWhile the previous section concerned itself with generalizing the integral to different paths - paths slightly more interesting that just a straight line on one coordinate axis - this section will be concerned with generalizing the integral to different **regions**: not just a curve in the two-dimensional plane, but surfaces in three-dimensional space, lower-dimensional projections of eldritch horrors inhabiting eleven-dimensional hyperspace, etc., etc.\n\nLet's begin with two dimensions. Unless you're a victim of some highly advanced neurodegenerative dementia (and if you are, god bless), I would suppose that you recall what a **definite integral** in the normal sense entails:\n$$\n\\int_a^{b} f(x)\\ dx\n$$\nfor a smooth function $f(x)$ and $a < b$ lying in the domain of $f(x)$ yields the area under the curve $f(x)$ from $x=a$ to $x=b$. The bog-standard, milque-toast, suburban-midlife-crisis-impending-dad-in-his-40s-energy definite integral is truly the vanilla ice cream of the calculus world.\n\nLine integrals generalized the notion of a definite integral to any curve, but what about a surface where there are now two degrees of freedom rather than one? We will discuss this notion in two parts. First, an **area integral**, an integral over a flat shape in 2D; second, a **surface integral**, which generalizes area integrals to any surface in any number of dimensions.\n\n## Area integrals\n\nSuppose that $D$ is a region **lying flat in** two-dimensional space $\\mathbb{R}^2$ (i.e. just a two-dimensional shape), and let $\\phi(\\mathbf{x})$ be a scalar function that maps points in $\\mathbb{R}^2$ to scalars in $\\mathbb{R}$. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Denote the **area integral** of $\\phi$ over $D$ as \n$$\n\\int_D \\phi(\\mathbf{x})\\ dA\n$$\n> with $A$ denoting area; this is to be distinguished from a surface integral by the fact that $D$ is a shape in 2D and not a surface.\n\nThe concept of an area integral evokes (some) familarity with the line integral. Whereas line integrals of the form\n$$\n\\int_C \\phi(\\mathbf{x})\\ ds\n$$\nsummed over infinitesimal pieces of the arc length of the curve $ds$, the area integral\n$$\n\\int_D \\phi(\\mathbf{x})\\ dA\n$$\nsums over infinitesimal pieces of the **area of the shape** $dA$, analogous to the arc length of the curve. (You may have noticed that we are now using $D$ rather than $C$, which subtly alludes to the trajectory my grades will take once area integrals are introduced.)\n\nThis gives us a rough idea of how to formalize the area integral.\n\n![alt text](./assets/images/image-16.png)\n\nSuppose that we subdivide the region $D$ into small rectangles, each of length $dx$ and width $dy$; the area $dA$ of these small rectangles would thus be $dA = dx\\ dy$. As such, we have\n$$\n\\int_D \\phi(\\mathbf{x})\\ dA = \\lim_{\\delta A \\to 0}\\sum_{i=1}^n \\phi(\\mathbf{x}_i)\\ \\delta A\n$$\nsubdividing $D$ into $n$ small rectangles with $\\mathbf{x}_i$ being the point $(x_i, y_i)$ in the center of each rectangle; as $\\delta A$ approaches $0$, $D$ will be divided into an infinite number of rectangles and, at least by intuition, the Riemann sum should approach the true value of the integral. But this tells us nearly nothing; $\\delta A$ holds no value as far as we are concerned. Instead, via $dA = dx\\ dy$, try:\n$$\n\\begin{aligned}\n\\int_D \\phi(\\mathbf{x})\\ dA &= \\lim_{\\delta A \\to 0}\\sum_{i=1}^n \\phi(\\mathbf{x}_i)\\ \\delta A \\\\\n&= \\lim_{\\delta x,\\ \\delta y \\to 0}\\sum_{i=1}^n \\phi(x_i, y_i)\\ \\delta x\\ \\delta y \\\\\n&= \\lim_{\\delta x, \\ \\delta y \\to 0}\\sum_{i=1}^m \\sum_{j=1}^n \\phi(x_{ij}, y_{ij})\\ \\delta x\\ \\delta y\n\\end{aligned}\n$$\nwhere we are now summing along the axes (i.e. labeling the areas as row $i$, column $j$, rather than area $1$, area $2$, ...). \n\n> This evokes many fond childhood memories of fifth-grade math problems, alongside twelfth-grade biology problems, where we were asked to estimate the area of a leaf with these funny little rectangles; it is immeasurably comforting to know that my mathematical journey since then has only been a footnote to struggling over mixed fractions and memorizing the first digit of $\\pi$.\n\nFrom what we know about Riemann sums and integrals, the above can be directly rewritten as\n$$\n\\int_{y_a}^{y_b} (\\int_{x_a}^{x_b} \\phi(x,y)\\ dx)\\ dy\n$$\nwhich can be interpreted as \n$$\n\\lim_{\\delta y\\to 0}\\sum_{i=1}^n \\int_{x_a}^{x_b}\\phi(x,y_i)\\ dx\n$$\n\ni.e. a sum of integrals:\n\n![alt text](./assets/images/image-17.png)\n\nwhere $\\int_{x_a}^{x_b}\\phi(x, y_i)\\ dx$, represented by the colored slices, is a regular definite integral for constant values of $y$ denoted $y_1, ..., y_n$, and the resulting double integral is the sum of these small slices. We will note now that just as single integrals represent areas under curves, as $z = \\phi(x,y)$ can be represented as a surface in three dimensions, a double integral can also be interpreted as the **volume** under $z=\\phi(x,y)$ bounded by the region $D$, as \n$$\n\\sum \\phi(x,y)\\ dA\n$$\ngives the volume of an infinitesimal cuboid of height $\\phi(x,y)$ just as $f(x)\\ dx$ gives the area of an infinitesimal rectangle of height $f(x)$. It would also be remiss not to point out that \n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The area integral described by $\\int_D 1 \\ dA$ over a region $D$ gives the area of $D$.\n\nThis is all well and good when $x_a, x_b, y_a$ and $y_b$ are constants, i.e. $D$ is a rectangle, but what if we wanted to integrate along more complex regions? Suppose now that for the double integral\n$$\n\\int_{y_a}^{y_b} (\\int_{x_a}^{x_b} \\phi(x,y)\\ dx)\\ dy\n$$\nthat the range $x_a$ to $x_b$ that $x$ takes depends on $y$: at $y=1$ perhaps the region $D$ ranges from $x=0$ to $x=3$, and at $y=3$ perhaps from $x=1$ to $x=5$. Thus write\n$$\nx_a = x_a(y), x_b = x_b(y)\n$$\nas functions of $y$, and proceed as normal. Let's prove that this works!\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> (of sorts).\nLet the rectangular region $R$ described by $a < x < b$, $c < y < d$ wholly encompass the region $D$; that is, $D$ lies entirely within the rectangle. We know how to integrate a function $\\phi$ within $R$; drawing upon its example, we define the following function based on $\\phi$:\n$$\n\\Phi(x,y) = \\begin{cases}\n\\phi(x,y)\\text{ if }(x,y) \\in D \\\\\n0\\ \\text{otherwise}\n\\end{cases}\n$$\nsuch that the area integral\n$$\n\\int_R \\Phi(x,y)\\ dA  \n$$\nwill only take nonzero values within the region $D$, and thus equal the area integral over $D$\n$$\n\\int_D \\phi(x,y)\\ dA.\n$$\nSuppose now that at any point $y_0$ in region $D$, the x-coordinate of $D$ has range $x_a(y_0) < x < x_b(y_0)$. Consequently, we have\n$$\n\\Phi(x,y_0) = \\begin{cases}\n\\phi(x,y_0)\\text{ if } x_a(y_0) < x < x_b(y_0) \\\\\n0\\ \\text{otherwise}\n\\end{cases}\n$$\nAnd thus the integral \n$$\n\\int_a^b \\Phi(x,y)\\ dx\n$$\nin reality ranges over\n$$\n\\int_{x_a(y)}^{x_b(y)} \\phi(x,y)\\ dx.\n$$\n\nThe main caveat of this derivation lies thus:\n\n$$\n x_a(y_0) < x < x_b(y_0)\n$$\n\nDid you catch it? I certainly didn't, and in the spirit of explaining both to you and to my own unfortunately dull-witted mind, the issue is this: if a region is \"sufficiently annoying\" (and indeed this is a proper mathematical term), i.e. if you draw a horizontal or vertical line through it and it crosses more than two points:\n\n![alt text](./assets/images/image-18.png)\n\nThen $x$ or $y$ are most assuredly not bounded by a single interval, and the above proof fails. Fortunately, just like a piecewise-smooth curve being divisible into finite numbers of smooth curves, we may divide certain \"sufficiently annoying\" regions into a finite number of insufficiently annoying regions, defined below:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **simple** region $D$ (proper terminology: insufficiently annoying) is one in $\\mathbb{R}^2$ defined by\n\n$$\nD = \\{(x,y)\\ |\\ a \\leq x \\leq b, y_a(x) \\leq y \\leq y_b(x)\\}\n$$\n> or\n$$\nD = \\{(x,y)\\ |\\ x_a(y) \\leq x \\leq x_b(y), a \\leq y \\leq b\\}\n$$\n> with all the above functions being continuous, i.e. $D$ is bounded by a single interval for either all points in the $x$- or the $y$-direction; passing a vertical or horizontal line through $D$ will always intersect $D$ no more than twice. \n\nCalculating area integrals over non-simple regions necessitates a division into a suitable number of simple regions. Another useful tool is\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Fubini's theorem** states that the **order of integration** for an area integral does not matter, under the condition that the region $D$ can be subdivided into a finite number of simple regions and the integrand $f(x,y)$ is continuous over $D$:\n\n$$\n\\int_{c}^{d} \\int_{x_a(y)}^{x_b(y)} f(x,y)\\ dx\\ dy = \\int_{a}^b \\int_{y_a(x)}^{y_b(x)} f(x,y)\\ dy\\ dx\n$$","n":0.028}}},{"i":27,"$":{"0":{"v":"Line Integrals","n":0.707},"1":{"v":"> In which the big swirly thing finally equals the sum of the little swirly things.\n\n\nIn the following section,  ![alt text](./assets/images/image-13.png){width: 15px} will denote the line integral over the curve representing my parents' disappointment and regret at not having just sent me to music school instead as a function of time.\n\n","n":0.14}}},{"i":28,"$":{"0":{"v":"Riemann Integrals","n":0.707},"1":{"v":"> Evil Riemann: my infinite sums actually never equal zero.\n\nRiemann was the first to rigorously define the concept of integration. His only venture into number theory blossomed into one of the most consequential unsolved problems ever to grace the field. And yet, for all his wisdom, he could not even anticipate the simplest flaw in the postulation of the hypothesis that now bears his name:\n\n![alt text](./assets/images/image-14.png)\n\nSuffice it to say, Riemann was a fool and someone is now a million dollars richer.\n\nThis launches us straight into\n\n### Partitions and Riemann Sums\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. To rigorously define the notion of (one type of) an integral, we begin with **partitions**. A **partition** $D$ of an interval $[t_a, t_b]$ is given by the finite set of $n$ points\n$$\nt_a = t_0 < t_1 < t_2 < ... < t_n < t_b\n$$\n> with its corresponding modulus, denoted $|D|$, representing the longest sub-interval of the partition:\n$$\n|D| = \\max_{i = 1, 2, ..., n}|t_{i} - t_{i-1}|\n$$\n\nGiven this, we now define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Riemann sums** over a partition $D$, for a bounded function $f$, denoted $\\sigma(D, \\zeta)$. If $D$ is as above, then \n$$\n\\sigma(D,\\zeta) = \\sum_{i=1}^{n} f(\\zeta_i)(t_i - i_{i-1})\n$$\n> for **any** $t_{i-1} < \\zeta_i < t_i$, meaning that for a given partition the Riemann sum is non-unique. \n\nSuppose that the sub-intervals are the same length $h = \\frac{t_n - t_0}{n}$, with $t_j = a + j h$. If we take $\\sigma_i = t_i$, we obtain the unique Riemann sum\n$$\n\\sigma(D) = \\sum_{i=1}^nf(t_i)h.\n$$\n\n\n~~The above expression is a succinct and poignant summary of Riemann's tragic life in mathematical form. Riemann had a lifelong fascination with the Greek letter zeta, indicated through both the above use of $\\zeta$ and its use in the zeta function; an unconscious reflection of his status as the world's first zeta male. However, the use of $\\sigma$ - both lowercas and uppercase, and twice too - in the above expression demonstrates his unfilfilled yet ever-present yearning towards becoming sigma. Unfortunately, it was not to be as he died of LBS (little baby syndrome) when he was 39~~\n\n### Integrals and Integrability\n\n> Supposedly Jane Austen's last novel, posthumously published after such works as \"Sense and Sensibility\", \"Pride and Prejudice\", \"Differentials and Differentiability\", etc., etc.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **bounded function** $f:\\mathbb{R \\to R}$ (finite at every point) is **integrable** if the above Riemann sum reaches a limit when the modulus of $D$ tends to zero:\n$$\n\\lim_{|D| \\to 0}\\sigma(D, \\zeta) = I\n$$\ni.e. when the sub-divisions made by the partition $D$ become infinitely small - independently of the choice of $\\zeta$ within the intervals, and the partition $D$ used. As such, it is insufficient for the partition yielding sub-intervals of the same length to converge; **all** partitions must converge, and the value they converge to is called\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **Riemann definite integral** of $f$ over the interval $[t_a, t_b]$ is denoted\n$$\n\\int_{t_a}^{t_b} f(t)\\ dt\n$$\n> and equals the above limit $I$.\n\nIntegrals should be thought of as the limiting value of a sum, **not** the area under a curve. Integrals should be thought of as the limiting value of a sum, **not** the area under a curve. And just in case you've forgotten, integrals should be thought of as the limiting value of a sum, **not** the area under a curve.\n\nAs always, we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span> **Fundamental Theorem(s) of Calculus**. \n\n$$\n\\frac{d}{dx} \\int_{t}^{x} f(t)\\ dt = f(x)\n$$\n> and\n$$\n\\int_{t_a}^{t_b} \\frac{df}{dt}\\ dt = f(t_b) - f(t_a).\n$$","n":0.042}}},{"i":29,"$":{"0":{"v":"Line Integals","n":0.707},"1":{"v":"> \\> Line integral <br/>\n\\> (Looks inside)<br/>\n\\> Curve <br/>\n**Are they stupid?**\n\nIn the previous section, it was hammered into our heads with lethal force that integrals are the limits of a sum, **not** the area under a curve. This section will bring back that hammer and bring further injury upon your wounds in full force.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Line integrals**. \n\nWhereas previously we defined a standard Riemann integral to be the limiting value of the sum\n$$\n\\sigma(D,\\zeta) = \\sum_{i=1}^{n} f(\\zeta_i)(t_i - i_{i-1})\n$$\nas $|D| \\to 0$, this sum is conducted only over a specific range - to be specific, the flat line represented by $t_a$ to $t_b$. But what if we conduct the sum over a different range - say the line $y = 2t$, or even (shockingly) a curve?\n\nWe'll take that suggestion to heart. Suppose that $\\mathbf{x} = r(t)$ is **not** a line, and is instead the parametric curve\n$$\nr(t) = (x(t), y(t)).\n$$\nThe Riemann sum above sub-divided the flat line from $t_a$ to $t_b$ into infinite partitions - can we do the same thing with the curve? The analogue to doing so would be to consider $n+1$ points on the curve\n$$\nr(s_0), r(s_1), ..., r(s_n), s_0 < s_1 < ... < s_n\n$$\nand the **change in position of the curve** between them:\n$$\n\\Delta s_i = |r(s_i) - r(t_{s-1})|\n$$\ndenoting the Euclidean distance. We thus define the line integral of a function $f$ along the curve $\\mathbf{x = r}(s)$ to be the limit of the Riemann sum\n$$\n\\sigma(D,\\zeta) = \\sum_{i=1}^{n} f(\\zeta_i)\\Delta s_i\n$$\nas $|D| \\to 0$ where $\\Delta s_i$ is defined as above; in essence, the sum of the values of the function along the curve multiplied by the little bits of the curve. In integral form, this is written\n$$\n\\int_{r(t)} f(\\mathbf{x})\\ ds = \\int_{s_a}^{s_b} f(r(s))\\ ds.\n$$\n\n> Integrals are the limits of a sum, not the area under a curve. If integrals were the area under a curve (which they are **not**), however, you could think of the line integral over $r(t)$ as this: the area of a very curvy wall whose top is bounded by $r(t)$. \n\nUsing arc length to evaluate integrals is often quite the hassle, so let's convert back to the original parameter $t$:\n\n$$\n\\int_{s_a}^{s_b} f(r(s))\\ ds = \\int_{t_a}^{t_b} f(r(t)) \\frac{ds}{dt} dt = \\int_{t_a}^{t_b}f(r(t))|r'(t)|\\ dt\n$$\nfrom previous results. Any parameter $t$ can be used, alongside the associated parameterization of the curve $\\psi(t)$; as such, the line integral is invariant with respect to the parameterization of the curve considered.\n\n### Scalar fields\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **scalar field** is a map $\\phi: \\mathbb{R}^n \\to \\mathbb{R}$, i.e. a function mapping a vector to a scalar. If the corresponding domain of $\\phi$ encompasses the vectors $\\mathbf{x} \\in \\mathbb{R^n}$, denote $\\phi$ as $\\phi(\\mathbf{x})$.\n\nIntegrating a scalar field over a curve $C$ with arc length parameterization $\\mathbf{r}(s)$ results in the same integral as presented above. We define by convention that the line integral\n$$\n\\int_C ds\n$$\nis equal to the (positive) arc length of the curve; in particular, this means that the line integral is **directionless**, i.e., if $C$ and $C'$ were the same curve in opposite directions, a line integral of the same function over them would have the same result. We further note the following:\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. For a **piecewise smooth curve** $C$ which can be subdivided into a finite number of smooth curves $C_1$, $C_2$, ..., $C_n$, the line integral of a scalar field over $C$ is equal to the sum of the line integrals over the smooth curves.\n\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Evaluate $\\int_C (2 + x^2 y)\\ ds$, where $C$ is the curve given by the upper half of the unit circle $x^2 + y^2 = 1$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\nFirst parameterize the curve via $x = \\cos \\theta$, $y = \\sin \\theta$ for a parameter $0 < \\theta < \\pi$. As such, we write\n$$\n\\int_C (2 + x^2 y)\\ ds = \\int_{0}^{\\pi} (2 + \\cos^2\\theta \\sin \\theta)\\frac{ds}{d\\theta} d\\theta\n$$\nWe proceed either by noting that $ds$ is equal to \n$$\n\\sqrt{(\\frac{dx}{d\\theta})^2 + (\\frac{dy}{d\\theta})^2}\\ d\\theta = \\sqrt{\\sin^2 \\theta + \\cos^2 \\theta}\\ d\\theta = d\\theta\n$$\nby definition, or that the locus of points on the curve can be represented by\n$$\n\\psi(\\theta) = \\begin{bmatrix}\n\\cos \\theta \\\\\n\\sin \\theta\n\\end{bmatrix}\n$$\nwhose derivative, i.e. the tangent vector at point $\\theta$, has magnitude $1$ at every point.\n\nSubstituting into the original line integral gives\n$$\n\\begin{aligned}\n\\int_{0}^\\pi (2 + \\cos^2 \\theta \\sin \\theta) \\ d\\theta \\\\\n= [2\\theta - \\frac{\\cos^3{\\theta}}{3}]^\\pi_0 \\text{ by substitution}\n \\\\\n= (2\\pi + \\frac{1}{3}) + \\frac{1}{3} \\\\\n= 2\\pi + \\frac{2}{3}\n\\end{aligned}\n$$\n\n### Vector fields\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **vector field** $\\mathbf{F}$ is a map $\\mathbb{R}^n\\to \\mathbb{R}^n$ that maps **each point** in $\\mathbb{R}^n$ to a vector in the same space, $\\mathbb{R}^n$.\n\nAs such, vector-valued functions are not necessarily vector fields; their domain does not necessarily encompass every point in $\\mathbb{R}^n$, nor do they map $\\mathbb{R}^n$ to vectors in $\\mathbb{R}^n$ in general. However, all vector-fields are vector-valued functions. \n\nThis somewhat stringent definition is necessary for the formulation of a line integral over vector fields. Specifically, we define two line integrals of a vector field: one vector-valued, one scalar-valued. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **vector-valued line integral** of a vector field as the vector containing the line integral of all its components. \n\nFor\n$$\n\\mathbf{F(x)} = \\begin{bmatrix}\nf_1(\\mathbf{x}) \\\\\nf_2(\\mathbf{x}) \\\\\n\\vdots \\\\\nf_n(\\mathbf{x})\n\\end{bmatrix}\n$$\nwe have\n$$\n\\int_C \\mathbf{F(x)}\\ ds = \\begin{bmatrix}\n\\int_C f_1(\\mathbf{x})\\ ds \\\\\n\\int_C f_2(\\mathbf{x})\\ ds \\\\\n\\vdots \\\\\n\\int_C f_n(\\mathbf{x})\\ ds\n\\end{bmatrix}\n$$\nwhich is directly equivalent to the Riemann sum defining the line integral\n$$\n\\int_C \\mathbf{F(x)}\\ ds = \\sum_{i=1}^n \\mathbf{F(x_i)}\\Delta s_i = \\sum_{i=1}^n \\begin{bmatrix}\nf_1(\\mathbf{x}) \\Delta s_i \\\\\nf_2(\\mathbf{x}) \\Delta s_i \\\\\n\\vdots \\\\\nf_n(\\mathbf{x}) \\Delta s_i\n\\end{bmatrix}\n$$\ni.e. this *very literally* interprets the line integral of a vector field as the Riemann sum of the **vector itself**. Such an interpretation, of course, has its uses; most of the time, however, it plays the Benito Mussolini to Herr **Scalar-Valued Line Integral**'s Hitler. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **scalar-valued line integral** of a vector field along a curve $C$ as the Riemann sum of the **magnitude of the vector** in the direction of the curve.\n\nThe primary difference between the two interpretations is thus: the vector-valued line integral interprets the integral as a sum of the vector field itself, while the scalar-valued line integral interprets the integral as a sum of the **magnitude** of the vector field. This is particularly useful because many physical quantities are **not** vectors; instead, they are scalars which involve vectors in their expressions. \n\nThe list of such quantities are numerous, and in general include any quantity that represents the magnitude of a vector quantity in a given direction, e.g. electric flux (magnitude of electric field strength in a given direction), magnetic flux (magnitude of magnetic field strength in a giveen direction), and, most significantly, work done by a force:\n$$\n\\text{Work done} = \\mathbf{F \\cdot s} = |\\mathbf{F}||\\mathbf{s}|\\cos \\theta\n$$\nwhere $\\mathbf{s}$ is the *displacement* of the object the force acts on, and $\\theta$ is the angle in-between the force and the direction of travel of the object, i.e. work is equal to the direction traveled multiplied by the magnitude of the force in that direction. \n\nIf instead the object travels along a curve $C$ - i.e. if the object changes its direction of travel at each infinitesimal time-step, with the direction being represented by the change in arc length $d\\mathbf{s}$ - the above expression can instead be written as the following line integral:\n\n$$\n\\int_{C} \\mathbf{F \\cdot d\\mathbf{s}} = \\lim_{n \\to \\infty} \\sum_{i=1}^n \\mathbf{F(x_i) \\cdot \\Delta s}\n$$\nmaking necessary the fact that the curve $C$, the domain of $\\mathbf{F}$, and the range of $\\mathbf{F}$ are in the same-dimensional space, as the line integral of $\\mathbf{F}$ along $C$ is the sum of the value of $\\mathbf{F}$ on points on $C$. Note in particular that $d\\mathbf{s}$ is no longer the **magnitude** of the change in arc length; instead, it is the **vector** representing the infinitesimal change in arc length.\n\n\nIf we have\n$$\n\\mathbf{F(x)} = \\begin{bmatrix}\nF_1(\\mathbf{x}) \\\\\nF_2(\\mathbf{x}) \\\\\n\\vdots \\\\\nF_n(\\mathbf{x})\n\\end{bmatrix}\n$$\nand the curve $C$ parameterized by $\\psi(t)$, with\n$$\n\\mathbf{x} = \\psi(t) =  \\begin{bmatrix}\n\\psi_1(t) \\\\\n\\psi_2(t) \\\\\n\\vdots \\\\\n\\psi_n(t)\n\\end{bmatrix}\n$$\nand consequently\n$$\n\\psi'(t) = \\begin{bmatrix}\n\\psi_1'(t) \\\\\n\\psi_2'(t) \\\\\n\\vdots \\\\\n\\psi_n(t)\n\\end{bmatrix} = d\\mathbf{s}\n$$\nthen, as a result, the line integral can be re-formulated as\n$$\n\\int_{C} \\mathbf{F \\cdot d\\mathbf{s}} = \\int_C \\sum_{i=1}^n F_i(\\psi(t))\\psi_i'(t) \\ dt.\n$$\n\nLet's have a quick example.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Your dad went to the grocery store to get milk. He tells you he will be back in 5 minutes, but it's been three days and he still isn't back. Suppose that the path between your home and the grocery store, then back again, is a full circle with radius 1 centered at the origin in $\\mathbb{R}^2$, and that your dad's will to return home can be modeled as an attractive force $\\mathbf{F}$, with\n$$\n\\mathbf{F} = \\begin{bmatrix}\ne^{-x} \\\\\ne^{-y} \\\\\n\\end{bmatrix}\n$$\n> which is exponentially decaying over time. What is your dad's total will to return home over the full journey from home to the grocery store, then back again?\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\nWe interpret the statement \"total will to return home\" as the total magnitude of the force $\\mathbf{F}$ in the direction of the path from the grocery store back home, i.e. the work done by $\\mathbf{F}$ in that direction. As such, the desired quantity is given by the line integral\n$$\n\\int_C \\mathbf{F \\cdot ds}\n$$\nwith the curve $C$ being a full circle with radius $1$ centered at the origin, parameterized by\n$$\n\\mathbf{x} = \\psi(t) = \\begin{bmatrix}\n\\cos t \\\\\n\\sin t\n\\end{bmatrix}\n$$\nwith $x= \\cos t$ and $y = \\sin t$ over the time parameter $0 < t < 2\\pi$, yielding\n$$\n\\psi'(t) = d\\mathbf{s} = \\begin{bmatrix}\n-\\sin t \\\\\n\\cos t\n\\end{bmatrix}\n$$\nSubstituting back into $\\mathbf{F}$ gives the value of the force $\\mathbf{F}$ on the curve $C$:\n\n$$\n\\mathbf{F} = \\begin{bmatrix}\ne^{-x} \\\\\ne^{-y}\n\\end{bmatrix} = \\begin{bmatrix}\ne^{-\\cos t} \\\\\ne^{-\\sin t}\n\\end{bmatrix}\n$$\nand thus\n$$\n\\int_C \\mathbf{F \\cdot ds} = \\int_0^{2\\pi} -e^{-\\cos t}\\sin t + e^{-\\sin t}\\cos t\\ dt\n$$\nequalling by substitution\n$$\n[-e^{-\\cos t}+ e^{\\sin t}]^{2\\pi}_0 = (-e^{-1}+1) - (-e^{-1} + 1) = 0.\n$$\nIn conclusion, your dad's total will to return home over the course of a two-way trip from home to the grocery store to get milk is zero.\n\n****\n\nBut why? Why will your dad never get home? Does it have to do with the intrinsic qualities of the vector field $\\mathbf{F}$, or with the path from home to the grocery store $C$? We're on the cusp of uncovering something essential - but what? Find out on the next episode of Vector Calculus Z!","n":0.024}}},{"i":30,"$":{"0":{"v":"Fundamental Theorem of Line Integrals","n":0.447},"1":{"v":"\n### Basic concepts\n\nRecall the **Fundamental Theorem of Calculus** as stating that, for a function $f(x)$ and its derivative $f'(x)$ which exists over an interval $a < x < b$, we have\n$$\n\\int_{a}^{b} f'(x)\\ dx = f(b) - f(a)\n$$\nwhich, in perhaps much more meaningful terms, speaks simply thus: *the total change of a function $f(b)-f(a)$ over the interval $(a,b)$ is the sum of the infinitesimal changes of the function $f'(x)$ over that interval*.\n\nThe question is, can the same statement - that the big swirly is the sum of all the little swirlies - be generalized, and how?\n\nWith the **Fundamental Theorem of Line Integrals**, we confirm that:\n1. The big swirly is **still** the sum of the little swirlies,\n2. The big swirly equals the sum of the little swirlies along **any** interval, from circles to smiley faces to the contours of Marie Antoinette's severed head (wig not included) - not just the same-old boring straight line from $a$ to $b$.\n3. The big swirly equals the sum of the little swirlies of **any** function in **any** number of variables, not just single-variable functions - that is, as long as the (partial) derivatives of the function exist.\n\nThe mathematical statement that encapsulates all the above is as follows.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Fundamental Theorem of Line Integrals**. For a smooth curve $C$ parameterized by $\\psi(t)$ for $a < t < b$ and a function in any number of variables $f(\\mathbf{x})$ whose gradient vector $\\nabla f$ is continuous on $C$, we have\n\n$$\n\\int_C \\nabla f\\cdot d\\mathbf{s} = f(\\psi(b)) - f(\\psi(a))\n$$\n\n> where $\\int_C \\nabla f \\cdot d\\mathbf{s}$ denotes the vector-valued line integral.\n\nDespite integrating far more bells and whistles into its ensemble, this says exactly the same thing as the Fundamental Theorem of Calculus: \n\n> *The total change of a function from the starting point to the endpoint of a curve ($f(\\psi(b)) - f(\\psi(a))$) is equal to the sum of the infinitesimal changes of that function along the direction of the curve ($\\nabla f \\cdot d\\mathbf{s}$, where the dot product gives the magnitude **in the direction of the curve**)*.\n\nThe intuition behind this statement is as clear as any mathematical theorem can hope to be (which to some people is \"very\", and to others like me, is \"not at all\"):\n1. $\\nabla f$ gives the multi-dimensional analogue to the \"rate of change\" of the function $f$ (that is, it is the vector which contains the rate of change in every direction, represented by the partial derivatives of $f$)\n2. The dot product $\\nabla f \\cdot d\\mathbf{s}$ gives the magnitude of the rate of change of $f$, represented by $\\nabla f$, in the direction of the curve $C$.\n3. The sum of the infinitesimal rates of change along the curve $C$ is the total change from the starting point to the end point of $C$.\n\nBut enough intuition - let's give a proof!\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nLet $f$ be a vector field in $\\mathbb{R}^n$. Then\n$$\n\\begin{aligned}\n\\int_C \\nabla f \\cdot d\\mathbf{s} \\\\\n= \\int_C \\nabla f(\\psi(t)) \\cdot \\psi'(t) \\ dt \\\\\n= \\int_{a}^{b} \\sum_{i=1}^n f_{x_i}(\\psi(t))\\psi'(t)_i\\ dt \\\\ \n= \\int_{a}^b \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i}\\frac{\\partial x_i}{\\partial t} \\ dt\n\\end{aligned}\n$$\nas the parameterization of $C = \\psi(t)$ gives\n$$\n\\psi(t) = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n$$\nalong $C$, and thus\n$$\n\\psi'(t) = \\begin{bmatrix}\n\\frac{dx_1}{dt} \\\\\n\\frac{dx_2}{dt} \\\\\n\\vdots \\\\\n\\frac{dx_n}{dt}\n\\end{bmatrix}\n$$\nleading to the above. As\n$$\n\\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i}\\frac{\\partial x_i}{\\partial t} = \\frac{\\partial f}{\\partial t}\n$$\nby the multivariate chain rule, we thus have\n$$\n\\int_{a}^b \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i}\\frac{\\partial x_i}{\\partial t} \\ dt = \\int_{a}^{b} \\frac{\\partial f}{\\partial t}\\ dt = f(\\psi(b)) - f(\\psi(a))\n$$\nby the Fundamental Theorem of Calculus. This proof is also valid for piecewise smooth curves via finite subdivision to smooth curves.\n\nBy the above, we define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. What do you call the farm where Margaret Thatcher, Ronald Reagan, and Donald Trump are buried together? ~~A gender-neutral public toilet~~ A conservative field.\n\n(Please clap.)\n\nWait, no, let's try again:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. If the vector field $\\mathbf{F}$ can be written as the continuous gradient $\\nabla f$ of some other function $f$, then $\\mathbf{F}$ is a **conservative field**.\n\nThe precise reasoning behind this rather interesting turn of phrase will be revealed in due time.\n\n### Path independence\n\nThe crucial consequence of the Fundamental Theorem of Line Integrals is the genuinely astonishing result that for certain functions - in particular, functions that are conservative fields - the line integral from point $A$ to point $B$ **does not depend on the path chosen between those two points**:\n\n> As long as path $A$ and path $B$ are the same points, you could take any path - from a straight line to a circle to the labyrinthine tunnels across Shrek's vomit-colored earholes - and end up with the same result. \n\nSuch a property is called \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Path independence**. If for a vector field $\\mathbf{F}$ and any two piecewise-smooth curves $C_1$ and $C_2$ with the same start and end points, we have\n$$\n\\int_{C_1} \\mathbf{F \\cdot ds} = \\int_{C_2} \\mathbf{F \\cdot ds},\n$$\n> call $\\mathbf{F}$ **path independent** over these start and end points.\n\nAs stated by the above, conservative fields are path independent over any start and end point; thus we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The vector field $\\mathbf{F}$, continuous over a domain $D$, is path independent in $D$ if and only if it is a conservative field in $D$; that is, there exists some function $f$ such that $\\mathbf{F} = \\nabla f$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.  \n\nBy the Fundamental Theorem of Line Integrals, if $\\mathbf{F} = \\nabla f$, then for any set of two points $\\mathbf{a, b}\\in D$ and any two different piecewise-smooth curves $C_1, C_2$ connecting the two points, we have\n$$\n\\int_{C_1}\\mathbf{F}\\cdot \\mathbf{ds} = \\int_{C_1}\\nabla f \\cdot \\mathbf{ds} = \\mathbf{F(a)-F(b)} = \\int_{C_2}\\mathbf{F\\cdot ds},\n$$\nand thus a conservative field $\\mathbf{F}$ is path independent in $D$. Conversely, if for any vector field $\\mathbf{F}$ we have\n$$\n\\int_{C_1}\\mathbf{F}\\cdot \\mathbf{ds} = \\int_{C_2}\\mathbf{F\\cdot ds}\n$$\nfor any two piecewise-smooth curves $C_1$ and $C_2$ joining the same two points in $D$, suppose that\n$$\n\\phi(\\mathbf{y}) = \\int_{\\mathbf{x\\to y}}\\mathbf{F \\cdot ds}\n$$\nwhere $\\mathbf{x}$ is some fixed point and $\\mathbf{y}$ is a variable point in $D$. Our goal is to show that $\\nabla \\phi = \\mathbf{F}$ and thus that $\\mathbf{F}$ is path independent by definition; taking the partial derivative with respect to variables $x_1, x_2, ..., x_n$ of $\\mathbf{F}$ yields\n$$\n\\begin{aligned}\n\\frac{\\partial \\phi}{\\partial x_i} = \\lim_{\\epsilon \\to \\mathbf{0}}\\frac{\\phi(\\mathbf{y} + \\epsilon_i) - \\phi(\\mathbf{y})}{\\epsilon_i} \\\\\n\\end{aligned}\n$$\nwhere $\\epsilon_i$ is the $i$th component of a vector $\\epsilon \\to \\mathbf{0}$, and thus\n$$\n\\begin{aligned}\n\\frac{\\partial \\phi}{\\partial x_i} = \\lim_{\\epsilon \\to \\mathbf{0}}\\frac{1}{\\epsilon_i}(\\int_{\\mathbf{x \\to (y +\\epsilon_i)}}\\mathbf{F \\cdot ds} - \\int_{\\mathbf{x \\to y}}\\mathbf{F\\cdot ds}) \\\\\n\\end{aligned}\n$$\nand as the former line integral encompasses the same path as the latter line integral excepting the infinitesimal line from $\\mathbf{y}$ to $\\mathbf{y+\\epsilon_i}$, we have\n$$\n\\begin{aligned}\n\\frac{\\partial \\phi}{\\partial x_i} = \\lim_{\\epsilon \\to \\mathbf{0}}\\frac{1}{\\epsilon_i}\\int_{\\mathbf{y \\to y + \\epsilon_i}}\\mathbf{F \\cdot ds} \\\\\n\\end{aligned}\n$$\nand as $\\epsilon_i$ is in the direction of $x_i$, $\\mathbf{ds}$ for the infinitesimal path from $\\mathbf{y \\to y+\\epsilon_i}$ is in the direction of the $x_i$-axis and $\\mathbf{F\\cdot ds}$ is simply the $i$th component of $\\mathbf{F}$ times the magnitude of $\\epsilon_i$:\n$$\n\\begin{aligned}\n\\frac{\\partial \\phi}{\\partial x_i} = \\lim_{\\epsilon \\to \\mathbf{0}}\\frac{1}{\\epsilon_i}\\mathbf{F}_i \\epsilon_i = \\mathbf{F}_i\n\\end{aligned}\n$$\nas desired.\n\n****\n\n### Closed paths\n\nAll of the above naturally leads us to a corresponding result that is just as consequential. First define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The piecewise-smooth curve $C$ is a **closed path** in a domain $D$ if its starting and ending points are the same point. Denote a line integral over a closed path $C$ by the symbol\n$$\n\\oint_{C}\n$$\n> with the funny little swirly thing in the middle, which looks sufficiently like a treble clef to remind me of how I maybe should've just gone to music college and run away from all this Greek black magic instead.\n\nFor conservative fields, closed-path line integrals aren't all that bad.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. $\\mathbf{F}$ is a conservative field over a domain $D$ if and only if, for **any** closed path $C$ within that domain, we have\n$$\n\\oint_C \\mathbf{F\\cdot ds} = 0.\n$$\n\nFinally, something that lets us take a bit of a breather!\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nFirst suppose that $\\mathbf{F}$ is a conservative field equalling the gradient vector $\\nabla f$ for some function $f$. By the Fundamental Theorem of Line Integrals, we have\n$$\n\\int_{C}\\mathbf{F\\cdot ds} = f(\\mathbf{b}) - f(\\mathbf{a})\n$$\nwhere $\\mathbf{b}$ and $\\mathbf{a}$ are the ending and starting points of curve $C$ respectively. If instead $\\mathbf{b = a}$, meaning that the curve is a closed path, we simply have\n$$\n\\int_{C}\\mathbf{F\\cdot ds} = f(\\mathbf{b}) - f(\\mathbf{b}) = 0\n$$\nas desired.\n\nConversely, suppose that\n$$\n\\oint_C \\mathbf{F\\cdot ds} = 0\n$$\nfor any closed path in $D$. Suppose that $C_1$ and $C_2$ are two distinct piecewise-smooth curves that join the same points $\\mathbf{a}$ and $\\mathbf{b}$ in $D$. Then the piecewise smooth curve, formed by joining $C_1$ and $-C_2$ ($C_2$ with direction reversed), is a closed path as its starting point matches its ending point:\n\n![alt text](./assets/images/image-15.png)\n\nDenote this path $C$. By our assumption, we have\n$$\n\\oint_C \\mathbf{F\\cdot ds} = \\int_{C_1} \\mathbf{F \\cdot ds} - \\int_{C_2} \\mathbf{F\\cdot ds} = 0\n$$\nthus implying that\n$$\n\\int_{C_1} \\mathbf{F \\cdot ds} = \\int_{C_2} \\mathbf{F \\cdot ds}\n$$\nfor any two curves $C_1$ and $C_2$ joining the same starting and ending points in $D$. It follows that $\\mathbf{F}$ is path independent in $D$, and, by the above theorem linking path independence to conservatism, that $\\mathbf{F}$ is a conservative field.\n\n### Putting the \"conservative\" in \"conservative field\"\n\nHere's a fun one for all you physics enjoyers out there! (read: LITERALLY NO ONE, and if you happen to be one, then if we were ever friends, I hang my head in shame over my dreadfully poor judgment and hereby renounce our friendship. )\n\nIn Newtonian mechanics, an obscure but important fact is that the **force** acting upon a particle is related to the second-order derivative of its position - in saner, less deranged terms, its acceleration - by the following equation:\n\n$$\n\\mathbf{F} = m\\ddot{\\mathbf{x}},\n$$\nwhere $\\mathbf{F}$ denotes force, $m$ denotes the mass of the object, and $\\mathbf{x}$ denotes position. Essentially, the greater the mass, the greater the attraction; or, as Newton artfully stated upon gazing towards your mother, \"I like 'em thicc\".\n\nWe can interpret $\\mathbf{F}$ as a vector field - it maps every position $\\mathbf{x}$ along a smooth path taken by the object, denoted $C$, to a vector denoting the force experienced by the object. We already know that the line integral\n$$\n\\int_{C} \\mathbf{F \\cdot ds}\n$$\nrepresents the work done by the force $\\mathbf{F}$ along the path $C$; substituting Newton's Second Law (that's what this is called) into the above, and supposing that the curve $C$ can be parameterized with respect to time as $\\mathbf{x} = \\psi(t)$, suggests that\n\n$$\n\\begin{aligned}\n\\int_C \\mathbf{F\\cdot ds} \\\\\n= m\\int_C \\mathbf{\\ddot{x} \\cdot ds} \\\\\n= m\\int_{\\psi(a)}^{\\psi(b)} \\mathbf{\\ddot{x}(t)\\cdot \\psi'(t)}\\ dt \\\\\n= m\\int_{\\psi(a)}^{\\psi(b)} \\mathbf{\\ddot{x}(t)\\cdot \\dot{x}(t)}\\ dt \\\\\n\\end{aligned}\n$$\nas the curve $C = \\psi(t)$ *is* the position vector $\\mathbf{x}$ along the curve and thus $\\psi(t) = \\mathbf{x}$, with $a < t < b$ being the starting and ending points of the curve. Earlier results suggest that\n$$\n\\frac{d}{dt} (\\mathbf{\\dot{x}(t)\\cdot \\dot{x}(t)}) = 2\\mathbf{\\ddot{x}(t)\\cdot \\dot{x}(t)}\n$$\nfor a vector $\\mathbf{x}$ due to the product rule; as such, we have\n$$\n\\int_C \\mathbf{F\\cdot ds} = \\frac{m}{2}\\int_{\\psi(a)}^{\\psi(b)}\\frac{d}{dt}(\\mathbf{\\dot{x}(t)\\cdot\\dot{x}(t)}) = \\frac{m}{2}[|\\mathbf{\\dot{x}}(\\psi(b))|^2 - |\\mathbf{\\dot{x}}(\\psi(a))|^2]\n$$\nwhere $|\\mathbf{x}|$ denotes the norm. The astute among you will notice that, as $\\dot{\\mathbf{x}}$ denotes the first-order derivative of position (in less clinically-insane terms, velocity), the above equation is the change in kinetic energy from $t=a$ to $t=b$! This leads to \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **The work-energy principle**. The **total work done** on a particle along a path $C$ is equal to the change in kinetic energy of the particle along that path:\n$$\nWD = K(b) - K(a)\n$$\n> where $WD$ denotes work done and $K(b)$ denotes kinetic energy at time $b$.\n\nWe can say even more. Suppose now that the force field $\\mathbf{F}$ is a conservative field; it equals $-\\nabla V$ for some **potential field** $V$, where the negative sign is by convention. We thus have\n\n$$\nWD = K(b) - K(a)\n$$\n*regardless* of the path $C$ chosen, as long as the start and end points are the same - energy is **conserved**, as all the energy being inputted into the system by some force (the work done) is utilized to change the velocity of the particle, i.e. converted to kinetic energy. We also have, as a consequence of the above theorems on conservative fields, that a **conservative force** will do no work along a closed loop. \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Law of Conservation of Energy** (deservedly capitalized). A conservative force-field $\\mathbf{F}$ upholds the property that its total work done on a particle on a trajectory from point $A$ to point $B$ equals to the change of the kinetic energy of the particle beetween the two points, no matter the trajectory taken between the two points.\n\n*This* - the conservation of energy - lends conservative fields their name, so named because they **conserve** energy (and not, in fact, because they'll begin spewing twenty racial slurs per second, or perform suspiciously-angled Roman salutes, or suggest that they're incapable of doing positive work \"because of woke\" when integrated over a closed loop.)\n\n### Testing if a field is conservative\n\n> Ask them where they were during the years 1933-1945.\n\nBy definition, a conservative field $\\mathbf{F = \\nabla \\phi}$ satisfies\n\n$$\n\\mathbf{F}_i = \\frac{\\partial \\phi}{\\partial x_i}\n$$\nfor all $i = 1, 2, ..., n$. Taking the partial derivative another time gives\n\n$$\n\\frac{\\partial}{\\partial x_j}\\mathbf{F_i} = \\frac{\\partial^2 \\phi}{\\partial x_i \\partial x_j} = \\frac{\\partial}{\\partial x_i}\\mathbf{F_j}\n$$\nby equality of mixed partial derivatives given the local continuity of the derivatives of $\\phi$, for **any** $i, j = 1, 2, ..., n$. As such, we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. If for a vector field $\\mathbf{F}: \\mathbb{R^n \\to R^n}$ we have\n$$\n\\frac{\\partial}{\\partial x_j}\\mathbf{F_i} = \\frac{\\partial}{\\partial x_i}\\mathbf{F_j}\n$$\n> for any $i, j = 1, 2, ..., n$, then $\\mathbf{F}$ is a conservative field (barring some annoying exceptions). In fact, this is not only a necessary but a sufficient condition; it is an \"if and only if\" statement, though this fact is not immediately evident at present.\n\nLet's take a look at one particularly annoying exception.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Is the vector field $\\mathbf{F} = (-\\frac{y}{x^2+y^2}, \\frac{x}{x^2+y^2})$ conservative?\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>\n\nPerforming the aforementioned test gives\n$$\n\\begin{aligned}\n\\frac{\\partial F_x}{\\partial y} &= -\\frac{(x^2+y^2)-2y^2}{(x^2+y^2)^2} \\\\\n&= -\\frac{(x^2-y^2)}{(x^2+y^2)^2} \\\\\n&= \\frac{y^2 - x^2}{(x^2+y^2)^2} \\\\ \\\\\n\\frac{\\partial F_y}{\\partial x} &= \\frac{(x^2+y^2)-2x^2}{(x^2+y^2)^2} \\\\\n&= \\frac{y^2 - x^2}{(x^2+y^2)^2}\n\\end{aligned}\n$$\nwhich shows signs of ardent and true conservatism - it can be swiftly verified that $\\mathbf{F} = \\nabla \\tan^{-1}(\\frac{y}{x})$. But watch as the veil of conservative ardor is ripped with utmost haste from the traitorous visage of $\\mathbf{F}$ to reveal - *DIOS MIO, A LIBERAL!*\n\nThe evidence? Consider, for instance, the line integral around the closed loop formed by the circle $C$ represented by $x^2 + y^2 = 1$ with radius $1$ about the origin. Parameterizing the circle as $(x,y) = (\\cos t, \\sin t),\\ 0 < t < 2\\pi$ leads to\n\n$$\n\\begin{aligned}\n\\oint_{C} \\mathbf{F\\cdot ds} \\\\\n= \\oint_{0}^{2\\pi} (-\\frac{y}{x^2+y^2}, \\frac{x}{x^2+y^2}) \\cdot \\frac {d}{dt}(\\cos t, \\sin t)\\ dt \\\\\n= \\int_{0}^{2\\pi} (-\\sin t, \\cos t)\\cdot (-\\sin t, \\cos t)\\ dt \\\\\n= \\int_0^{2\\pi} \\cos^2 t + \\sin^2 t \\ dt \\\\\n= \\int_0^{2\\pi} 1\\ dt \\\\\n= [1]^{2\\pi}_0 \\\\\n= 2\\pi\n\\end{aligned}\n$$\nwhich is decidedly *not* zero, as the closed-loop integral of a well-behaved conservative field should be. But why? The catch lies at the point $(0,0)$, where $\\mathbf{F}$ is not at all well-defined. This is an important caveat - the closed-loop integral of a conservative field is only zero if the field is well-defined for every point inside that closed loop!","n":0.02}}},{"i":31,"$":{"0":{"v":"Integral Theorems","n":0.707},"1":{"v":"\n![alt text](./assets/images/image-27.png)\n\n> stonks theorem","n":0.5}}},{"i":32,"$":{"0":{"v":"Stokes' Theorem","n":0.707},"1":{"v":"## The big ideas\n\n### Generalizing Green's Theorem\n\nRecall from the previous section that Green's theorem related *microscopic circulation* - the infinitesimal rotation of a vector field at every point in a region - to *macroscopic circulation*, the total rotation of that vector field around the boundary of the region.\n\n![alt text](./assets/images/image-41.png)\n\n> The big horse is the unholy amalgamation of all of the smaller horses.\n\nStokes' theorem generalizes such a concept to *any* region and *any* boundary surface (with one less dimension than the region) in any-dimensional space, no matter how curved or straight, or in polar or Cartesian coordinates. \n\n> An infinite number of infinitesimal horses are spinning in place in George Green's pasture, enclosed by a fence. Mr. Green asked the horses politely if they could move out of the pasture, so that he could go harvest some potatoes and carrots; the horses neighed, and POOF! One giant horse was all that was left behind, now tip-toeing its dainty feet around the top of the fence.<br/><br/>\nThis is where all the first-year undergraduate math students in the room erupted in raucous laughter, for they had just studied Green's theorem and wished to demonstrate their familiarity with the concept. \"But wait,\" one particularly well-studied student chimed in. \"What if the pasture wasn't flat at all - what if the horses were grazing on the side of a mountain, and not in the $xy$ Cartesian plane?\"<br/><br/>\nOf course, that student did not know that Green's theorem was applicable to non-Cartesian coordinate systems; but to explain that would be putting Descartes before the horse.\n\n(Please clap.)\n\nThe statement goes:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Stokes' Theorem**. For $S$ a smooth surface in $\\mathbb{R^3}$ and $C = \\partial S$ its boundary curve, Stokes' theorem states for a smooth vector field $\\mathbf{F(x)}$ that\n$$\n\\int_S (\\nabla \\times \\mathbf{F})\\cdot \\mathbf{dS} = \\int_C \\mathbf{F}\\cdot d\\mathbf{x},\n$$\n\n> where $d\\mathbf{x} = (dx,dy,dz)$, and the orientations of $S$ and $C$ are assumed to be **compatible**, meaning that ~~$S$ doesn't bat for the other team~~ the chosen directions of the tangent to $C$, $\\mathbf{t}$, and the normal to $S$, $\\mathbf{n}$, have their cross product $\\mathbf{t} \\times \\mathbf{n}$ pointing out of the surface.\n\n![alt text](./assets/images/image-42.png)\n\nIn particular, this is satisfied if $\\mathbf{n}$ is chosen to point out of the surface and $\\mathbf{t}$ is chosen to be in the anticlockwise direction (pictured above).\n\n### A new understanding of curl\n\nLet's elaborate a little bit on Green's theorem again. Previously, we inferred - but by no means rigorously justified - the statement that the left-hand side of Green's theorem, featuring the integrand\n$$\n\\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y},\n$$\nequalling the divergence of the normal vector to $\\mathbf{F}$, was a measure of the rotation of the field - and thus a two-dimensional analogue to curl.\n\nStokes' theorem allows us to codify this notion a bit more clearly: the integrand of Stokes' theorem features curl, with\n$$\n\\int_S (\\nabla \\times \\mathbf{F})\\cdot \\mathbf{dS} = \\int_C \\mathbf{F}\\cdot d\\mathbf{x};\n$$\nAs with the Divergence Theorem and its recontextualization of divergence, we may now consider an infinitesimal surface $S$ shrunken to contain only the single point in space $\\mathbf{x}$. In this infinitesimal surface $S_x$, the curl is constant, and so the left-hand side surface integral is simply the surface area $S$ times the constant:\n$$\n\\int_{S_x} (\\nabla \\times \\mathbf{F})\\cdot \\mathbf{dS} = S\\mathbf{n}\\cdot (\\nabla \\times \\mathbf{F})\n$$\nwhere $\\mathbf{n}$ is the normal to the surface. \n\nAs such, we define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Coordinate-free definition of curl**. For a vector field $\\mathbf{F}$ and a normal vector $\\mathbf{n}$ to an infinitesimal surface $S$ with area $|S|$, define the **curl** of $\\mathbf{F}$ as the vector-valued function given by the limit\n$$\n\\mathbf{n} \\cdot (\\nabla \\times \\mathbf{F}) = \\lim_{S \\to 0}\\frac{1}{|S|}\\int_{\\partial S} \\mathbf{F}\\cdot \\mathbf{dx}.\n$$\nWe can interpret this as the curl $\\nabla \\times \\mathbf{F}$ representing a **circulation density** (circulation in an infinitesimal point $\\int_{\\partial S}\\mathbf{F\\cdot dx}$, divided by the area of that point), and the dot product $\\mathbf{n}\\cdot (\\nabla \\times \\mathbf{F})$ representing the circulation density in a particular plane normal to $\\mathbf{n}$. \n\nThis definition makes the intuition behind Stokes' theorem a little bit easier to understand, notwithstanding the fact that the definition is only true if Stokes' theorem is true in the first place. \n\nThe curl $\\nabla \\times \\mathbf{F}$ represents the local circulation at every point in a region; for two neighboring points, a clockwise circulation for one is an anticlockwise circulation for the other, and so the circulations cancel and cancel when integrated throughout the region until only the circulation on the very edge remains.\n\n## Sketching a proof\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nStokes' Theorem is Green's Theorem across a general surface, rather than a surface lying in the $xy$-plane. Let $S$ be a smooth surface in $\\mathbb{R^3}$ parameterizable by $S = \\mathbf{x}(u,v)$; as such, our (non-Cartesian) coordinate plane of operations is the $uv$-plane rather than the $xy$-plane. Now all that remains is proving that Stokes' theorem is indeed valid for the $uv$-plane. \n\nSuppose that the curve $C$ bounding $S$ can be denoted $C = \\mathbf{x}(u(t), v(t))$ for a parameter $t$ in the $uv$-plane. Then the circulation of a vector field $\\mathbf{F}(x,y,z)$ around this boundary is\n$$\n\\int_C \\mathbf{F \\cdot dx} = \\int_C F_u\\ du + F_v\\ dv,\n$$\nwhere $\\mathbf{dx} = \\frac{\\partial \\mathbf{x}}{\\partial u} + \\frac{\\partial \\mathbf{x}}{\\partial v}$ by definition, and so $F_u = \\mathbf{F} \\cdot \\frac{\\partial \\mathbf{x}}{\\partial u}$ and $F_v = \\mathbf{F} \\cdot \\frac{\\partial \\mathbf{x}}{\\partial v}$.\n\nBy Green's Theorem, we have\n$$\n \\int_C F_u\\ du + F_v\\ dv = \\int_A (\\frac{\\partial F_v}{\\partial u} - \\frac{\\partial F_u}{\\partial v})\\ du\\ dv\n$$\nwhere $A$ is the area corresponding to the surface $S$. Our task is now to convince ourselves that the right-hand side area integral equals the surface integral\n$$\n\\int_\\mathbf{S} (\\nabla \\times \\mathbf{F}) \\cdot \\mathbf{dS}.\n$$\nLet's look at the two partial derivatives in the integrand one by one. First we have\n$$\n\\frac{\\partial F_v}{\\partial u} = \\frac{\\partial}{\\partial u}(\\mathbf{F} \\cdot \\frac{\\partial \\mathbf{x}}{\\partial v}) = \\frac{\\partial}{\\partial u}(F_i \\frac{\\partial x_i}{\\partial v}) = (\\frac{\\partial F_i}{\\partial x_j}\\frac{\\partial x_j}{\\partial u})\\frac{\\partial x_i}{\\partial v} + F_i (\\frac{\\partial^2 x_i}{\\partial u\\ \\partial v}) \n$$\nusing nothing more than the product rule and the chain rule from the fact that $\\mathbf{F}$ was originally a function of $x_i$ instead of $u$ and $v$. Similarly, we also have\n$$\n\\frac{\\partial F_u}{\\partial v} = (\\frac{\\partial F_i}{\\partial x_j}\\frac{\\partial x_j}{\\partial v})\\frac{\\partial x_i}{\\partial u} + F_i (\\frac{\\partial^2 x_i}{\\partial u\\ \\partial v}) \n$$\nnoting the different indices $i$ and $j$. The integral\n$$\n\\int_A (\\frac{\\partial F_v}{\\partial u} - \\frac{\\partial F_u}{\\partial v})\\ du\\ dv\n$$\nfeatures the difference between the two partial derivatives, and so the second-order derivative term cancels; all that is left is\n$$\n\\begin{aligned}\n&(\\frac{\\partial F_i}{\\partial x_j}\\frac{\\partial x_j}{\\partial u})\\frac{\\partial x_i}{\\partial v} - (\\frac{\\partial F_j}{\\partial x_i}\\frac{\\partial x_i}{\\partial v})\\frac{\\partial x_j}{\\partial u} = (\\frac{\\partial x_i}{\\partial v}\\frac{\\partial x_j}{\\partial u})(\\frac{\\partial F_i}{\\partial x_j} - \\frac{\\partial F_j}{\\partial x_i}) \\\\\n&= (\\delta_{ik}\\delta_{jl} - \\delta_{il}\\delta_{jk})(\\frac{\\partial x_i}{\\partial v}\\frac{\\partial x_j}{\\partial u}\\frac{\\partial F_k}{\\partial x_l}) \n\\end{aligned}\n$$\nwhere, for instance, $\\delta_{ik}\\delta_{jl}$ simply says \"$i$ should equal $k$ and $j$ should equal $l$\". Applying the identity\n$$\n\\delta_{ik}\\delta_{jl} - \\delta_{il}\\delta_{jk} = \\epsilon_{pij}\\epsilon_{pkl}\n$$\ngives rise to\n$$\n\\epsilon_{pij}\\epsilon_{pkl} (\\frac{\\partial x_i}{\\partial v}\\frac{\\partial x_j}{\\partial u}\\frac{\\partial F_k}{\\partial x_l}) = (\\epsilon_{pij} \\frac{\\partial x_i}{\\partial v}\\frac{\\partial x_j}{\\partial u})(\\epsilon_{pkl}\\frac{\\partial}{\\partial x_l} F_k) = (\\frac{\\partial \\mathbf{x}}{\\partial u}\\times \\frac{\\partial \\mathbf{x}}{\\partial v})\\cdot(\\nabla \\times \\mathbf{F})\n$$\nfrom the definition of the cross product. As\n$$\n\\mathbf{dS} = \\frac{\\partial \\mathbf{x}}{\\partial u}\\times \\frac{\\partial \\mathbf{x}}{\\partial v}\n$$\nby definition, we finally have\n$$\n\\int_A (\\frac{\\partial F_v}{\\partial u} - \\frac{\\partial F_u}{\\partial v})\\ du\\ dv = \\int_S (\\nabla \\times \\mathbf{F}) \\cdot \\mathbf{dS}.\n$$\n\n\n### Corollary: irrotational fields are conservative fields\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nAn irrotational field $\\mathbf{F}$, by definition, satisfies $\\nabla \\times \\mathbf{F} = 0$. By Stokes' theorem, we have\n$$\n\\int_S (\\nabla \\times \\mathbf{F})\\cdot \\mathbf{dS} = \\int_S 0\\ dS = 0\n$$\nequalling\n$$\n\\oint_C \\mathbf{F \\cdot dx}\n$$\nfor any surface $S$, and thus any closed curve $C$ in $\\mathbb{R^3}$.\n\n****","n":0.029}}},{"i":33,"$":{"0":{"v":"Green's Theorem","n":0.707},"1":{"v":"\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Green's Theorem**. Suppose that $\\mathbf{F} = (P(x,y),Q(x,y))$ is a vector field in $\\mathbb{R^2}$ where both $P(x,y)$ and $Q(x,y)$ are smooth functions of $x$ and $y$. Then for a region $A$ in $\\mathbb{R^2}$ and its (closed) boundary curve $C$, we have\n\n$$\n\\int_A (\\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y})\\ dA = \\oint_C P\\ dx + Q\\ dy\n$$\n> where the right-hand side can also be rewritten $\\oint_C \\mathbf{F\\cdot ds}.$\n\n### The big idea\n\nThe idea behind Green's theorem is not at all obvious from the equation given: in reality, it relates *microscopic circulation* (the little swirlies) to *macroscopic circulation* (the big swirly).\n\nThe right-hand side of the given statement is perhaps easier explained: $\\oint_C \\mathbf{F \\cdot ds}$ is an integral over the *projection* of the vector field $\\mathbf{F}$ in the direction of the curve $C$, resulting in the total magnitude of $\\mathbf{F}$ that aligns with the direction of $C$ - the *circulation* of $\\mathbf{F}$ about $C$. Call this the **macroscopic** circulation, so named because it represents the circulation over the entire boundary of the curve (helpful graphic below).\n\n![alt text](./assets/images/image-36.png)\n\nOn the other hand, what do we make of $\\int_A (\\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y})\\ dA$? Clearly this is a sum of some quantity within the region bounded by the curve, but what exactly does \n$$\n\\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y}\n$$\nrepresent?\n\nWe will soon see that this quantity is, in reality, a representation of curl - and thus rotation - in two dimensions; but as that fact is far from immediately clear, we argue from another perspective. At every point, the vector field represented by \n$$\n\\mathbf{N} = (Q, -P) \n$$\nis normal to $\\mathbf{F} = (P,Q)$; the expression $\\frac{\\partial Q}{\\partial x} - \\frac{\\partial P}{\\partial y}$ can thus be interpreted as the divergence $\\nabla \\cdot \\mathbf{N}$ of the normal vector field to $\\mathbf{F}$. Tangents to a curve measure rates of change; normals to a curve measure torsion (and thus rotation). \n\nFor a vector field, it helps us sleep at night to think much the same; and so it shall be, for the time being, that the divergence of the normal vector field - the flow of $\\mathbf{F}$ in the normal direction - represents the rotation of the curve. Call these infinitesimally little rotations - the amount to which the curve rotates, or *circulates*, at each point in the region $A$ - the *microscopic circulation*:\n\n![alt text](./assets/images/image-38.png) \n\nGreen's theorem thus tells us that the *microscopic circulation* of a vector field $\\mathbf{F}$ in a region $A$ - the circulation at every point - is equal to the *macroscopic rotation* of $\\mathbf{F}$ around the boundary curve $C$ of $A$. Or, in much more eloquent terms:\n\n> The sum of all of the little swirlies on the inside is equal to the big swirly on the outside.\n\nThis statement is remarkable not only because of its concision, but also - much like the Fundamental Theorem of Calculus, and the Divergence Theorem after it - because it states that to understand what happens *inside* a region (or straight line on an axis, or some vaguely humanoid 3D solid), we need only to look at what happens on its boundary. This is true for many things, like the head of cabbage I've kept on my shelf for seven months straight. Or Lord Voldemort. Or the US, if you're a Fox News reporter. \n\n### Sketching a <span style=\"background-color: #1eff12; color: black;\">proof</span>\n\n> The highlighting is entirely necessary to ensure finer absorption of knowledge.\n\nIn the case where we're gallivanting about a **simply-connected region** (i.e. a McMuffin instead of a bagel), Green's Theorem is rather convenient to prove with the Divergence Theorem in our arsenal. In the two-dimensional case of the Divergence Theorem, we have\n$$\n\\int_A \\mathbf{\\nabla \\cdot F}\\ dA = \\int_C \\mathbf{F \\cdot n}\\ ds;\n$$\nSupposing that $\\mathbf{N} = (Q, -P)$, the normal vector field to $\\mathbf{F} = (P,Q)$ as before, yields\n$$\n\\int_A \\nabla \\cdot \\mathbf{N}\\ dA = \\int_A Q_x - P_y\\ dA\\\\ (=\\int_C \\mathbf{N \\cdot n}\\ ds)\n$$\nwhich is the left-hand side of the desired statement. All that's left is to prove that \n$$\n\\int_C \\mathbf{N\\cdot n}\\ ds = \\int_C \\mathbf{F\\cdot ds};\n$$\nSupposing that $\\mathbf{ds}$ is in the direction of the tangent vector to $C$, $(x'(s), y'(s))$, we have $\\mathbf{n} = (y'(s), -x'(s))$ as a viable value of the normal vector (with magnitude $1$, as the tangent vector when $C$ is parameterized by its arc length is a unit vector), leading to \n$$\n\\int_C \\mathbf{N\\cdot n}\\ ds = \\int_C \\mathbf{F\\cdot ds}.\n$$\n\nIf instead the region is not simply connected, a nifty infinitesimal cut does the trick:\n\n![alt text](./assets/images/image-40.png)\n\ni.e. Green's theorem can be applied counterclockwise on the exterior boundary and clockwise on the interior boundary, because slicing the region reveals that the clockwise interior boundary is a part of the counterclockwise exterior boundary.\n\n### Corollaries of Green's theorem: The Fundamental Theorem of Calculus\n\nConsider Green's theorem along a simple rectangle $R$ bounded by $\\{(x,y): 0 \\leq x \\leq a,\\ 0 \\leq y \\leq b\\}$, and for the simple vector field $\\mathbf{F} = (P, 0)$. \n\n![alt text](./assets/images/image-39.png)\n\nThus we have \n$$\n\\int_R -\\frac{\\partial P}{\\partial y} \\ dA = \\int_{\\partial R} P\\ dx\n$$\nwhere the right-hand side devolves into only the $x$-component because the $y$-component is zero. Thus we have\n$$\n\\int_0^a \\int_0^b -\\frac{\\partial P}{\\partial y}\\ dy\\ dx\n$$\non the left-hand side, and\n$$\n\\int_{\\partial R}P\\ dx = \\int_0^a P(x,0)\\ dx - \\int_0^a P(x,b)\\ dx\n$$\nas for the two paths along the boundary of $R$ where this line integral is nonzero, one is the straight line from $(0,0)$ to $(a,0)$ (and thus positive), and the other is the straight line from $(a,b)$ to $(0,b)$ (and thus negative). Equality between the two sides leads us to\n$$\n\\int_0^b \\frac{\\partial P}{\\partial y}\\ dy = P(x,b) - P(x,0)\n$$\nequivalent with the Fundamental Theorem of Calculus.\n\n\n","n":0.033}}},{"i":34,"$":{"0":{"v":"Examples and Applications","n":0.577},"1":{"v":"\n> There are three important types of applications in our modern world: LaTeX-typesetting applications, job applications, and real-world applications of pure math. A good mathematician should hate all three with a burning passion.\n\n## Basic examples\n\n### Divergence theorem\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Evaluate the surface integral $\\int_S \\mathbf{F\\cdot dS}$ over the surface $S$ of the region $E$ bounded by the parabolic cylinder $z = 1-x^2$ and the planes $z = 0, y = 0$ and $y+z=2$, and the vector field $\\mathbf{F} = (xy, y^2+e^{xz^2}, \\sin(xy))$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\nTrying to evaluate this as a surface integral would be what the kids call \"an epic fail\". (Math has many of those.) Using the Divergence Theorem gives us \n$$\n\\nabla \\cdot \\mathbf{F} = (xy)_x + (y^2+e^{xz^2})_y + (\\sin(xy))_z = y + 2y + 0 = 3y\n$$\nwhich pleases us greatly. As such we have\n$$\n\\int_S \\mathbf{F\\cdot dS} = \\int_{0}^1 \\int_{0}^{2-z} \\int_{-\\sqrt{1-z}}^{\\sqrt{1-z}}3y\\ dx\\ dy\\ dz.\n$$\nwhere the range of $x$ is derived from $z = 1-x^2$, $y$ from $y+z=2$, and $z$ from spiritual guidance. As such, we have\n$$\n\\begin{aligned}\n\\int_S \\mathbf{F \\cdot dS} &= \\int_{0}^1\\int_0^{2-z}3y(2\\sqrt{1-z})\\ dy\\ dz \\\\\n&= 3\\int_0^1 \\sqrt{1-z}(2-z)^2\\ dz\n\\end{aligned}\n$$\nwhich is legitimately solvable by 39 nested applications of integration by parts, but that would be another \"epic fail\", so we're not doing that. Maybe a different order of integration is **in order**? \n\nIf instead we integrate over $dx\\ dz\\ dy$ in that order, we have $0 \\leq y \\leq 2-z$, $0 \\leq z \\leq 1 - x^2$, and $-1 \\leq x \\leq 1$.  Thus\n$$\n\\begin{aligned}\n\\int_S \\mathbf{F \\cdot dS} &= \\int_{-1}^1 \\int_0^{1-x^2} \\int_{0}^{2-z} 3y\\ dy\\ dz\\ dx \\\\\n&= \\int_{-1}^1 \\int_0^{1-x^2}[\\frac{3}{2}y^2]^{2-z}_0 \\ dz \\ dx \\\\\n&= -\\frac{3}{2}\\int_{-1}^1[\\frac{(2-z)^3}{3}]^{1-x^2}_0 \\ dx \\\\\n&= -\\frac{1}{2}\\int_{-1}^1 (1+x^2)^3-8\\ dx \\\\\n&= -\\frac{1}{2}[x + x^3 + \\frac{3}{5}x^5 + \\frac{1}{7}x^7 - 8x]^1_{-1} \\\\\n&= \\frac{184}{35}.\n\\end{aligned}\n$$\n\n### Green's theorem\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Evaluate the line integral of the vector field $\\mathbf{F}(x,y) = (x^2y, xy^2)$ over the curve $C$ encompassing the region bounded by the parabola $y^2 = 4ax$ and the line $x = a$. \n\nGreen's theorem tells us that, with $P = x^2y$ and $Q = xy^2$, we have\n$$\n\\int_A Q_x - P_y\\ dA = \\oint_C P\\ dx + Q\\ dy,\n$$\nwith the integrand of the area integral evaluating to $(xy^2)_x - (x^2y)_y = y^2 - x^2$. This actually doesn't look that bad! The parabola $y^2 = 4ax$ intersects $x=a$ at points $(a,-2a)$ and $(a,2a)$, so $y$ ranges from $-2a$ to $2a$ while $x$ ranges from $a$ to $\\frac{y^2}{4a}$; we thus have\n$$\n\\begin{aligned}\n\\int_{-2a}^{2a} \\int_{a}^{y^2/4a} y^2 - x^2\\ dx\\ dy \\\\\n= \\int_{-2a}^{2a} [y^2x - \\frac{x^3}{3}]^{y^2/4a}_a\\ dy \\\\\n= \\int_{-2a}^{2a} [(\\frac{y^4}{4a} - \\frac{y^6}{192a^3}) - (ay^2 - \\frac{a^3}{3})]\\ dy \\\\\n= [\\frac{y^5}{20a} - \\frac{y^7}{(3 \\cdot 64 \\cdot 7)a^3} - \\frac{ay^3}{3} + \\frac{a^3y}{3}]^{2a}_{-2a} \\\\\n= 2[\\frac{8a^4}{5} - \\frac{2a^4}{21} - \\frac{8a^4}{3} + \\frac{2a^4}{3}] \\\\\n= 2(\\frac{(168-10-210)}{105})a^4 \\\\\n= -\\frac{104}{105} a^4.\n\\end{aligned}\n$$\n\nThat really sucked. Clearly, trying to evaluate this as an area integral would be what the kids call \"a bruh moment\". (Math has many of those.) Instead, evaluating the line integral gives the exact same result (e.g. via the parameterization $y = 2at, x = at^2, -1 \\leq t \\leq 1$ for the parabola and $x = a, y = t, -2a \\leq t \\leq 2a$ for the line):\n$$\n\\begin{aligned}\n\\oint_C P\\ dx + Q\\ dy &= \\oint_C x^2y\\ dx + xy^2\\ dy \\\\\n&= \\int_{-1}^1 2a^3t^5(2at\\ dt) + 4a^3 t^4(2a\\ dt) + \\int_{-2a}^{2a} a^2 t(0\\ dt) + (a t^2)(dt) \\\\\n&= [4a^4\\frac{t^7}{7} + 8a^4\\frac{t^5}{5}]^{1}_{-1}+[\\frac{at^3}{3}]^{2a}_{-2a} = -\\frac{104}{105}a^4\n\\end{aligned}\n$$\nagreeing with the above result.\n\n\n\n### Stokes' theorem\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Verify Stokes' Theorem for the hemispherical surface $r=1$, $z\\geq 0$, and the vector field $\\mathbf{F(r)} = (y,-x,z)$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\nStokes' Theorem states that for a surface in $\\mathbb{R^3}$ and its boundary curve $C$, we have\n$$\n\\int_S (\\nabla \\times \\mathbf{F}) \\cdot \\mathbf{dS} = \\oint_C \\mathbf{F \\cdot dx}. \n$$\nIn this case, the boundary curve of the hemisphere is the intersection between the plane $z=0$ and the sphere, i.e. the circle with radius 1 centered about the origin $x^2+y^2=1$. A full sphere has no boundary - it does not begin or end anywhere; however, a hemisphere\n\n![alt text](./assets/images/image-44.png)\n\nis \"bounded\" by the circle at its bottom.  \n\nThus, we have\n$$\n\\nabla \\times \\mathbf{F} = \\begin{vmatrix}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n\\partial_x & \\partial_y & \\partial_z \\\\\ny & -x & z\n\\end{vmatrix} = 0\\mathbf{i} - 0\\mathbf{j} + (-1-1)\\mathbf{k} = -2\\mathbf{k}\n$$\nThe surface integral can thus be calculated via the spherical-coordinate parameterization $(x,y,z)\\to(r,\\phi,\\theta)$ where $r=1$, yielding\n$$\nS = (x,y,z),\\begin{cases}\nx = \\sin \\phi \\cos \\theta \\\\\ny = \\sin \\phi \\sin \\theta \\\\\nz = \\cos \\phi\n\\end{cases}\n$$\nat every point on the surface of the hemisphere $S$; thus we have tangent vectors\n$$\n\\frac{\\partial S}{\\partial \\phi} = \\begin{bmatrix}\n\\cos\\phi \\cos \\theta\\\\\n\\cos\\phi\\sin\\theta \\\\\n-\\sin\\phi\n\\end{bmatrix},\\ \\frac{\\partial S}{\\partial \\theta} = \\begin{bmatrix}\n-\\sin\\theta \\sin \\phi  \\\\\n\\cos\\theta \\sin \\phi \\\\\n0\n\\end{bmatrix}\n$$\nwith normal vector equal to\n$$\n\\begin{aligned}\n\\mathbf{dS} &= \\frac{\\partial S}{\\partial \\phi} \\times \\frac{\\partial S}{\\partial \\theta} \\\\ \n&= \\begin{vmatrix}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\\n\\cos\\phi\\cos\\theta & \\cos \\phi \\sin \\theta & -\\sin \\phi \\\\\n-\\sin \\theta \\sin \\phi & \\cos\\theta \\sin \\phi & 0\n\\end{vmatrix} \\\\\n&= (\\sin^2\\phi\\cos\\theta) \\mathbf{i} + (\\sin^2 \\phi \\sin \\theta)\\mathbf{j} + (\\sin \\phi \\cos\\phi)\\mathbf{k}\n\\end{aligned}\n$$\ntaking the positive cross product to preserve orientation (normal vector pointing out of the hemisphere; boundary curve oriented counterclockwise). This leads to\n$$\n\\begin{aligned}\n\\int_S (\\nabla \\times \\mathbf{F}) \\cdot \\mathbf{dS} &= \\int_0^{2\\pi} \\int_0^{\\pi/2} \\begin{bmatrix}\n0 \\\\\n0 \\\\\n-2\n\\end{bmatrix}\n\\cdot\\begin{bmatrix}\n\\sin^2\\phi\\cos\\theta \\\\ \n\\sin^2 \\phi \\sin \\theta \\\\ \n\\sin \\phi \\cos\\phi\n\\end{bmatrix}\\ d\\phi\\ d\\theta \\\\\n&= \\int_{0}^{2\\pi} \\int_0^{\\pi/2}-\\sin 2\\phi\\ d\\phi\\ d\\theta \\\\\n&= \\int_0^{2\\pi}[\\frac{\\cos 2\\phi}{2}]^{\\pi/2}_0\\ d\\theta \\\\\n&= \\int_0^{2\\pi} -d\\theta \\\\\n&= -2\\pi.\n\\end{aligned}\n$$\nOn the other side of Stokes' Theorem, we take the boundary curve $C$ as the circle $x^2 + y^2 = 1$ parameterized by $(x,y,z) = (\\cos\\theta, \\sin \\theta,0)$ and with tangent vector $(x',y') = (-\\sin\\theta, \\cos\\theta,0)$  in the counterclockwise direction. On the curve, we have\n$$\n\\begin{aligned}\n\\int_C \\mathbf{F \\cdot dx} &= \\int_0^{2\\pi} (-\\sin \\theta, \\cos \\theta, 0)\\cdot (\\sin \\theta, -\\cos \\theta, 0)\\ d\\theta \\\\\n&= \\int_0^{2\\pi} -\\sin^2 \\theta - \\cos^2\\theta\\ d\\theta \\\\\n&= \\int_0^{2\\pi} -1\\ d\\theta \\\\\n&= -2\\pi\n\\end{aligned}\n$$\nas desired.\n\n\n\n## Applications\n\n### Conservation laws\n\nIf in the middle of a Vector Calculus lecture an idiot gets up from their seat, and ten minutes later someone spots the same idiot drunkenly pissing themselves seven liters deep into the beer barrel at the Cambridge Blues, then the **conservation law of idiocy** states simply thus:\n\n> The amount of idiocy existing within Cambridge has not changed; rather, it has simply moved from a region of space to another **nearby** region of space.\n\n(It's me. I'm the idiot.)\n\nIn essence, a conservation law for a quantity in a system states two things:\n1. The total amount of that quantity (e.g. energy, momentum, etc.) in that system remains constant;\n2. The quantity is conserved *locally*, i.e. a cookie disappearing from my jar is less likely to now be swimming within the rocky wastelands of Saturn's rings than it is to be swimming within the rocky wastelands of my roommate's stomach linings.\n\nThe only way for the amount of a locally conserved quantity to change at a point in a system is if that quantity moves into that point from a nearby point in the system, or moves out from that point to other local points; i.e. teleportation doesn't exist.\n\nIn other words, the **density** of the quantity at a certain point in space and time - denoted $\\rho(x,t)$ - must change over time in a way that can be explained by a continuous function at every point:\n$$\n\\frac{\\partial \\rho}{\\partial t} = -\\nabla\\cdot \\mathbf{J}\n$$\nwhere $\\mathbf{J}(x,t)$ is a continuous vector field with continuous partial derivatives, and $\\nabla \\cdot \\mathbf{J}$, its divergence, captures its inflow or outflow at every point $\\mathbf{x}$ in space. Intuitively, this tells us all we need to know: the change over time of the quantity in the system at a point must be 1) continuous, and 2) describable as the divergence of a continuous vector field - i.e. $\\nabla\\cdot \\mathbf{J}$. \n\nArmed with the divergence theorem, however, we can say a little more. Suppose for example's sake that the quantity being conserved is electric charge, denoted $Q$; the rate of change of electric charge over an entire region $V$ in space is thus given by the volume integral\n$$\n\\frac{dQ}{dt} = \\int_V \\frac{\\partial \\rho}{\\partial t}\\ dV\n$$\ni.e. the changes of its density at every point in the region added together. By the Divergence Theorem, this equals\n$$\n\\frac{dQ}{dt} = \\int_V \\frac{\\partial \\rho}{\\partial t}\\ dV  =  -\\int_V \\nabla \\cdot \\mathbf{J}\\ dV = -\\int_S \\mathbf{J\\cdot dS}\n$$\nor, in other words:\n\n> The rate of change of a quantity over a region is equal to its flux over the surface boundary of the region; a quantity only changes inside a region if that quantity flows out of the surface of the region.\n\nCertain quantities in the physical world have weird and interesting conservation laws. Case in point: fluids. Fluids are generally assumed to be **incompressible**, meaning that their density $\\rho$ must be constant at every point; this is all fine and dandy until everyone's had one too many beers and we end up having to model a human body as an incompressible cylindrical tube with negligible mass, and as someone who's watched sumo wrestling way too much I can say none of these three words are true in any way. \n\nIn addition to incompressibility, we also assume that for a fluid its **velocity field** - the vector field indicating its magnitude and direction of flow at every point - is proportional to its density at that point: $\\mathbf{J} = \\rho \\mathbf{u}.$ This gives us\n$$\n\\frac{\\partial \\rho}{\\partial t} = 0 = -\\nabla\\cdot (\\rho \\mathbf{u}) = -\\rho(\\nabla \\cdot \\mathbf{u})\n$$\nmeaning that the velocity field $\\mathbf{u}$ is solenoidal - it has no net flow in and out of any point in space.\n\n### Heat diffusion\nThis is less of an application of vector calculus and more of an application of an application of vector calculus. We will also soon witness an application of heat diffusion, which would naturally be an application of an application of an application of vector calculus; this is truly the sequel to *Inception* nobody asked for.\n\nSuppose that energy, with its density denoted $E$, is conserved locally in some system; by the statement of the conservation law above we have\n$$\n\\frac{\\partial E}{\\partial t} + \\nabla \\cdot \\mathbf{J} = 0\n$$\nfor some vector field $\\mathbf{J}$ - call this the *heat current*. The principles of thermodynamics tell us that \n\n1. Energy in a gas at a point is proportional to the temperature at that point;\nand\n2. Systems will tend towards thermal equilibrium, meaning that heat flows from points of high temperature towards points of low temperature.\n\nThese principles can be summarized mathematically as \n$$\nE = cT\n$$\nleading to the fact that they share an underlying vector field $\\mathbf{J}$ save for a constant, and \n$$\n\\mathbf{J} = -\\kappa \\nabla T\n$$\nwhere $\\kappa$ is a constant called *thermal diffusivity*, and $-\\nabla T$ is the direction of greatest descent of the temperature - where high temperature flows to low temperature most efficiently. Combining the two gives us the *heat diffusion equation*:\n$$\n\\frac{\\partial T}{\\partial t} = \\kappa\\nabla^2T\n$$\nwhere $\\nabla^2$ denotes the Laplacian. (Compare and contrast this derivation to the similar derivation found in [[Differential Equations.Partial Differential Equations.Heat Equation]], in the Differential Equations course).\n\n### Magnetism\n\nRecall Maxwell's equations concerning electric and magnetic fields:\n$$\n\\begin{cases}\n\\nabla \\cdot \\mathbf{E} = \\frac{\\rho}{\\epsilon_0} \\\\\n\\nabla \\times \\mathbf{E} = -\\frac{\\partial \\mathbf{B}}{\\partial t}\\\\\n\\nabla \\cdot \\mathbf{B} = 0 \\\\\n\\nabla \\times \\mathbf{B} = \\mu_0(\\mathbf{J} + \\epsilon_0\\frac{\\partial\\mathbf{E}}{\\partial t})\n\\end{cases}\n$$\nIn particular, the final two equations - which concern the magnetic field $\\mathbf{B}$ - are of interest. Suppose that we have a time-invariant electric field $\\mathbf{E}$, e.g. a constant current through a circular wire: $\\frac{\\partial \\mathbf{E}}{\\partial t} = 0$. This yields two equations\n$$\n\\nabla \\cdot \\mathbf{B} = 0,\\ \\nabla \\times \\mathbf{B} = \\mu_0\\mathbf{J}\n$$\nfor some vector field $\\mathbf{J}$ describing electric current density and a constant $\\mu_0$ that (and I quote my textbook) \"has some pretentious name that I can never remember\"; they describe the magnetic field around a **current in an infinitely long wire.**\n\nThe presence of $\\nabla \\times \\mathbf{B}$ lends these equations nicely towards Stokes' Theorem. Consider a cross-section of the wire that represents a surface $S$ in space, and its boundary curve $C$:\n\n![alt text](./assets/images/image-45.png)\n\nStokes' theorem tells us that the microscopic circulation of the magnetic field $\\mathbf{B}$ within the area of $S$ is equivalent to the macroscopic circulation around the boundary $C$, i.e.\n$$\n\\int_S (\\nabla \\times \\mathbf{B})\\cdot \\mathbf{dS} = \\int_C \\mathbf{B \\cdot dx}\n$$\nleading to\n$$\n\\int_S \\mu_0 \\mathbf{J} \\cdot \\mathbf{dS} = \\int_C \\mathbf{B\\cdot dx}.\n$$\nThe left-hand side surface integral represents electric flow through a cross-section of the wire at a given unit of time - better known as *current* $I$. Thus we simply have\n$$\n\\mu_0 I = \\int_C\\mathbf{B\\cdot dx},\n$$\ni.e. the circulation of the magnetic field around a wire is proportional to the current within the wire. If we actually calculate this circulation, we can go one step further and obtain an expression for $\\mathbf{B}$; for instance, suppose that the wire is centered about the origin and lies on the $z$-axis with radius $\\rho$. Then the curve $C$ bounding the surface $S$ is parameterized by \n$$\n(x,y,z) = (\\rho \\cos \\theta, \\rho \\sin \\theta, 0),\\ \\mathbf{t} = (- \\sin \\theta, \\cos\\theta, 0)\n$$\nwith $\\mathbf{t}$ denoting the unit tangent vector. As $\\mathbf{B}$ circulates around the wire, suppose that it is parallel to the tangent vector to $C$:\n\n$$\n\\mathbf{B} = f(\\rho)(- \\sin \\theta, \\cos\\theta, 0),\\ \\mathbf{B\\cdot\\mathbf{t}}=f(\\rho)\\rho\n$$\nfor some function of $\\rho$, $f(\\rho)$. This leads to\n$$\n\\int_C \\mathbf{B \\cdot dx} = \\int_0^{2\\pi}f(\\rho)\\rho\\ d\\theta = 2\\pi f(\\rho)\\rho = \\mu_0 I\n$$\nfinally giving\n$$\nf(\\rho) = \\frac{\\mu_0 I}{2\\pi \\rho},\\ \\mathbf{B} = \\frac{\\mu_0 I}{2\\pi \\rho} (-\\sin \\theta, \\cos \\theta, 0)\n$$\nat every point in the plane, with field strength inversely proportional to radius $\\rho$ and proportional to current in the wire.\n","n":0.021}}},{"i":35,"$":{"0":{"v":"Divergence Theorem","n":0.707},"1":{"v":"\n> The Divergence Theorem comes full circle.\n\n## A new understanding of divergence\n\n\n> <span style=\"background-color: #ffb812; color: black;\">Profile</span>: **Carl Freidrich Johann Heinrich Kaiser Wilheim Ludwig Beethoven von Gauss**.<br/><br/>\n![alt text](./assets/images/image-28.png)<br/><br/>\nGauss calculated the precise value of $\\sum_{i=1}^{100} i$ in three picoseconds while still an unborn fetus. He is best known for being the namesake to 100 concepts, 7 billion theorems, and my firstborn child, and for since ascending to divinity as eternal patron saint of all mathematicians worldwide.\n\nThe Divergence Theorem, also known as Gauss' Theorem, is stated as follows:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **The Divergence Theorem**. For a smooth vector field $\\mathbf{F(x)}$ defined over $\\mathbb{R^3}$, as well as a bounded region $V$ and its associated piecewise-smooth boundary $\\partial V = S$, we have\n$$\n\\int_{V}\\nabla \\cdot \\mathbf{F}\\ dV = \\int_{S} \\mathbf{F \\cdot dS}.\n$$\n> (where the direction of $\\mathbf{dS}$ is out of the boundary.)\n\n### Building intuition for the Divergence Theorem\n\n> Why I ~~season~~ integrate over my ~~cutting board~~ surface, NOT my ~~steak~~ volume\n\nOur rudimentary understanding of divergence is that of a measure of flow. (This is admittedly putting Descartes before the horse, because this rudimentary understanding exists only because of the Divergence Theorem.) A positive divergence at a point indicates that the vector field has a net-flow **into** the point; a negative divergence indicates a net flow **out of** the point.\n\nIf divergence quantifies flow at very point, then what does the left-hand side of the Divergence Theorem\n$$\n\\int_V \\nabla\\cdot \\mathbf{F}\\ dV,\n$$\na volume integral of the divergence of $\\mathbf{F}$ over the entire region $V$, mean? Intuitively, this is the sum of the net flow of $\\mathbf{F}$ at every point in $V$ - the **total net flow** of the field **inside** the region. Meanwhile, the surface integral on the right-hand side\n$$\n\\int_S \\mathbf{F \\cdot dS}\n$$\nmeasures something very particular: the **flux** of the vector field out of the surface $S$ which bounds the region $V$ - in essence, the net flow of $\\mathbf{F}$ on the **outside**/exterior of $V$.\n\nTaken together, the Divergence Theorem states simply thus: the net amount of $\\mathbf{F}$ flowing outward from the **inside** of a region $V$ equals the amount of $\\mathbf{F}$ that **actually** flows outward through its surface. Or, better yet:\n\n> If you got gas in your belly, you better believe you're gonna pass that gas out the other side, yo.\n\n### Re-defining divergence\n\nDivergence measures the net flow of a vector field into or out of a given point - but why? The divergence theorem itself provides us with the essential clues.\n\nConsider an infinitesimal region $V$ that encloses exactly a single point $\\mathbf{x}$ in space - a region approaching zero volume. The boundary surface $S$ enclosing that region is the point itself; the volume integral of the divergence of a vector field $\\mathbf{F}$ throughout that region\n$$\n\\int_V \\nabla \\cdot \\mathbf{F}\\ dV\n$$\nis no more than the divergence $\\nabla\\cdot\\mathbf{F}$ at $\\mathbf{x}$, multiplied by the volume $V$ of the region:\n\n$$\n\\int_V \\nabla \\cdot \\mathbf{F}\\ dV\n = V(\\nabla \\cdot \\mathbf{F})(\\mathbf{x})\n$$\n\nas, at infinitesimal scales, the divergence $\\nabla \\cdot \\mathbf{F}$ is roughly constant.\n\n On the other hand, the Divergence Theorem gives us\n$$\n\\int_S \\mathbf{F \\cdot dS}\n$$\nleading to \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Coordinate-free** definition of divergence. For a vector field $\\mathbf{F}: \\mathbb{R^3 \\to R^3}$, the Divergence Theorem about an infinitesimal region $V$ enclosing some point $\\mathbf{x} \\in \\mathbb{R^3}$ gives\n$$\n\\nabla \\cdot \\mathbf{F} = \\lim_{V\\to 0}\\frac{\\int_S \\mathbf{F\\cdot dS}}{V}.\n$$\n\nAt such infinitesimal scales, $\\int_S \\mathbf{F\\cdot dS}$ is not only the flux of $\\mathbf{F}$ out of the surface $S$, but - as $S$ is the point $\\mathbf{x}$ itself - also just the flow of $\\mathbf{F}$ out of the point $\\mathbf{x}$. This ultimately gives rise to our interpretation of divergence as flow: for instance, Maxwell's equations state that\n$$\n\\nabla \\cdot \\mathbf{B} = 0\n$$\n\nindicating a magnetic field that does not contain any sources or sinks and flows in and out of every point at equal measure, and \n$$\n\\nabla \\cdot \\mathbf{E} = \\frac{\\rho}{\\epsilon_0}\n$$\nwith $\\rho$ being charge density, indicating that electric field gathers/flows out of points where charge is densely collected.\n\n### The Divergence Theroem in scalar fields\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. (Divergence theorem in scalar fields). For a region $V$ in $\\mathbb{R^3}$ and its boundary surface $S = \\partial V$, along with a smooth scalar field $\\phi$ in $\\mathbb{R^3}$, we have\n$$\n\\int_V \\nabla \\phi\\ dV = \\int_S \\phi\\ d\\mathbf{S}. \n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThis arises from a special case of the (general) Divergence Theorem: take $\\mathbf{F} = \\mathbf{a}\\phi$ for a constant vector $\\mathbf{a}$, with\n\n$$\n\\int_V \\nabla \\cdot \\mathbf{F}\\ dV = \\int_V \\nabla \\cdot (\\mathbf{a}\\phi)\\ dV = \\int_V \\mathbf{a}\\nabla\\phi\\ dV = \\int_S\\phi (\\mathbf{a}\\cdot \\mathbf{dS})\n$$\nby the Divergence Theorem, leading to\n$$\n\\int_V \\nabla \\phi\\ dV = \\int_S \\phi\\ d\\mathbf{S}. \n$$\n\n## Sketching the proof\n\n### Attempt 1\n\n> Picasso would be spinning in his grave.\n\nSuppose that we divide a region $V$ in 3D space into an infinite number of infinitesimal cubes, denoted $V_{\\mathbf{x}}$; we want to claim that these cubes added together will exactly equal $V$. Sadly, this is extremely suspect; no matter how small the cubes become, there will always be \"gaps\" between the cube and the actual region that remain nonzero even when the cubes have zero volume. This is called the **staircase paradox**, e.g.\n\n![alt text](./assets/images/image-31.png)\n\nwhose natural conclusion would suggest $\\pi = 4$. In engineering, this statement is known as \"the Engineer's Approximation\"; in math, it's known as \"a really bad idea.\"\n\nSo with the foreknowledge that **this does not work**, and in the parallel universe where it did work, $\\pi$ also equals $4$ and $\\sqrt{2}$ equals $1$ and I still managed to \"befriend\" your mom last night, let us sally forth.\n\n![alt text](./assets/images/image-32.png)\n\nPictured above is one of the infinitesimal cubes $V_x$, along with a vector field $\\mathbf{F}$. Consider the Divergence Theorem for this miniature example: does\n$$\n\\int_{V_x} \\nabla\\cdot \\mathbf{F}\\ dV = \\int_{S_x} \\mathbf{F\\cdot dS}\n$$\nhold, with the flux being evaluated along the six sides of the cube?\n\nLet's consider flux first. The cube has three pairs of opposite sides, which, in turn, have opposite normal vectors; consider, for example, the two sides placed in the $(y,z)$ plane, with $e_x = \\pm(1,0,0)$ as their normal vector(s). On the side to the left (light-red color above), suppose that $\\mathbf{F} = \\mathbf{F}(x,y,z)$ (held constant as the cube is infinitesimally small); as such, on the side to the right, we have $\\mathbf{F} = \\mathbf{F}(x+\\delta x, y, z)$. This results in\n$$\n\\text{Flux} = [(\\mathbf{F}(x+\\delta x, y, z) - \\mathbf{F}(x,y,z))\\cdot (1,0,0)](\\text{Area of face})\n$$\nwhich is \n$$\n[\\mathbf{F}_x(x+\\delta x, y, z) - \\mathbf{F}_x(x,y,z)]\\ \\delta y\\ \\delta z\n$$\nrewritable in terms of partial derivatives and linear approximations as \n$$\n\\frac{\\partial \\mathbf{F}_x}{\\partial x} \\ \\delta x\\ \\delta y\\ \\delta z.\n$$\nSimilarly, out of the pair of faces in the $xz$-plane, we have\n$$\n\\frac{\\partial \\mathbf{F}_y}{\\partial y} \\ \\delta x\\ \\delta y\\ \\delta z;\n$$\nand in the $xy$-plane,\n$$\n\\frac{\\partial \\mathbf{F}_z}{\\partial z} \\ \\delta x\\ \\delta y\\ \\delta z.\n$$\nIn total, therefore, the flux through the surface of the cube is\n$$\n\\int_{S_x} \\mathbf{F}\\cdot d\\mathbf{S} = (\\frac{\\partial F_x}{\\partial x} + \\frac{\\partial F_y}{\\partial y} + \\frac{\\partial F_z}{\\partial z})\\ \\delta x\\ \\delta y\\ \\delta z = (\\nabla \\cdot \\mathbf{F})\\ \\delta V\n$$\nwhich is **by definition** equal to\n$$\n\\int_{V_x} \\nabla \\cdot \\mathbf{F}\\ dV\n$$\nas $\\nabla \\cdot \\mathbf{F}$ is constant over an infinitesimal range and the volume integral is thus a constant multiplied by the volume $dV = dx\\ dy\\ dz$.\n\nIf the divergence theorem holds for an infinitesimal cube, does it hold for the entire region $V$? Let's now consider\n$$\n\\int_{V} \\nabla\\cdot \\mathbf{F}\\ dV\n$$\nover the entire volume $V$, which - by our assumption that all the little cubes add up to $V$ - is equal to the volume integral for each little cube $V_x$ added together:\n$$\n\\int_{V} \\nabla\\cdot \\mathbf{F}\\ dV = \\sum_x \\int_{V_x} \\nabla\\cdot \\mathbf{F}\\ dV\n$$\nBut as the divergence theorem holds for these little cubes, the above is equivalent to\n$$\n\\sum_x \\int_{S_x} \\mathbf{F\\cdot dS}\n$$\nalso. Our last task is to prove that this sum of infinitesimal surface integrals is equal to the bigger surface integral over the entire boundary surface $S$,\n$$\n\\int_S \\mathbf{F \\cdot dS}.\n$$\nTo accomplish this, consider the following picture.\n\n![alt text](./assets/images/image-33.png)\n\nAny two faces that do not lie directly on the boundary $S$ of $V$ will touch. Along these touching faces, the value of $\\mathbf{F}$ is identical - but the normal vector $\\mathbf{dS}$ is equal and opposite in direction. The surface integrals along these two faces thus cancel out; and along the next pair of touching faces, it, too, cancels out; until all that is left is the surface integral along the faces that form the boundary $S$, i.e.\n$$\n\\sum_x \\int_{S_x} \\mathbf{F\\cdot dS} = \\int_S \\mathbf{F \\cdot dS}.\n$$\n\n### Attempt 2-D\n\n> A lot of things in life would be easier if we could reduce them to two dimensions. Like proving the divergence theorem. Or relationships.\n\nThe glaring issue we want to amend in our previous proof is the way in which we sub-divided the region: dividing a region into infinitesimal cubes does not yield a \"smooth\" approximation whose difference with the actual region vanishes as the subdivision gets finer. Instead of parameterizing a 3D region into cubes - or a 2D region into squares - we want to work with parallelograms and angled lines. \n\nTo make this concept a bit clearer, we consider\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **The Divergence Theorem, 2D**. The anime adaptation of the  famed Hollywood live-action blockbuster. For a two-dimensional region $D$ and its bounding curve $C$, as well as a vector field in $\\mathbb{R^2}$ denoted $\\mathbf{F}$, we have\n\n$$\n\\int_D \\nabla \\cdot \\mathbf{F}\\ dA = \\int_C \\mathbf{F\\cdot n}\\ ds.\n$$\n\n> Note that the right-hand side is **not** the usual line integral defined for vector fields; the dot product is with the normal vector to the curve, and not the tangent vector.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n**The big idea**: consider the $x$-component and $y$-component of $\\mathbf{F}$ separately. Divergence of a single component is a single partial derivative. Area integral of the partial derivative is a line integral along the boundary. \n\n**** \nWe begin with\n$$\n\\mathbf{F} = \\mathbf{F}_x(x,y)\\mathbf{e_1} + \\mathbf{F}_y(x,y)\\mathbf{e_2}.\n$$\nIf the above statement is valid for the $x$- and $y$-components separately, then it is valid for the general vector field $\\mathbf{F}$ due to the linearity of integration.\n\nFirst consider the $y$-component of the vector field **in vector form**, $\\mathbf{\\vec{F}}_y(x,y) = (0, F_y)$. Beginning with the left-hand side:\n$$\n\\begin{aligned}\n\\int_D \\nabla \\cdot \\mathbf{F}_y\\ dA &= \\int_D (\\partial_x, \\partial_y) \\cdot (0, F_y)\\ dy\\ dx \\\\\n&= \\int_{X}\\int_{y_-(x)}^{y_+(x)}\\partial_y F_y\\ dy\\ dx \\\\\n&= \\int_X F_y(x, y_+(x)) - F_y(x, y_-(x))\\ dx\n\n\\end{aligned}\n$$\nby the Fundamental Theorem of Calculus.\n\n![alt text](./assets/images/image-34.png)\n\nTo convert this to the line integral we desire, we ponder on the relationship between $dx$ and $ds$. Suppose that the unit normal to the curve $C$ is denoted $\\mathbf{n}$; then the angle $\\theta$ formed by $\\mathbf{n}$ and the $y$-axis is given by\n$$\n\\cos \\theta = \\mathbf{n} \\cdot \\hat{y}\n$$\nfor the upward-pointing portion of the curve $C_+$, and \n$$\n\\cos \\theta = -\\mathbf{n} \\cdot \\hat{y}\n$$\nfor the downward-pointing portion $C_-$, where $\\hat{y}$ is the unit vector in the $y$-direction. $\\theta$ is also the angle formed by the $x$-axis and the tangent to the curve $ds$:\n\n![alt text](./assets/images/image-35.png)\n\nwhere we now have\n\n$$\n\\delta x = \\delta s \\cos \\theta = \\pm \\delta s (\\mathbf{n\\cdot\\hat{y}})\n$$\n\ndepending on if we are on $C_+$ or $C_-$. In any case, we have\n$$\n\\vec{F}_y(x,y) \\cdot \\mathbf{n} = (0, F_y)\\cdot \\mathbf{n} = F_y(x,y) (\\mathbf{n} \\cdot \\hat{y})\n$$\nas $\\hat{y} = (0,1)$ and the vector $\\vec{F}_y(x,y) = F_y(x,y)\\hat{y}$.\n\nSubstituting back into the area integral, we have\n\n$$\n\\begin{aligned}\n\\int_X \\vec{F_y}(x, y_+(x)) - \\vec{F_y}(x, y_-(x))\\ dx &= \\int_X F_y(x, y_+(x)) - F_y(x, y_-(x))\\ (\\mathbf{n \\cdot \\hat{y}})\\ ds \\\\\n&= \\int_X (\\vec{F_y}(x, y_+ (x))\\cdot \\mathbf{n})\\ ds + (\\vec{F}_y(x, y_- (x))\\cdot\\mathbf{n})\\ ds \\\\\n&\\text{(as $y_+$ lies on $C_+$ and $y_-$ lies on $C_-$)} \\\\\n&= \\int_{C_-}\\vec{F}_y\\ ds + \\int_{C_+} \\vec{F}_y\\ ds \\\\\n&= \\int_C \\vec{F_y}\\cdot \\mathbf{n}\\ ds \n\n\\end{aligned}\n$$\nas desired. \n\nThe same procedure can be carried out for the $x$-component in vector form, $\\vec{F}_x$; combining the two statements together yields\n$$\n\\int_D \\nabla \\cdot \\mathbf{F}\\ dA = \\int_{\\partial D}\\mathbf{F \\cdot n}\\ ds\n$$\nfor general $\\mathbf{F}$.\n\n(And for the caveat of more complicated regions that have complicated orientations: just cut them into small pieces that are convex.)\n\n### Attempt 3\n\n> ~~Proving the Divergence Theorem is like losing your virginity: the third time's the charm.~~\n\n**The big idea**: consider the $x$-, $y$-, and $z$-components separately. Divergence of a single component is a partial derivative. Volume integral of partial derivative is surface integral along boundary. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nSuppose, as above, that we may write\n$$\n\\mathbf{F} = \\vec{F_x} + \\vec{F_y} + \\vec{F_z}\n$$\nwith each vector in the above expression representing the $x$-, $y$-, and $z$-components of a general vector field $\\mathbf{F} \\in \\mathbb{R^3}$, **as vectors** (e.g. $(0,0,F_x)$). \n\nWe thus proceed as above. The Divergence Theorem in three dimensions states that for a region $V$ in $\\mathbb{R^3}$ and its bounding surface $\\partial V$, we have\n\n$$\n\\int_V \\nabla \\cdot \\mathbf{F}\\ dV = \\int_{\\partial V} \\mathbf{F \\cdot dS}. \n$$\n\nConsider the left-hand side of this expression on a component-wise basis. For instance, for $\\vec{F_x}$ we have\n\n$$\n\\begin{aligned}\n\\int_V \\nabla \\cdot \\vec{F_x}\\ dV &= \\int_V (\\partial_x, \\partial_y, \\partial_z) \\cdot (F_x, 0, 0)\\ dV \\\\\n\n&= \\int_{Z} \\int_{Y} \\int_{x(y_-, z_-)}^{x(y_+, z_+)} \\partial_x F_x\\ dx\\ dy\\ dz \\\\\n&= \\int_Z \\int_Y (F(x^+(y, z),y,z) - F(x^-(y,z), y, z))\\ dA\n\\end{aligned}\n$$\nwhere $dA$ is the area component in the $yz$-plane, $dy\\ dz$, and $x^+$ and $x^-$ are on the positively-oriented and negatively-oriented portions of the surface respectively when parameterized by $y$ and $z$. $dA$ differs from our desired surface-area component $\\mathbf{dS}$ by only the angle between them, $\\theta$, given by\n$$\n\\cos \\theta = \\pm \\mathbf{n} \\cdot \\hat{x}\\ dA = \\pm \\frac{\\mathbf{n}\\cdot\\vec{F_x}}{F_x}\\ dA\n$$\ndepending on orientation, with $\\hat{x}$ the unit vector in the $x$-direction and $\\mathbf{n}$ the unit normal vector to the surface. This leads to\n$$\n\\begin{aligned}\n\\int_Z \\int_Y (F(x^+(y, z),y,z) - F(x^-(y,z), y, z))\\ dA \\\\\n= \\int_{\\partial V_+} \\mathbf{F \\cdot dS} + \\int_{\\partial V_-} \\mathbf{F \\cdot dS} \\\\\n= \\int_{\\partial V} \\mathbf{F\\cdot dS}\n\\end{aligned}\n$$\nas desired.\n\n> Alternative <span style=\"background-color: #1eff12; color: black;\">proof</span>. The vector representation of a point $(x,y,z)$ on $\\partial V = z(x,y)$ (parameterized by $y$ and $z$) is given by\n$$\n\\mathbf{x} = \\begin{bmatrix}\nx(y,z) \\\\\ny \\\\\nz\n\\end{bmatrix}\n$$\n\n\n> As previously mentioned, the surface area component $\\mathbf{dS}$ of this surface is given by\n$$\n\\begin{aligned}\n\\mathbf{dS} &= (\\frac{\\partial \\mathbf{x}}{\\partial y}\\times \\frac{\\partial \\mathbf{x}}{\\partial z})\\ dA  \\\\\n\n&= (\\begin{bmatrix}\nx_y \\\\ 1 \\\\ 0\n\\end{bmatrix} \\times\n\\begin{bmatrix}\nx_z \\\\ 0 \\\\ 1\n\\end{bmatrix}\n)\\ dA \\\\\n\n&= \\begin{bmatrix}\n1 \\\\\n-x_y \\\\\n-x_z\n\\end{bmatrix}\n\\ dA\n\\end{aligned}\n$$\n> for the top surface, and\n$$\n\\begin{bmatrix}\n-1 \\\\\nx_y \\\\\nx_z\n\\end{bmatrix}\n\\ dA\n$$\n> for the bottom surface, leading to\n\n$$\n\\begin{aligned}\n\\int_{\\partial V}\\mathbf{F_x \\cdot dS} &= \\int_{Z} \\int_{Y} P(x^+(y,z),y,z) - P(x^-(y,z),y,z)\\ dA  \\\\\n\\end{aligned}\n$$\n> which is exactly the double integral from above. A similar derivation follows for the $y$- and $z$-components, albeit with parameterizations in $x, z$ and $x,y$ respectively.\n\n****\n\n","n":0.021}}},{"i":36,"$":{"0":{"v":"Curvilinear Differential Operators","n":0.577},"1":{"v":"\n### Divergence and curl\n\nFor a vector field $\\mathbf{F}$ written in terms of curvilinear coordinates $(x_1, x_2, x_3)$\n$$\n\\mathbf{F} = F_1 \\mathbf{e}_1 + F_2 \\mathbf{e}_2 + F_3 \\mathbf{e}_3,\n$$\nwe claim the following two theorems on divergence and curl:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Divergence in curvilinear coordinates** for the above vector field $\\mathbf{F}$, coordinates $(x_1,x_2,x_3)$, basis vectors $(\\mathbf{e_1,e_2,e_3})$, and corresponding scale factors $h_1, h_2, h_3$, the divergence $\\nabla \\cdot \\mathbf{F}$ is given by\n$$\n\\frac{1}{h_1h_2h_3}(\\partial_{x_1}(h_2 h_3 F_1)+\\partial_{x_2}(h_1 h_3 F_2) + \\partial_{x_3}(h_1 h_2 F_3))\n$$\n> noting that the scale factors $h_1, h_2, h_3$ **are not constant**!\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThis is far easier with the integral definition of divergence. Recall that\n$$\n\\nabla \\cdot \\mathbf{F} = \\lim_{V \\to 0}\\frac{\\oint_S \\mathbf{F\\cdot dS}}{V}\n$$\nfor an infinitesimally small surface $S$ enclosing a single point $\\mathbf{x}$; in the new curvilinear coordinate system, consider a similar infinitesimal cuboid enclosed by sides of length $\\delta x_1, \\delta x_2, \\delta x_3$, with volume\n$$\n(\\delta x_1 \\delta x_2 \\delta x_3)(h_1 h_2 h_3)\n$$\nby definition of the scale factors and the orthogonality of the coordinate system. All that's left to do is dealing with the surface integral, which - as a reminder - is evaluated out of the following cuboid, with normal vectors equal to the curvilinear basis vectors $\\mathbf{e_1, e_2, e_3}$:\n\n![alt text](./assets/images/image-43.png)\n\nAs such, the numerator of the limit which defines the divergence\n$$\n\\oint_S \\mathbf{F \\cdot dS}\n$$\nis simply the flux integral out of the faces of the cube: for instance, the faces perpendicular to $\\mathbf{e_1}$ (notated $\\mathbf{e_w}$ on the diagram) have combined integrals\n$$\n\\begin{aligned}\n\\oint_{\\text{top face}}\\mathbf{F\\cdot dS} + \\oint_{\\text{bottom face}}\\mathbf{F \\cdot dS} \\\\\n= (\\mathbf{F(x_1+\\delta x_1,x_2,x_3)} \\cdot \\mathbf{e_1} - \\mathbf{F(x_1, x_2, x_3)}\\cdot \\mathbf{e_1})(\\text{area of face}) \\\\\n= (F_1(x_1+\\delta x_1,x_2,x_3) - F_1(x_1,x_2,x_3))(h_2 h_3 \\delta x_2 \\delta x_3) \\\\\n= \\delta x_1 \\delta x_2 \\delta x_3\\frac{\\partial (h_2 h_3 F_1)}{\\partial x_1}\n\n\\end{aligned}\n$$\nby definition of the partial derivative, and due to the fact that the value of $\\mathbf{F}$ remains roughly constant over an infinitesimal cuboid. \n\n(Beware here! The scale factors $h_1, h_2, h_3$ are **not** constant; as such, the area of the top face may not be equal to the bottom face. Thus, in reality we have\n$$\n\n(F_1(\\text{top face})(h_2h_3)_{\\text{top face}} - F_2(\\text{bottom face})(h_2h_3)_{\\text{bottom face}})\\delta x_2 \\delta x_3\n$$\nas our expression for flux above, which is why $h_2 h_3$ is enclosed within the partial derivative rather than multiplied outside it. $\\delta x_2 \\delta x_3$, however, **are** constant.)\n\nFor both of the other pairs of faces, we have equivalent terms $\\delta x_1 \\delta x_2 \\delta x_3 \\frac{\\partial (F_2 h_1 h_3)}{\\partial x_2}$ and $\\delta x_1 \\delta x_2 \\delta x_3 \\frac{\\partial (F_3 h_1 h_2)}{\\partial x_3}$; in total, the limit which defines the divergence in curvilinear coordinates is thus\n$$\n\\lim_{V\\to 0}\\frac{\\oint_S \\mathbf{F\\cdot dS}}{V} = \\lim_{\\delta x_1, \\delta x_2, \\delta x_3 \\to 0}\\frac{\\sum_{i=1}^3(\\delta x_1 \\delta x_2 \\delta x_3)\\frac{\\partial (F_i h_{\\bar{i}})}{\\partial x_i}}{(h_1 h_2 h_3)(\\delta x_1 \\delta x_2 \\delta x_3)} \n$$\nin which the term $\\delta x_1 \\delta x_2 \\delta x_3$ cancels, leaving us with \n$$\n\\frac{1}{h_1h_2h_3}[\\frac{\\partial (F_1 h_2 h_3)}{\\partial x_1} + \\frac{\\partial (F_2 h_1 h_3)}{\\partial x_2} + \\frac{\\partial (F_3 h_1 h_2)}{\\partial x_3}]\n$$\nwithout the limit.\n\n***\n\nCorrespondingly, we have the following result for curl:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Curl in curvilinear coordinates** for vector field in $\\mathbb{R^3}$ denoted $\\mathbf{F}$, curvilinear basis vectors $\\mathbf{e_1, e_2, e_3}$, and scale factors $h_1, h_2, h_3$ is given by the determinant\n$$\n\\nabla \\times \\mathbf{F} = \\frac{1}{h_1 h_2 h_3}\\begin{vmatrix}\nh_1 \\mathbf{e_1} & h_2 \\mathbf{e_2} & h_3 \\mathbf{e_3} \\\\\n\\partial_1 & \\partial_2 & \\partial_3 \\\\\nh_1F_1 & h_2F_2 & h_3 F_3\n\\end{vmatrix}\n$$\n(This differs from the standard expression for curl, given in Cartesian orthonormal basis vectors as\n$$\n\\begin{vmatrix}\n\\mathbf{e_x} & \\mathbf{e_y} & \\mathbf{e_k} \\\\\n\\partial_x & \\partial_y & \\partial_z \\\\\nF_x & F_y & F_z\n\\end{vmatrix}\n$$\nby only the presence of scale factors.)\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nBegin as above with the integral definition of curl, given by\n$$\n\\mathbf{n} \\cdot (\\nabla \\times \\mathbf{F}) = \\lim_{A \\to 0}\\frac{\\oint_C \\mathbf{F \\cdot dx}}{A}\n$$\nfor an infinitesimally small curve $C$ enclosing an area $A$, and $\\mathbf{n}$ as the normal vector to that area. Suppose we choose the normal vector $\\mathbf{e_1}$; this would give us the first component of the vector representing curl in the basis $\\mathbf{e_1, e_2, e_3}$.\n\nConsider an infinitesimally small (rectangular) area on a surface normal to $\\mathbf{e_1}$; in other words, the area has tangent vectors $\\mathbf{e_2}$ and $\\mathbf{e_3}$, as they are themselves orthonormal to $\\mathbf{e_1}$. As such, the area can be approximated by\n$$\n\\delta x_2 \\delta x_3 (h_2h_3)\n$$\nby definition of the scale factors; the total circulation of $\\mathbf{F}$ over the curve which bounds that area is thus the vector-valued line integral over a rectangle whose sides are parallel to $\\mathbf{e_2}$ and $\\mathbf{e_3}$. For instance, for the pair of sides parallel to $\\mathbf{e_2}$ and running in opposite directions, we have\n$$\n\\mathbf{e_2} \\cdot (\\mathbf{F}(x_1,x_2, x_3+\\delta x_3) - \\mathbf{F}(x_1,x_2,x_3)) (\\text{length of line})\n$$\nequalling\n$$\n(\\mathbf{F_2}(x_1,x_2, x_3 x_3) - \\mathbf{F_2}(x_1,x_2,x_3+\\delta)) (\\delta x_2 h_2) = -\\frac{\\partial (F_2 h_2)}{\\partial x_3}\\delta x_2 \\delta x_3\n$$\nas $\\mathbf{e_3}$ is \"going downwards\" in a right-handed basis, and correspondingly, for the pair of sides parallel to $\\mathbf{e_3}$, we have\n$$\n\\frac{\\partial(F_3 h_3)}{\\partial x_2}\\delta x_2 \\delta x_3\n$$\npositive because $\\mathbf{e_2}$ is \"going upwards\" in a right-handed basis. This gives us\n$$\n\\mathbf{e_1}\\cdot (\\nabla \\times \\mathbf{F}) = \\lim_{\\delta x_2, \\delta x_3 \\to 0}\\frac{(\\partial_2(F_3 h_3) - \\partial_3(F_2 h_2))(\\delta x_2\\delta x_3)}{(h_2h_3)(\\delta x_2 \\delta x_3)} = \\frac{1}{h_2 h_3}\\begin{vmatrix}\n\\partial_2 & \\partial_3 \\\\\nF_2 h_2 & F_3 h_3\n\\end{vmatrix}\n$$\nas the first term for our expression for curl. The other two terms follow suit in a similar manner, with choices of $\\mathbf{e_2}$ and $\\mathbf{e_3}$ as their normal vectors. In total, we have\n\n$$\n\\nabla \\times \\mathbf{F} = \\frac{1}{h_1 h_2 h_3}\\begin{vmatrix}\nh_1 \\mathbf{e_1} & h_2\\mathbf{e_2} & h_3\\mathbf{e_3} \\\\\n\\partial_1 & \\partial_2 & \\partial_3 \\\\\nh_1 F_1 & h_2 F_2 & h_3 F_3\n\\end{vmatrix}\n$$\n***\n\n\n### Differential operators in cylindrical coordinates\nRecall from above the basis vectors and associated scale factors for cylindrical coordinates\n$$\n\\begin{cases}\n\\partial_r\\mathbf{x} = (\\cos \\theta, \\sin \\theta, 0), h_r = 1 \\\\\n\\partial_\\theta \\mathbf{x} = (-r\\sin \\theta, r\\cos\\theta, 0), h_\\theta = r \\\\\n\\partial_z \\mathbf{x} = (0,0,1), h_z = 1\n\\end{cases}\n$$\n\n\n**The gradient**. We have \n$$\n\\begin{aligned}\n\\nabla f &= \\frac{\\mathbf{e_r}}{h_r}\\partial_r f + \\frac{\\mathbf{e_\\theta}}{h_\\theta}\\partial_\\theta f + \\frac{\\mathbf{e_z}}{h_z}\\partial_z f \\\\\n&= ((\\cos\\theta\\ \\partial_r  - \\sin \\theta\\ \\partial_\\theta)f, (\\sin\\theta \\partial_r + \\cos\\theta \\partial_\\theta) f, \\partial_z f)\n\n\\end{aligned}\n$$\nwhich can also be written (quite interestingly) as\n$$\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta & 0 \\\\\n\\sin \\theta & \\cos \\theta & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\\begin{bmatrix}\n\\partial_r f \\\\\n\\partial_\\theta f \\\\\n\\partial_z f\n\\end{bmatrix}\n$$\nindicating the relationship between the gradient in cylindrical coordinates and Cartesian coordinates: a rotation counterclockwise in the $xy$-plane by an angle $\\theta$!\n\n**The divergence**. Using the above, for a vector field $\\mathbf{F}$ we have\n$$\n\\begin{aligned}\n\\nabla \\cdot \\mathbf{F} &= \\frac{1}{h_r h_\\theta h_z}[\\frac{\\partial (F_r h_\\theta h_z)}{\\partial r} + \\frac{\\partial (F_\\theta h_r h_z)}{\\partial \\theta} + \\frac{\\partial (F_z h_r h_\\theta)}{\\partial z}] \\\\\n&= \\frac{1}{r}(\\frac{\\partial (r F_r)}{\\partial r} + \\frac{\\partial F_\\theta}{\\partial \\theta} + \\frac{\\partial (r F_z)}{\\partial z}).\n\\end{aligned}\n$$\n\n**The curl**. We have\n\n$$\n\\begin{aligned}\n\\nabla \\times \\mathbf{F} &= \\begin{vmatrix}\nh_r \\mathbf{e_r} & h_\\theta \\mathbf{e_\\theta} & h_z \\mathbf{e_z} \\\\\n\\partial_r & \\partial_\\theta & \\partial_z \\\\\nh_r F_r & h_\\theta F_\\theta & h_zF_z \n\\end{vmatrix} \\\\\n&= \\begin{vmatrix}\n\\mathbf{e_r} & r\\mathbf{e_\\theta} & \\mathbf{e_z} \\\\\n\\partial_r & \\partial_\\theta & \\partial_z \\\\\nF_r & r F_\\theta & F_z \n\\end{vmatrix}\n\n\\end{aligned}\n$$\n\n**The Laplacian**. Defined as the divergence of the gradient of a scalar field $f$, we have\n$$\n\\begin{aligned}\n\\nabla \\cdot (\\nabla f) = \\nabla^2 f &= \\nabla \\cdot \\begin{bmatrix}\nf_r / h_r \\\\ f_\\theta / f_\\theta \\\\ f_z / h_z\n\\end{bmatrix} \\\\\n&= \\nabla \\cdot \\begin{bmatrix}\nf_r \\\\ f_\\theta/r \\\\ f_z\n\\end{bmatrix} \\\\\n&= \\frac{1}{h_r h_\\theta h_z}[\\frac{\\partial(f_r h_\\theta h_z)}{\\partial r} + \\frac{\\partial (f_\\theta h_r h_z)}{\\partial \\theta} + \\frac{\\partial(f_z h_r h_\\theta)}{\\partial z}] \\\\\n&= \\frac{1}{r}[\\frac{\\partial (rf_r)}{\\partial r} + \\frac{\\partial f_\\theta}{\\partial \\theta} + \\frac{\\partial(r f_z)}{\\partial z}]\n\\end{aligned}\n$$\nwhere $r$ is treated as a constant in the final partial derivative, but not in the first.","n":0.029}}},{"i":37,"$":{"0":{"v":"Differential Operators","n":0.707},"1":{"v":"> Curl, grad and div walk into a bar, in that order. The bartender can't serve them anything because the bar isn't there anymore.\n\n(Please clap.)","n":0.2}}},{"i":38,"$":{"0":{"v":"Second-Order Operators","n":0.707},"1":{"v":"## Curling the grad\n\nLet's consolidate our study of irrotational (curl-free) and solenoidal (divergence-free) fields. Together, their properties inform us on how the three differential operators - curl, grad and div - combine together. Irrotational fields are conservative fields, meaning that\n$$\n\\nabla \\times \\mathbf{F} = 0 \\iff \\mathbf{F} = \\nabla \\phi\n$$\ncombining together to give\n$$\n\\nabla \\times (\\nabla \\phi) = 0\n$$\nfor all scalar fields $\\phi$. Similarly, solenoidal fields can be written as the curl of another vector field $\\mathbf{A}$:\n$$\n\\nabla \\cdot \\mathbf{F} = 0 \\iff \\mathbf{F} = \\nabla \\times \\mathbf{A}\n$$\nalso combining together to give\n$$\n\\nabla \\cdot (\\nabla \\times \\mathbf{A}) = 0\n$$\nfor a vector field $\\mathbf{A}$ defined everywhere in $\\mathbb{R^3}$. Altogether, we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. For a scalar field $\\phi$ and a vector field $\\mathbf{A}$, $\\text{curl grad } \\phi = 0$ and $\\text{div curl }\\mathbf{A} = 0$. \n\nI am morally and ethically obliged to now repeat the one-liner at the beginning of this section I spent exactly thirty-five seconds thinking up while in the shower, for the world to appreciate its genius with newfound perspective:\n\n> Curl, grad and div walk into a bar, in that order. The bartender can't serve them anything because the bar isn't there anymore.\n\n(Please clap.)\n\nThe above results also have another interesting consequence: the *Helmholtz Decomposition*, stated below.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Helmholtz Decomposition**. Any general vector field $\\mathbf{F}$ in $\\mathbb{R^3}$ can be written as a sum of irrotational and solenoidal vector fields:\n$$\n\\mathbf{F} = \\nabla\\phi + \\nabla \\times \\mathbf{A}\n$$\nA proof will be given later when morale improves.\n\n## The Laplacian\n\nSo $\\text{curl}$ + $\\text{grad}$ and $\\text{div}$ + $\\text{curl}$ turned out to be busts; are there any second-order operators, i.e. a combination of two of curl, grad and div, that (a) are nonzero and (b) have any physical meaning at all beyond being identified by a PhD thesis from a severely underfunded college as the seventh-order time derivative of position of a subatomic muon emitted by a supernova exploding seven quintillion light-years away?\n\nThe answer is yes.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define **the Laplacian on scalar fields** as the second-order operator\n$$\n\\nabla^2 = \\nabla\\cdot \\nabla = \\frac{\\partial^2}{\\partial x_i \\partial x_i}\n$$\n> denoting summation, to be understood as \n$$\n\\nabla^2 \\phi = \\frac{\\partial^2}{\\partial x^2} \\phi + \\frac{\\partial^2}{\\partial y^2}\\phi + \\frac{\\partial^2}{\\partial z^2}\\phi\n$$\n> in the three-dimensional case, and more generally\n$$\n\\nabla^2 \\phi = \\text{div grad }\\phi\n$$\n> where $\\phi$ is a scalar field.\n\nOn vector fields $\\mathbf{F}$, the Laplacian can also be defined via the vector triple product identity\n\n$$\n\\nabla \\times (\\nabla \\times \\mathbf{F}) = \\nabla (\\nabla\\cdot \\mathbf{F}) - (\\nabla \\cdot \\nabla)\\mathbf{F}\n$$\nin which the latter term is the Laplacian $\\nabla^2 \\mathbf{F}$, resulting in\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Laplacian on vector fields**.\n\n$$\n\\nabla^2 \\mathbf{F} = \\nabla(\\nabla\\cdot \\mathbf{F}) - \\nabla \\times (\\nabla \\times \\mathbf{F}) = \\text{grad div }\\mathbf{F} - \\text{curl curl }\\mathbf{F}.\n$$\n\n## Applications of differential operators\n\nUnless you despise physics with every fiber of your being, differential operators do not disappoint as far as applications go. Prime among these are Maxwell's equations. It is rumored in the Book of Genesis ([[Vector Calculus]]) that on the first day of creation, God said:\n\n\n$$\n\\begin{cases}\n\\nabla \\cdot \\mathbf{E} = \\frac{\\rho}{\\epsilon_0} \\\\\n\\nabla \\times \\mathbf{E} = -\\frac{\\partial \\mathbf{B}}{\\partial t}\\\\\n\\nabla \\cdot \\mathbf{B} = 0 \\\\\n\\nabla \\times \\mathbf{B} = \\mu_0(\\mathbf{J} + \\epsilon_0\\frac{\\partial\\mathbf{E}}{\\partial t})\n\\end{cases}\n$$\n\n> And there was light --\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Maxwell's equations** for the vector fields $\\mathbf{E}$ (electric field), $\\mathbf{B}$ (bagnetic field), $\\mathbf{J}$ (distribution of electric currents) and scalar field $\\rho$ (distribution of charge) encompass all we know about electromagnetism and light, or so it is to be believed.\n\nThe Laplacian also makes an entrance, particularly in equations studying diffusion of quantities through space, e.g. the heat flow equation ([[Differential Equations.Partial Differential Equations.Heat Equation]]), which also describes, and I quote, the spread of \"the smell of that guy who didn't shower before lectures through the room\":\n\n$$\n\\frac{\\partial T}{\\partial t} = \\kappa \\nabla^2 T\n$$\n\nand the Schr$\\ddot{o}$dinger equation, which features Schr$\\ddot{o}$dinger saying \"we can't tell what position a particle is in until we observe it\", then the particle saying \"$H(t)|\\psi(t)\\rangle = i\\hat{h}\\frac{\\partial}{\\partial t}|\\psi(t)\\rangle$\", then Schr$\\ddot{o}$dinger saying \"shut up\".\n\n\n\n","n":0.039}}},{"i":39,"$":{"0":{"v":"Orthogonal Curvilinear Coordinates","n":0.577},"1":{"v":"\n## On the oxymoronic qualities of curvilinearity\n\n*Curvilinear* is a seriously funny word; in fact, it might just be out-competing \"thank God I'm an atheist\", \"fun fact\", and \"happy mathematician\" as my favorite oxymoron of all time. Thankfully, though, orthogonal curvilinear coordinates do not satisfy the two conditions\n1. It is curved, and\n2. It is linear;\n\nInstead, they satisfy the conditions detailed below.\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Orthogonal curvilinear coordinates** are any coordinate system $\\mathbf{x} = (x_1,x_2,x_3)$ in $\\mathbb{R^3}$ where the tangent vectors to $\\mathbf{x}$ along the $x_1$-, $x_2$-, and $x_3$-axes form a right-handed orthonormal basis:\n\n$$\n\\frac{\\partial \\mathbf{x}}{\\partial x_i}\\cdot \\frac{\\partial\\mathbf{x}}{\\partial x_j} = \\delta_{ij},\\ (\\frac{\\partial \\mathbf{x}}{\\partial x_j})\\times (\\frac{\\partial\\mathbf{x}}{\\partial x_k}) = \\epsilon_{ijk}\\frac{\\partial\\mathbf{x}}{\\partial x_i}\n$$\n> e.g. $\\partial_{x_2}\\mathbf{x}\\times\\partial_{x_3}\\mathbf{x} = \\partial_{x_1}\\mathbf{x}$.\n\nLet's pick apart this definition one bit at a time.\n\nFirst, for a coordinate system - say the standard Cartesian coordinate system $\\mathbf{x} = (x,y,z)$ - what do the tangent vectors $\\partial_x\\mathbf{x}, \\partial_y\\mathbf{x}, \\partial_z\\mathbf{x}$ indicate? $\\partial_x\\mathbf{x}$, for example, denotes the change of $\\mathbf{x}$ as we move a small step along the $x$-axis; in other words, it indicates the direction the $x$-axis is moving at the point $\\mathbf{x}$ - more commonly known as the basis vector for the $x$-axis, $\\mathbf{e}_x$. \n\nThis clues us in a little bit: we can say much more generally that for any coordinate system $(x_1, x_2, x_3)$, the tangent vectors $\\partial_{x_1} \\mathbf{x}, \\partial_{x_2}\\mathbf{x}, \\partial_{x_3}\\mathbf{x}$ - upon normalization, as convention dictates - points us in the direction of the basis vectors $\\mathbf{e_1, e_2, e_3}$ of the coordinate system at that point. In particular, we call\n\n$$\nh_{x_1} = |\\partial_{x_1} \\mathbf{x}|,\\ h_{x_2} = |\\partial_{x_2} \\mathbf{x}|,\\ h_{x_3} = |\\partial_{x_3} \\mathbf{x}|\n$$\n*scale factors* that tell us the change in length along each coordinate axis.\n\nAn **orthogonal curvilinear coordinate** system is thus one where the basis vectors form a right-handed perpendicular system (**orthogonal**), but not necessarily constant at every point (**curvilinear**). \n\n## Cylindrical and spherical polar coordinates\n\n### Cylindrical coordinates\n\nTake the cylindrical coordinate system in $\\mathbb{R}^3$ as an example of orthogonal curvilinear coordinates:\n$$\n\\mathbf{x} = (x,y,z) = (r\\cos \\theta, r\\sin \\theta, z)\n$$\nresulting in the tangent vectors\n$$\n\\begin{cases}\n\\partial_r\\mathbf{x} = (\\cos \\theta, \\sin \\theta, 0) \\\\\n\\partial_\\theta \\mathbf{x} = (-r\\sin \\theta, r\\cos\\theta, 0) = (-\\sin \\theta, \\cos \\theta, 0) \\text{ (normalized)} \\\\\n\\partial_z \\mathbf{x} = (0,0,1)\n\\end{cases}\n$$\nwith associated scale factors $1, r, 1$ which are mutually orthogonal, but not at all constant. \n\n### Spherical coordinates\n\nFor the coordinate system $(r, \\theta, \\phi)$, we have\n$$\n\\mathbf{x} = (x,y,z) = (r\\sin\\theta\\cos\\phi, r\\sin\\theta\\sin\\phi, r\\cos\\theta)\n$$\nand thus the tangent vectors and scale factors\n$$\n\\begin{cases}\n\\partial_r \\mathbf{x} = (\\sin \\theta \\cos \\phi, \\sin \\theta \\sin \\phi, \\cos \\theta), h_r = 1 \\\\\n\\partial_\\theta \\mathbf{x} = (r\\cos\\theta\\cos\\phi, r\\cos\\theta\\sin\\phi, -r\\sin\\theta), h_\\theta = r \\\\\n\\partial_\\phi \\mathbf{x} = (-r\\sin\\theta\\sin\\phi, r\\sin\\theta\\cos\\phi, 0), h_\\phi = r\\sin\\theta\n\n\\end{cases}\n$$\n\n## Differential operators in curvilinear coordinates\n\n### The gradient and the gradient operator\n\nRecall our previous *coordinate-independent* definition of the gradient $\\nabla f$ of a scalar field $f(\\mathbf{x})$ as being the vector satisfying\n$$\ndf = \\nabla f \\cdot d\\mathbf{x}\n$$\nfor $d\\mathbf{x} \\to 0$, where $d\\mathbf{x}$ is the vector given by\n$$\nd\\mathbf{x} = (\\partial_{x_1}\\mathbf{x})\\ dx_1 + (\\partial_{x_2}\\mathbf{x})\\ dx_2 + (\\partial_{x_3}\\mathbf{x})\\ dx_3\n$$\ni.e. the total change in position resulting from a small change along each of the coordinate axes.\n\nThis definition is valid for any coordinate system $(x_1,x_2,x_3)$, with orthogonal curvilinear basis $(\\mathbf{e_1, e_2, e_3})$. For such a coordinate system, we also have\n$$\ndf = (\\partial_{x_1} f)\\ dx_1 + (\\partial_{x_2} f)\\ dx_2 + (\\partial_{x_3} f)\\ dx_3\n$$\nwith a comparison of coefficients yielding\n$$\n\\begin{aligned}\n(\\partial_{x_1} f)\\ dx_1 + (\\partial_{x_2} f)\\ dx_2 + (\\partial_{x_3} f)\\ dx_3 &= \\nabla f \\cdot ((\\partial_{x_1}\\mathbf{x})\\ dx_1 + (\\partial_{x_2}\\mathbf{x})\\ dx_2 + (\\partial_{x_3}\\mathbf{x})\\ dx_3) \\\\\n&= \\nabla f \\cdot (h_1 \\mathbf{e_1}dx_1 + h_2\\mathbf{e_2}dx_2 + h_3\\mathbf{e_3}dx_3)\n\n\\end{aligned}\n$$\ncombining all the above. Suppose now that the gradient vector $\\nabla f$ can be written in the orthogonal curvilinear basis $(\\mathbf{e_1, e_2, e_3})$ as \n$$\n\\nabla f = \\alpha_1 \\mathbf{e_1} + \\alpha_2 \\mathbf{e_2} + \\alpha_3 \\mathbf{e_3}\n$$\nBy the orthonormality of this basis, we obtain the expression\n$$\n\\begin{aligned}\n&(\\partial_{x_1} f)\\ dx_1 + (\\partial_{x_2} f)\\ dx_2 + (\\partial_{x_3} f)\\ dx_3 = \\nabla f \\cdot (h_1 \\mathbf{e_1}dx_1 + h_2\\mathbf{e_2}dx_2 + h_3\\mathbf{e_3}dx_3) \\\\\n&= (\\alpha_1 \\mathbf{e_1} + \\alpha_2 \\mathbf{e_2} + \\alpha_3 \\mathbf{e_3})\\cdot (h_1 \\mathbf{e_1}dx_1 + h_2\\mathbf{e_2}dx_2 + h_3\\mathbf{e_3}dx_3) \\\\\n&= \\alpha_1 h_1 dx_1 + \\alpha_2 h_2 dx_2 + \\alpha_3 h_3 dx_3 \\text{           \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ as $\\mathbf{e_i\\cdot e_i} = 1$} \\\\\n\\end{aligned}\n$$\nComparing coefficients of $dx_1, dx_2$ and $dx_3$ now yield\n$$\n\\begin{cases}\n\\alpha_1 = \\frac{1}{h_1}(\\partial_{x_1} f) \\\\\n\\alpha_2 = \\frac{1}{h_2}(\\partial_{x_2} f) \\\\\n\\alpha_3 = \\frac{1}{h_3}(\\partial_{x_3} f) \\\\\n\\end{cases}\n$$\nand thus \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **The gradient in general orthogonal curvilinear coordinates $(x_1,x_2,x_3)$.**\n$$\n\\nabla f = \\frac{1}{h_1}(\\partial_{x_1} f)\\mathbf{e_1} + \\frac{1}{h_2}(\\partial_{x_2} f)\\mathbf{e_2} + \\frac{1}{h_3}(\\partial_{x_3} f)\\mathbf{e_3}\n$$\nwith the newfound scale-factor terms in the denominator intuitively understood as \"scaling down\" the rates of change in the $x_1$, $x_2$ and $x_3$ axes (for instance, a single step in the $x_1$ axis is worth $h_1$ length-units instead of $1$ length-unit, so $\\partial_{x_1}f$ is $h_1$ times longer than it should be).\n\nAssociated with this is\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **The gradient operator in curvilinear coordinates**, derived from above as\n$$\n\\nabla = \\frac{\\mathbf{e_1}}{h_1} \\partial_{x_1} + \\frac{\\mathbf{e_2}}{h_2} \\partial_{x_2} + \\frac{\\mathbf{e_3}}{h_3} \\partial_{x_3}.\n$$\nThe abstracted gradient operator can then be used to derive expressions for divergence and curl in different coordinate systems; unfortunately, this stone shall be left unturned until we introduce the Big Integral Theorems.\n","n":0.034}}},{"i":40,"$":{"0":{"v":"Basic Definitions","n":0.707},"1":{"v":"\n## Gradient and the Gradient Operator\n\n### Defining the Gradient 2: Electric Boogaloo\n\nThe gradient of a scalar field $\\phi(\\mathbf{x}): \\mathbb{R}^n \\to \\mathbb{R}$ is defined as the vector\n$$\n\\frac{\\partial\\phi}{\\partial x_i}\\mathbf{e_i},\n$$\nitself a vector field, where $\\mathbf{e_i}$ is the standard orthonormal basis in Cartesian coordinates; this we are well-aware, yet the necessity of restricting the gradient to Cartesian coordinates limits our understanding. If the gradient is the multivariable analogue of the derivative, is it possible to define the gradient independently of choice of coordinates, based simply on first principles?\n\nConsider the definition of the derivative for uni-variate functions:\n$$\nf'(x) = \\lim_{h\\to 0}\\frac{f(x+h)-f(x)}{h}\n$$\nor, for $|h| << 1$,\n$$\nf(x+h) = f(x) + hf'(x) + O(h^2)\n$$\nIn the multi-variable case, the above holds for every single variable the function is subjected to:\n$$\nf(\\mathbf{x}+\\mathbf{h}) = f(\\mathbf{x}) + \\mathbf{h}_1f_{x_1}(\\mathbf{x}) + \\mathbf{h_2}f_{x_2}(\\mathbf{x}) + ... + \\mathbf{h_n}f_{x_n}(\\mathbf{x}) + O(\\mathbf{h}^2)\n$$\nand thus \n$$\nf(\\mathbf{x+h}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x}) \\cdot \\mathbf{h} + O(\\mathbf{h}^2).\n$$\nWe arrive at\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **coordinate-independent definition of the gradient** defines the gradient $\\nabla f(\\mathbf{x})$ at a given vector-valued point $\\mathbf{x}$ as the vector that satisfies\n$$\nf(\\mathbf{x+h}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x}) \\cdot \\mathbf{h} \n$$\n> as $\\mathbf{h} \\to \\mathbf{0}$.\n\n### The Gradient Operator\n\nIf there's anything mathematicians are great at, it's getting divorced (math comes a distant second), so it's not surprising that we proceed towards the next stage of our mathematical journey via the forceful separation of two mathematical objects that were once joined together by the decree of God. Behold:\n$$\n\\nabla\n$$\nThere is nothing more terrifying.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **gradient operator**, denoted $\\nabla$ without the following function, as the quasi-vector-like object\n$$\n\\begin{bmatrix}\n\\frac{\\partial}{\\partial x_1} \\\\\n\\frac{\\partial}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial x_n}\n\\end{bmatrix}\n$$\n> which doesn't make much sense alone, but awaits a scalar function to form the **gradient** $\\nabla f$ defined above.\n\n## Curl and Divergence\n\nThe existence of the gradient operator, i.e. a vector-form representation of what $\\nabla$ does to scalar functions, allows us to generalize the gradient to vector fields $\\mathbf{F}: \\mathbb{R^n \\to R^n}$. \n\nFor a vector field, $\\nabla \\mathbf{F}$ - the gradient in its previously-understood meaning, i.e. a vector containing each of the partial derivatives of $\\mathbf{F}$ - makes little sense because each partial derivative is also a vector. What **does** make sense, however, is the following:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **divergence** of a vector field $\\mathbf{F}: \\mathbb{R^n \\to R^n}$ as the scalar product $\\nabla \\cdot \\mathbf{F}$ with the gradient operator $\\nabla$ as above, outputting a scalar field\n$$\n\\text{div } \\mathbf{F} = \\nabla \\cdot \\mathbf{F} = \\frac{\\partial F_i}{\\partial x_i}\n$$\n> understood as index notation.\n\nBesides the scalar-valued analogue obtained via the dot (scalar) product, we naturally come to the vector-valued analogue obtained via the vector (cross) product, viable only in $\\mathbb{R^3}$:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **curl** of a vector field $\\mathbf{F}$ **exclusively in $\\mathbb{R^3}$** as the vector product $\\nabla \\times \\mathbf{F}$, outputting a vector field\n$$\n\\text{curl }\\mathbf{F} = \\nabla \\times \\mathbf{F} = \\begin{vmatrix}\n\\mathbf{e_1} & \\mathbf{e_2} & \\mathbf{e_3} \\\\\n\\frac{\\partial}{\\partial x_\n1} & \\frac{\\partial}{\\partial x_2} & \\frac{\\partial}{\\partial x_3} \\\\\nF_1 & F_2 & F_3\n\\end{vmatrix}\n$$\n> which, when fully expanded, equals \n$$\n\\mathbf{e_1}(\\frac{\\partial F_3}{\\partial x_2} - \\frac{\\partial F_2}{\\partial x_3}) + \\mathbf{e_2} (\\frac{\\partial F_1}{\\partial x_3} - \\frac{\\partial F_3}{\\partial x_1}) + \\mathbf{e_3} (\\frac{\\partial F_2}{\\partial x_1} - \\frac{\\partial F_1}{\\partial x_2}).\n$$\n\n### Interpreting curl and divergence\n\n**Divergence**. Consider a single term (non-suffix notation) within the dot product that defines divergence:\n$$\n\\nabla \\cdot \\mathbf{F} = \\sum_{i=1}^n \\frac{\\partial F_i}{\\partial x_i}\n$$\n$F_i$ is the component of $\\mathbf{F}$ along the $x_i$-axis, and so this term asks the question \"how fast does the $i$th component of $\\mathbf{F}$ change, or **diverge**, along the $x_i$ axis?\" \n\nAdding up the rate of change along all axes at a point gives a value that captures the **total** rate of change of $\\mathbf{F}$ at that point: whether $\\mathbf{F}$ is coming into, or leaving, that point. \n\nIf divergence is positive at a point $\\mathbf{x}$, then the amount of $\\mathbf{F}$ leaving the point exceeds the amount of $\\mathbf{F}$ entering the point, and so we call $\\mathbf{x}$ a **source**; conversely, if divergence is negative, then $\\mathbf{F}$ enters the point more than it leaves, and so the point is a **sink**. \n\n**Curl**. Curl is an immortal eldritch abomination delivered to the mortal plane from the ninth circle of Hell that defies earthly interpretation or understanding. (Also, it measures rotation.)\n\n### Basic properties\n\n> <span style=\"background-color: #ffb812; color: black;\">Property 1</span>. **The linear property**. Grad, div and curl fulfill the properties of **a linear transformation**:\n\n$$\n\\begin{cases}\n\\nabla (\\alpha f + \\beta g) = \\alpha \\nabla f + \\beta \\nabla g \\\\\n\\nabla \\cdot (\\alpha\\mathbf{F} + \\beta \\mathbf{G}) = \\alpha (\\nabla \\cdot \\mathbf{F}) + \\beta (\\nabla \\cdot \\mathbf{G}) \\\\\n\\nabla \\times (\\alpha \\mathbf{F} + \\beta \\mathbf{G}) = \\alpha (\\nabla \\times \\mathbf{F}) + \\beta (\\nabla \\times \\mathbf{G})\n\\end{cases}\n$$\n> for scalar fields $f$ and $g$, vector fields $\\mathbf{F}$ and $\\mathbf{G}$, and constants $\\alpha$ and $\\beta$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property 2</span>. **The Leibniz property**. Each of grad, div and curl fulfill some generalization of the product rule:\n\n$$\n\\begin{cases}\n\\nabla(fg) = (\\nabla f) g + f(\\nabla g) \\\\\n\\nabla \\cdot (\\mathbf{F}g) = (\\nabla \\cdot \\mathbf{F})g + \\mathbf{F} \\cdot (\\nabla g) \\\\\n\\nabla \\times (\\mathbf{F}g) = (\\nabla \\times \\mathbf{F})g + (\\nabla g)\\times \\mathbf{F}\n\\end{cases}\n$$\n> noting that the last term is $(\\nabla g) \\times \\mathbf{F}$, rather than the other way around.\n\nFurther properties follow from dot-product and cross-product properties of vectors, which $\\nabla$ counts itself among.\n\n## Irrotational and solenoidal fields\n\n### Irrotational fields\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. If we accept unambiguously and wholly without justification that the curl of a vector field $\\mathbf{F}$, $\\nabla \\times \\mathbf{F}$, quantifies its rotation at a point, then we find ourselves obliged to define an **irrotational field** - a field which has no rotation - as that which satisfies\n$$\n\\nabla \\times \\mathbf{F} = 0\n$$\n> at all points.\n\nThe following theorem provides a connection between irrotational fields and conservative fields:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. If $\\mathbf{F}$ is a vector field defined everywhere on $\\mathbb{R^3}$, then irrotational implies conservatism:\n$$\n\\nabla \\times \\mathbf{F} = 0 \\iff \\mathbf{F} = \\nabla \\phi\n$$\n> for some scalar field $\\phi$.\n\nRonald Reagan *would* be spinning in his grave, but he can't, because conservative implies irrotational.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nIf $\\mathbf{F} = \\nabla \\phi$, then we have\n$$\n\\begin{aligned}\n\\nabla \\times \\mathbf{F} &= \\nabla \\times (\\nabla \\phi) \\\\\n&= \\nabla \\times (\\frac{\\partial \\phi}{\\partial x_i}) \\\\\n&= \\epsilon_{ijk} \\frac{\\partial}{\\partial x_j}\\frac{\\partial}{\\partial x_k} \\phi \\\\\n&= 0\n\\end{aligned}\n$$\narguing that for every permutation $\\epsilon_{ijk}$ and its corresponding $\\epsilon_{ikj}$, the mixed second derivatives\n$$\n\\frac{\\partial^2\\phi}{\\partial x_j \\partial x_k} = \\frac{\\partial^2 \\phi}{\\partial x_k \\partial x_j}\n$$\nare equal, but $\\epsilon_{ijk} = -\\epsilon_{ikj}$, forming a pair that sums to zero.\n\nConversely, we find the nearest corner, curl up into fetal position, and cry for the next half-hour. (Or at least, until le Grande Théorème Intégral swoops in and saves us - but that'll be much later!)\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. The above theorem implies that the curl of conservative fields is zero:\n$$\n\\nabla\\times (\\nabla \\phi) = 0\n$$\n> for some scalar field $\\phi$.\n\n\n### Solenoidal fields\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define a **solenoidal** or a **divergence-free** field as a vector field $\\mathbf{F}$ which satisfies $\\nabla \\cdot \\mathbf{F} = 0$ at every point where it is defined.\n\nSolenoidal fields derive their names from **solenoids**: coils of wires that produce magnetic fields $\\mathbf{B}$ which are divergence-free, seen below.\n\n![alt text](./assets/images/image-23.png)\n\nNo point in the field can be understood as a source or sink; the field leaves and enters each point in equal amounts. Solenoidal fields connect themselves to the concept of curl through the following theorem:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Given a solenoidal field $\\mathbf{F}$ satisfying $\\nabla \\cdot \\mathbf{F} = 0$, we have\n$$\n\\mathbf{F = \\nabla \\times A}\n$$\n> for some vector field $\\mathbf{A}$, provided that $\\mathbf{F}$ is defined everywhere on $\\mathbb{R^3}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nAgain, the forward direction is fairly straightforward: if $\\mathbf{F = \\nabla \\times A}$, then we have\n$$\n\\begin{aligned}\n\\nabla \\cdot \\mathbf{F} &= \\nabla \\cdot (\\nabla \\times \\mathbf{A}) \\\\\n&= \\nabla \\cdot (\\epsilon_{ijk}\\frac{\\partial}{\\partial x_j}A_k) \\\\\n&= \\frac{\\partial}{\\partial x_i}(\\epsilon_{ijk}\\frac{\\partial}{\\partial x_j}A_k)\n\\end{aligned}\n$$\nwhich vanishes due to symmetry.\n\nConversely, we now are no longer reduced to crying and vomiting; instead, for an arbitrary point in the plane $(\\mathbf{x_0,y_0,z_0})$, we claim an actual solution for $\\mathbf{F} = \\nabla \\times \\mathbf{A}$ in the form\n$$\n\\mathbf{A(x)} = (\\int_{z_0}^z F_y(x,y,z')\\ dz', \\int_{x_0}^x F_z(x',y,z_0)\\ dx' - \\int_{z_0}^z F_x(x,y,z')\\ dz',\\ 0)\n$$\nwhere $x'$ and $z'$ are meant to be dummy variables with no other purpose than to satisfy the cosmic whims of the Fundamental Theorem of Calculus. This construction is neither unique nor aesthetically pleasing nor comprehensible to the human mind, but its saving grace is that its $z$-component $A_z = 0$, simplifying the formula for curl:\n$$\n\\nabla \\times \\mathbf{A} = \\begin{vmatrix}\n\\mathbf{e_1} & \\mathbf{e_2} & \\mathbf{e_3} \\\\\n\\partial_x & \\partial_y & \\partial_z \\\\\nA_x & A_y & A_z\n\\end{vmatrix} = \\begin{vmatrix}\n\\mathbf{e_1} & \\mathbf{e_2} & \\mathbf{e_3} \\\\\n\\partial_x & \\partial_y & \\partial_z \\\\\nA_x & A_y & 0\n\\end{vmatrix}\n$$\nresulting in\n$$\n\\nabla \\times \\mathbf{A} = \\mathbf{e_1}(-\\partial_z A_y) + \\mathbf{e_2}(\\partial_z A_x) + \\mathbf{e_3} (\\partial_x A_y - \\partial_y A_x).\n$$\nLet's check the terms one by one. FIrst, $-\\partial_z A_y$ is\n$$\n\\begin{aligned}\n-\\frac{\\partial}{\\partial z}(\\int_{x_0}^x F_z(x',y,z_0)\\ dx' - \\int_{z_0}^z F_x(x,y,z')\\ dz') \\\\ \n= F_x(x,y,z) - 0 = F_x(x,y,z)\n\\end{aligned}\n$$\nby the Fundamental Theorem, and because the first integral in the expression is not a function of $z$. Similarly, $\\partial_z A_x$ yields $F_y(x,y,z)$ with even less kicking and screaming. Finally, \n$$\n\\begin{aligned}\n\\partial_x A_y - \\partial_y A_x \\\\ = \\partial_x (\\int_{x_0}^x F_z(x',y,z_0)\\ dx' - \\int_{z_0}^z F_x(x,y,z')\\ dz') - \\partial_y (\\int_{z_0}^z F_y(x,y,z')\\ dz') \\\\\n= F_z(x,y,z_0) - \\int_{z_0}^z \\frac{\\partial F_x}{\\partial x}(x,y,z')\\ dz' - \\int_{z_0}^{z}\\frac{\\partial F_y}{\\partial y} F_y(x,y,z')\\ dz' \n\\end{aligned}\n$$\ndifferentiating under the integral sign. As previously we had $\\nabla \\cdot \\mathbf{F} = 0$ by definition, we have\n$$\n\\partial_x F_x + \\partial_y F_y + \\partial_z F_z = 0 \\iff \\partial_z F_z = -(\\partial_x F_x + \\partial_y F_y)\n$$\nleading to \n$$\n\\begin{aligned}\nF_z(x,y,z_0) - \\int_{z_0}^z \\frac{\\partial F_x}{\\partial x}(x,y,z')\\ dz' - \\int_{z_0}^{z}\\frac{\\partial F_y}{\\partial y} F_y(x,y,z')\\ dz' \\\\\n= F_z(x,y,z_0) - \\int_{z_0}^z (\\partial_x F_x + \\partial_y F_y) (x,y,z')\\ dz' \\\\\n= F_z(x,y,z_0) + \\int_{z_0}^z (\\partial_z F_z)(x,y,z')\\ dz' \\\\\n= F_z(x,y,z_0) + F_z(x,y,z) - F_z(x,y,z_0) \\\\\n= F_z(x,y,z)\n\\end{aligned}\n$$\nas desired.\n\n","n":0.025}}},{"i":41,"$":{"0":{"v":"Curves and Surfaces","n":0.577},"1":{"v":"> ~~If straight lines are just special examples of curves that aren't curved, are straight people are just special examples of gay people who aren't gay?~~\n\n\n","n":0.196}}},{"i":42,"$":{"0":{"v":"Surfaces","n":1},"1":{"v":"> The difference between other mathematicians and me is what we do with smooth surfaces. Other mathematicians merely think **about** smooth surfaces; I think **with** a smooth surface.\n\n## Representations of surfaces\n\nA **curve** is a set of points represented by a single parameter; a **surface** is an extension of the notion of a curve to two parameters, i.e.\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **parametric surface** $\\mathbf{x = r}(u,v)$, with two parameters $u, v$, is either (again, depending on whether you're asking the question in a quant firm or a mental institution) a locus of points mapped to by a certain mapping from $u,v$, or that mapping itself.\n\nAs with curves, the representation of a surface is far from unique; they come in various shapes, sizes, tastes, and colors, from **parametric form** (as above), to explicit representations, to PG-13 representations, to silver with 8GB of RAM, 512GB of disk space, and a OLED 13.8'' touchscreen which kicked the bucket two seconds after my extended warranty expired. (Screw you, Microsoft!) Examples of these representations are given below:\n\n1. **Explicit representations** (NSFW). Directly representing surfaces in Cartesian coordinates without parameterization ~~(gone wrong) (gone sexual)~~:\n$$\nz = f(x,y)\n$$\nFor instance, a hemisphere of radius $a$ is\n$$\nz = \\sqrt{a^2 - x^2 - y^2}\n$$\n\n2. **Implicit representations** are explicit representations, but in the form $F(x,y,z) = 0$:\n$$\nx^2 + y^2 + z^2 - a^2 = 0.\n$$\n\n3. **Parametric representations** map parameters $u$, $v$ to points $\\mathbf{x}$ on the surface:\n$$\n\\mathbf{x} = (a\\sin u \\cos v, a \\sin u \\sin v, a \\cos u)\n$$\n\nOf which the last two forms are not unique.\n\n## Normals to surfaces\n\nWe draw upon our definition of a **tangent plane** to a function of two variables to define a normal to a surface. For a curve\n$$\n\\mathbf{x} = r(u,v)\n$$\nat a point $(u_0, v_0)$, two **curves** $x_1 = r(u_0, v)$ and $x_2 = r(u, v_0)$ can be taken by treating each of $u$, $v$ as constant and the other as a variable; each of these curves has tangent vector \n$$\n\\frac{\\partial r}{\\partial u}(u_0, v_0),\\ \\frac{\\partial r}{\\partial v}(u_0, v_0)\n$$\nrespectively at $(u_0, v_0)$. By our previous definition for a general two-variable function, the tangent plane at $u_0, v_0$ is the plane uniquely determined by two tangents to the surface; as such the tangent plane to $\\mathbf{x} = r(u,v)$ has normal vector\n$$\n\\frac{\\partial r}{\\partial u}(u_0, v_0) \\times \\frac{\\partial r}{\\partial v}(u_0, v_0) = \\mathbf{x}_u \\times \\mathbf{x}_v\n$$\n\nand thus\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The **unit normal vector** to a surface $\\mathbf{x} = \\mathbf{r}(u,v)$ at a point $(u_0, v_0)$ is given by\n$$\n\\hat{\\mathbf{n}} = \\frac{\\mathbf{x}_u \\times \\mathbf{x}_v}{|\\mathbf{x}_u \\times \\mathbf{x}_v|}.\n$$\n\nAlternatively, if $\\mathbf{x}$ is instead in parametric form $F(\\mathbf{x}) = 0$, the gradient $\\nabla F$ represents the normal vector (as it is normal to all level curves of the surface).\n\n## Classification of surfaces\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **smooth surface** is a surface with a unique normal at every point, excepting a sign, and with a direction that depends continuously on the position of the surface. A **piecewise smooth** surface is a surface which can be divided into a finite number of smooth surfaces.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Choose the **positive normal direction** of a smooth surface $S$ at a point $P$ as either the direction of the positive normal vector to $S$ at $P$, or the negative normal vector. If this positive normal direction is consistent for every point $P$ on $S$, then $S$ is an **orientable** surface.\n\nAn orientable surface can be understood to \"have two sides\"; a piece of paper is an orientable surface, but a Mobius strip is not (only has one side).\n\n![alt text](./assets/images/image-11.png)\n\n(Note that the \"positive normal direction\" suddenly changes when you get to the end of the strip!)\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **closed surface** is one that can contain water and have it not spill from a single place; it bounds a solid object. Examples of closed surfaces: spheres, donuts, JFK's head before Nov. 22, 1963. Examples of open surfaces: a piece of paper, my bedroom ceilings, JFK's head after Nov. 22, 1963.\n\nA straight line (excluding tangents, but we don't talk about that) will intersect a closed surface an even number of times (2 for a round balloon, 4 for a sausage balloon, 10,036 for a caterpillar balloon, etc., etc.)\n\nThe Arch of Honors (see \"Review\") can be formed with your right hand to determine the positive normal direction of the surface at a point. \n\n## Stationary points in surfaces   \n\nIn extending the notion of maximum and minimum points to functions $f: \\mathbb{R^m \\to R}$, we define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Suppose that the above function $f$ is defined on a domain $D$. A point $\\mathbf{a} \\in D$ is a **local maximum** or a **local minimum** if for all $\\mathbf{x} \\in D$ **sufficiently close** to $\\mathbf{a}$, we have either\n$$\nf(\\mathbf{x}) < f(\\mathbf{a}) \\text{ or } f(\\mathbf{x}) > f(\\mathbf{a}).\n$$\n\nNote that \"sufficiently close\" indicates infinitesimal closeness, i.e. $|\\mathbf{x} - \\mathbf{a}| \\to 0$.\n\nSimultaneously, define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **weak local minimum or maximum** corresponds with the above, except with\n\n$$\nf(\\mathbf{x}) \\leq f(\\mathbf{a}) \\text{ or } f(\\mathbf{x}) \\geq f(\\mathbf{a}).\n$$\n\nCall a strict local maximum or minimum an **extremum**,, and a weak local maximum or minimum a **weak extremum**.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. Local extremums are not necessarily global extremums (i.e. maximum or minimum over the entire domain).\n\nWe define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> A **critical point** or **stationary point** $\\mathbf{a}$ of the function $f$ is one where $\\nabla f(\\mathbf{a}) = \\mathbf{0}$.\n\nAnd indeed, as with the univariate case, we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. All extremums on **the interior** of the domain $D$ of $f$ are critical points (excluding those on the boundary).\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> by contradiction (shocking)!\n\nSuppose that a (strong or weak) extremum $\\mathbf{a}$ is **not** a critical point of a function $f$, i.e. $\\nabla f(\\mathbf{a}) \\neq 0$. This implies that for at least one of $x_1, x_2, ..., x_m$, say $x_i$, we have\n$$\n\\frac{\\partial f}{\\partial x_i} = \\lim_{h\\to 0}\\frac{f(\\mathbf{a} + h_i) - f(\\mathbf{a})}{h} \\neq 0\n$$\nSuppose, without loss of generality, that the above expression is greater than zero. However, we must now also have\n$$\n\\frac{\\partial f}{\\partial x_i} = \\lim_{h\\to 0}\\frac{f(\\mathbf{a}) - f(\\mathbf{a}- h_i)}{h} > 0\n$$\nwhich leads to\n$$\n\\begin{cases}\nf(a+h_i) - f(a) > 0 \\\\\nf(a-h_i) - f(a) < 0\n\\end{cases}\n$$\nAs $a+h_i$ and $a-h_i$ are both \"sufficiently close\" to $a$, and one is larger and one is smaller than $f(a)$, $a$ is not an extremum by definition. Points on the boundary are excluded from this argument beecause there will either not be an $a + h_i$ or an $a - h_i$ (you cannot go any distance beyond the boundary, no matter how small).\n\nIn simpler words, the crux of the argument is this: if the gradient is nonzero, then the partial derivative is nonzero in some direction. The partial derivative tells you the rate of change of $f$ in that direction, so if you travel in that direction you can increase or decrease the value of the function to your liking - which is not what happens at an extremum, where you can only either increase or decrease the function.\n\nHowever, it is important to note that although all extremums are critical points, not all critical points are extremums:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Critical points which are not extremums are called **saddle points**, analogous to points of inflection in 1D; they occur when the function is at a maximum in one direction and at a minimum in another direction.\n\nTo reach a local maximum or minimum, the function **has to be** a maximum or minimum no matter what direction you're looking at it; for instance, if you sliced $f(x,y)$ along the $x$ and $y$ axes, both cross-sections have to be minimums. \n\nAs the number of variables increases, it is heuristically more and more unlikely that minimums or maximums can coincide in every axis - for instance, a function of 10 variables would have to be a minimum in the $x_1, x_2, ..., x_{10}$ planes to be an actual local minimum - and thus saddle points are expected to be much greater in quantity than extremums.\n\n## Nature of multivariate stationary points\n\n(Shamelessly copied from notes on [[Linear Algebra]] and [[Differential Equations]].)\n\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Hessian matrix**.\n\nRecall from **Differential Equations** that the **Hessian matrix** for a multivariate function $f(x_1,x_2,...,x_n),\\ \\mathbb{R^n \\to R},$ of $n$ variables was defined\n$$\nH_{ij} = \\frac{\\partial f}{\\partial x_i x_j}\n$$\nwith the matrix-valued function $H(\\mathbf{a})$ for some $\\mathbf{a}\\in\\mathbb{R}^n$ defined as \n$$\nH_{ij}(\\mathbf{a}) = \\frac{\\partial f}{\\partial x_i x_j}(\\mathbf{a}).\n$$\nIf $f$ has continuous second-order derivatives, recall that the mixed second-order derivatives commute, i.e.\n$$\nH_{ij} = \\frac{\\partial f}{\\partial x_i x_j} = \\frac{\\partial f}{\\partial x_j x_i} = H_{ji}\n$$\nmeaning that $H$ is symmetric. Suppose that $f(\\mathbf{a})$ is a critical point of $f$, $\\mathbf{a}\\in\\mathbb{R}^n$: $\\frac{\\partial f}{\\partial x_i} (\\mathbf{a}) = 0$ for all $i= 1, 2, ..., n$. To analyze whether $\\mathbf{a}$ is a maximum, a minimum, or a saddle point, we consider a small perturbation $\\mathbf{a + x}$ near $\\mathbf{a}$ with $|\\mathbf{x}|<<1$. By Taylor's theorem, we have\n$$\nf(\\mathbf{a}+ \\mathbf{x}) = f(\\mathbf{a}) + \\sum_{i=1}^n \\mathbf{x}_i \\frac{\\partial f}{\\partial x_i} (\\mathbf{a}) +\\frac{1}{2} \\sum_{i=1}^n\\sum_{j=1}^n \\mathbf{x}_i \\frac{\\partial^2 f}{\\partial x_i x_j} (\\mathbf{a})\\mathbf{x}_j\n$$\nplus terms with powers of $\\mathbf{x}$ above $2$. The term containing the first derivatives are zero as the first derivatives themselves are zero. The second-derivative term can be rewritten in matrix form as\n$$\n\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\mathbf{x}_i H_{ij}(\\mathbf{a})\\mathbf{x}_j = \\frac{1}{2}\\mathbf{x}^{T} H(\\mathbf{a})\\mathbf{x}\n$$\nThis is a form with a real symmetric associated coefficient matrix, and as real symmetric matrices are diagonalizable, it can be simplified to the form\n$$\n(\\mathbf{x}')^T \\Lambda \\mathbf{x}\n$$\nwhere $\\Lambda$ is its diagonal matrix of eigenvalues, which allows us to write\n$$\nf(\\mathbf{a+x})-f(\\mathbf{a})\\approx \\frac{1}{2}(\\mathbf{x}')^T \\Lambda \\mathbf{x} = \\sum_{i=1}^n \\lambda_i(x_i)^2,\n$$\nwhich is positive if all the $\\lambda_i$ are positive, negative if all the $\\lambda_i$ are negative, and indeterminate if there is at least one positive and one negative eigenvalue. These cases correspond to a minimum point (surrounding point $f(\\mathbf{a+x})$ larger than $f(\\mathbf{a})$), a maximum point (surrounding point $f(\\mathbf{a+x})$ smaller than $f(\\mathbf{a})$), and a saddle point respectively. \n\nThus we arrive at the \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **(Hessian determinant test.)** The Hessian determinant test for classifying stationary points of multivariate functions is as follows. Let $H = \\nabla \\nabla f(x,y)$ be the Hessian matrix as defined above:\n\n$$\nH = \\begin{bmatrix}\n            f_{xx} & f_{xy} \\\\\n            f_{yx} & f_{yy}\n        \\end{bmatrix}\n$$\n\n> If the point $(x_0,y_0)$ with associated Hessian $H(x_0,y_0)$ is a stationary point, then we can classify its nature as follows:\n\n$$\n\\begin{cases}\n            \\det H>0,\\ f_{xx}(x_0,y_0) > 0: \\text{ $(x_0, y_0)$ is a minimum.}\\\\\n            \\det H>0,\\ f_{xx}(x_0,y_0) < 0: \\text{ $(x_0, y_0)$ is a maximum.}\\\\\n            \\det H<0: \\text{ $(x_0, y_0)$ is a saddle point.}\\\\\n            \\det H = 0: \\text{ the point is indeterminate in nature.}\n        \\end{cases}\n$$\n","n":0.024}}},{"i":43,"$":{"0":{"v":"Curves","n":1},"1":{"v":"\n> The world has a lot of curves: algebraic curves, rational curves, curved toenails, and Curves Women's Health and Fitness Club. Curves are occasionally useful: for example, the bell curve measures the distribution of human intelligence, and I flatter myself to say that my well-developed mind lies atop its very peak. \n\n![alt text](./assets/images/image-10.png)\n\n## What are curves?\n\nSo anyways, what are curves?\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Different people will tell you different things about curves: pure mathematicians, applied mathematicians, that one gym dude whose vocabulary consists of the word \"gains\" and nothing else, and spacetime-studying physicists when in the presence of your mom. But for the purpose of this course, a curve is the parametric equation\n$$\n\\mathbf{x} = \\psi(t)\n$$\n> where $\\mathbf{x}$ is a vector representing a point on the curve, and $t$ is a parameter that varies along the curve.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remarks</span>.\n1. Intuitively, this definition tells us that a curve is **one-dimensional** - it has only length but no width or depth. Of course, the curve may exist in multi-dimensional space, but it only has one parameter $t$ that varies as you ride along its valleys and peaks.\n2. Pure mathematicians define a curve to be a mapping from a parameter $t$ to a set of points on the curve. Corrupt mathematicians take their paychecks at the end of every month and don't ask questions. (Applied mathematicians define a curve to be the **set of points** itself rather than the mapping that maps to that set of points).\n3. A curve can be parametrized in more than one way. For example, $x^2 + y^2 = 1$ can be parametrized as $(\\cos t, \\sin t)$ or as the slightly more unhinged\n$$\n(x,y)= (\\frac{1-t^2}{1+t^2}, \\frac{2t}{1+t^2})\n$$\n\n(why would you do this?)\n\n## Classification of curves\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **simple curve** is a curve that does not intersect or touch itself (except at the endpoints, in which case it is simple and closed). A **closed curve** is a curve whose initial point corresponds with its endpoint, i.e. if the curve $\\mathbf{x = \\psi}(t)$ is defined over the range $a < b < t$ only, then $\\psi(a) = \\psi(b)$. A **preda-curve** is a curve which touches other curves without their consent.\n\n![alt text](./assets/images/image-7.png)\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A curve is **smooth** if its derivative exists at every point; a curve is **piecewise smooth** if it can be divided into a finite number of smooth curves; and a curve is **awkward** if it struggles to strike up conversations with other curves.\n\n## Length of a simple curve\n\nSuppose that $\\mathbf{x}$ is a simple curve defined by $\\mathbf{x} = \\psi(t)$ within the range $t_a < t < t_b$. A rigorous definition of the **length of the curve** is given as follows:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Length of a simple curve**.\n\nLet a **partition** of the interval $t_a < t < t_b$ into $N$ smaller intervals be given by $(t_0, t_1), (t_1, t_2), ..., (t_{N-1}, t_N)$, in which $t_a = t_0 < t_1 < t_2 < ... < t_N = t_b$. This partition also partitions the curve itself into $N$ smaller intervals\n$$\n(\\psi(t_0), \\psi(t_1)), (\\psi(t_1), \\psi(t_2)), ..., (\\psi(t_{N-1}, \\psi(t_N)))\n$$\nwhere joining the endpoints of each intervals yields $N$ lines inscribed within the curve:\n\n![alt text](./assets/images/image-8.png)\n\nThe combined length of the lines provide a **discrete approximation** for the actual length of the curve, the accuracy of which improves as the number of partitions $N$ is increased. Denote the total length of the $N$ lines over a partition $D$ as\n$$\nl_D = \\sum_{i=1}^N |\\psi(t_i) - \\psi(t_{i-1})|\n$$\nindicating the Euclidean distance, and thus define the **length of the curve** $\\mathbf{x} = \\psi(t)$ as\n$$\n\\sup_{D} l_D,\n$$\nwhere $\\sup$, pronounced 'sup (as in, \"**'sup**, homeslice?\"), denotes the maximum value $l_D$ can attain over all possible $D$. In essence, if $D$ partitions the curve into infinitely small pieces, then the discrete approximation becomes continuous reality and $l_D$ as defined equals the actual length of the curve; this can be better represented by\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Integral representation of the arc length of a curve**. Let $\\mathbf{x} = \\psi(t),\\ t_a < t < t_b$ be the curve above. Then the **arc length** of the curve from $t_1$ to $t_2$, with $t_a < t_1 < t_2 < t_b$, is given by the integral\n$$\n\\int_{t_1}^{t_2} |\\psi'(t)|\\ dt\n$$\n> where $|\\psi'(t)|$ denotes the norm. Further denote the length of the curve from its starting point $t_a$ to some $t$ as\n$$\ns(t) = \\int_{t_a}^{t} |\\psi'(t)|\\ dt\n$$\n> as above.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nProceed from the discrete approximation of the curve length given above. Suppose that the curve lies in $n$-dimensional space, represented by the point\n$$\n(x_1(t), x_2(t), ..., x_n(t))\n$$\nwhen its parameter equals $t$.\nThe length of any subsection $(\\psi(t_{i-1}), \\psi(t_{i}))$ is given by $|\\psi(t_i) - \\psi(t_{i-1})|$; if $\\psi(t) \\in \\mathbb{R^n}$ for some $n$, it is also given by the expression\n$$\n|\\Delta \\psi| = \\sqrt{\\Delta x_1^2 + \\Delta x_2^2 + ... + \\Delta x_n^2}\n$$\nfor changes in $x_1, x_2, ..., x_n$ from $\\psi(t_{i-1})$ to $\\psi(t_i)$, representing the Euclidean distance between the start and end points of the interval. When the interval becomes infinitesimal, we have\n$$\n|d\\psi| = \\sqrt{dx_1^2 + dx_2^2 + ... + dx_n^2}\n$$\nor, dividing by $dt$,\n$$\n|\\frac{d\\psi}{dt}| dt = \\sqrt{(\\frac{dx_1}{dt})^2 + (\\frac{dx_2}{dt})^2 + ... + (\\frac{dx_n}{dt})^2}\\ dt\n$$\nwhere the sum of all such expressions from infinitesimal divisions of the curve over some range $t_1$ to $t_2$ yields the length of the curve along that range. Such a sum is given by an integral over that range, as defined by the Riemann sum (*more later*):\n$$\ns(t) = \\int_{t_a}^{t} |\\frac{d\\psi}{dt}|\\ dt = s(t) = \\int_{t_a}^{t} |\\psi'(t)|\\ dt\n$$\nas desired. Note the fact that \n$$\n\\frac{ds}{dt} = |\\psi'(t)|\n$$\nregardless of the parameterization $\\psi$ and $t$ used.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remarks</span>.\n1. Call a curve with finite arc length **rectifiable**.\n2. If $\\psi$ is **not** a simple curve but can be sub-divided into a finite number of simple curves, then the length of $\\psi$ is the sum of the lengths of the simple curves.\n3. The arc length function $s(t)$ as defined above is an increasing function; $\\frac{ds}{dt} \\geq 0$.\n4. An **arc-length** parametrization of the curve $\\mathbf{x} = \\psi(t)$, denoted $\\mathbf{x = r}(s)$, maps the arc length of the curve to a point on the curve which has the corresponding arc length. For instance, while $\\mathbf{x} = \\psi(t)$ gives the position of a point on the curve with respect to time, e.g. \"I am five minutes down the road\", $\\mathbf{x} = r(s)$ gives the position with respect to distance travelled, e.g. \"I am five meters inside your house\". <br/><br/>\nThe arc length parameterization can be obtained through the inverse function of $s(t)$, denoted $\\lambda$:\n$$\nt = \\lambda(s(t))\n$$\n...which always exists as $s(t)$ is increasing with respect to $t$ (and is thus one-to-one). This yields $\\mathbf{x} = \\psi(\\lambda(s)) = \\mathbf{r}(s)$; such a parameterization is beneficial because it is an **intrinsic quality** of the curve independent on the path taken. \n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The arc length parameterization satisfies\n$$\n|\\frac{d\\mathbf{r}}{ds}| = 1\n$$\n> which seems like a baffling result, but in reality just says \"the rate of change of the arc length with respect to the arc length is 1\", as $|\\frac{dr}{ds}|$ is the **magnitude** of the rate of change of position (and thus the distance travelled along the curve, and thus the arc length). \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> via the chain rule.\n\nFirst, we have\n$$\n\\mathbf{r}(s) = \\psi(t) = \\psi(\\lambda(s))\n$$\nfor some parameterization of the curve $\\psi(t)$, with $\\lambda(s) = t$. Thus\n$$\n\\frac{d\\lambda}{ds} = \\frac{dt}{ds} = \\frac{1}{(\\frac{ds}{dt})} = \\frac{1}{|\\psi'(t)|}\n$$\nfrom above; as such\n$$\n\\frac{d\\mathbf{r}}{ds} = \\frac{(\\frac{d\\mathbf{r}}{dt})}{(\\frac{ds}{dt})} = \\frac{\\psi'(t)}{|\\psi'(t)|}\n$$\nas $\\mathbf{r}(s) = \\psi(t)$ and so $\\frac{d\\mathbf{r}}{dt} = \\frac{d\\psi}{dt}$. Thus $\\frac{d\\mathbf{r}}{ds}$ has unit length. \n\n## Tangents to curves\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **tangent** to a curve $\\mathbf{x} = \\psi(t)$ at the point with parameter $t$ is given by the line joining the points $\\psi(t)$ and $\\psi(t+h)$ as $h \\to 0$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. The line joining $\\psi(t)$ and $\\psi(t+h)$ has direction vector\n$$\n\\frac{\\psi(t+h) - \\psi(t)}{h}\n$$\n> the limit of which as $h \\to 0$ equals $\\psi'(t)$ if it exists and is nonzero.\n\nThis implies that \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The tangent vector to $\\psi(t)$ at point $t$ is given by $\\psi'(t)$; the tangent vector of unit magnitude is given by $\\frac{\\psi'(t)}{|\\psi'(t)|}$.\n\nIf the arc length parameterization of the curve is used, its derivative with respect to its parameter always has magnitude one, and thus\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. If the arc length parameterization of the curve is used, the rate of change of the position of the curve with respect to arc length\n$$\n\\mathbf{r}'(s) = \\mathbf{r}'(s(t)) = \\frac{d\\mathbf{r}}{ds}\n$$\n> is the **unit tangent vector** to the curve at all points.\n\n## Curvature, Normal, and Bi-normal\n\n### Curvature\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A parameterization of the curve $\\mathbf{x} = r(t)$ is considered **smooth** if $r'(t)$ is continuous and nonzero at every point on the curve. If the curve $\\mathbf{x}$ has a smooth parameterization, then it is a **smooth curve**.\n\nIf $r'(t)$ is indeed zero at some point, the position of the curve does not change even if we travel along the path of the curve; as such the path is not smooth, and the curve will have sharp corners, cusps or discontinuities. For smooth curves without discontinuities, we define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **curvature** of a smooth curve $\\mathbf{x} = r(t)$ is defined as the magnitude of the rate of change of its **unit tangent vector** with respect to arc length:\n\n$$\n\\kappa = |\\frac{d\\mathbf{T}}{ds}|\n$$\n\n> where $\\mathbf{T}$ is the unit tangent vector to the curve at $t$, given above by the derivative of the arc length parameterization with respect to arc length.\n\nCurvature can be understood as how sharp a curve changes direction at a given point; if it heads in roughly the same direction in a region its unit tangent vector will change very little, corresponding with low curvature, and if it suddenly changes direction at a point the magnitude of the rate of change of the unit tangent vector will be significant, corresponding to a high curvature.\n\nNote that we can rewrite the above with respect to $t$ via the chain rule, for a curve parameterized by $\\phi(t)$:\n\n$$\n\\kappa = |\\frac{(\\frac{d\\mathbf{T}}{dt})}{(\\frac{ds}{dt})}| = |\\frac{\\mathbf{T}'(t)}{\\mathbf{\\phi}'(t)}|\n$$\nas $\\mathbf{\\phi}(t)$ is the position of a point on the curve with respect to $t$, and so $\\mathbf{\\phi}'(t)$ is the rate of change of that position (and thus the magnitude of the rate of change of the arc length).\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Find the curvature of a circle with radius $a$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>. \n\nParameterize the circle as the curve $\\mathbf{x} = \\mathbf{r}(t) = (a \\cos t, a \\sin t)$, yielding \n$$\n\\mathbf{r}'(t) = (-a\\sin t, a \\cos t),\\ |\\mathbf{r}'(t)| = a.\n$$\nThe unit vector to $r(t)$ is given by\n$$\nT(t) = \\frac{r'(t)}{|r'(t)|} = \\frac{(-a\\sin t, a\\cos t)}{a} = (-\\sin t, \\cos t)\n$$\nwith\n$$\nT'(t) = (-\\cos t, -\\sin t)\n$$\nand thus\n$$\n\\kappa = |\\frac{(-\\cos t, -\\sin t)}{a}| = \\frac{1}{a}\n$$\nat all points $t$.\n\n**** \n\n### Normal and bi-normal\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **normal vector** $N(t)$ to the curve $\\mathbf{x} = r(t)$ at point $t$ is such that $N(t)$ is perpendicular to the unit tangent $T(t)$ at $t$.\n\nMany normal vectors exist at any given point, but in particular\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. For a curve $r(t)$ and its **unit** tangent vector $T(t)$, a normal vector at $t$ is given by $T'(t)$ (and its corresponding unit normal vector $\\hat{N}$ by $\\frac{T'(t)}{|T'(t)|})$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nBy virtue of $T(t)$ being a unit vector, we have $T(t) \\cdot T(t) = 1$. Suppose that the components of $T$ are denoted $T_i$; using summation convention results in\n$$\n(T(t) \\cdot T(t))' = (T_i^2)' =2T_iT_i' = (1)' = 0 \n$$\nand thus $T_i T_i' = T(t) \\cdot T'(t) = 0$, implying orthogonality. \n\nCorrespondingly define \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **binormal vector** to $r(t)$ at $t$, denoted $B(t)$, is also a normal vector to $r(t)$, but is normal to both the normal and tangent vectors:\n$$\nB(t) = T(t) \\times N(t).\n$$\n\nThus the binormal, normal, and tangent vectors form a\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Moving trihedral**: a right-handed coordinate system of three unit vectors, all mutually orthogonal, formed around a point $t$ on curve $r(t)$.\n\nLater on these vectors will be used to define an **osculating plane**; \"osculating\" is derived from the Latin root \"osculatus\", meaning \"to kiss\". This is drawn from the fact that the bi-normal vector is \"kissing\" the normal vector, and reminds me of how I am still bi-myself.\n\nAll of this allows us to define a much more efficient formula for curvature:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The curvature of a curve parameterized by $\\mathbf{x = r}(t)$ at a point $t$ is given by\n$$\n\\kappa = \\frac{|\\mathbf{r}'(t) \\times \\mathbf{r}''(t)|}{|\\mathbf{r}'(t)|^3}\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nFrom \n$$\n\\kappa = \\frac{|\\mathbf{T}'(t)|}{|\\mathbf{r}'(t)|}\n$$\nand (being careful to note that $|r'(t)|$ is not a constant!)\n$$\n\\begin{aligned}\n\\mathbf{T}'(t) = (\\frac{r'(t)}{|r'(t)|})' &= (\\frac{r'(t)}{s'(t)})' \\\\\n&= \\frac{r''(t)s'(t) - r'(t)s''(t)}{s'(t)^2} \\\\\n&= \\frac{r''(t) - T(t)s''(t)}{s'(t)} \\\\\n&= \\frac{r''(t) - \\frac{r'(t)}{|r'(t)|}s''(t)}{|r'(t)|} \\\\\n&= \\frac{r''(t)r'(t) - r'(t) s''(t)}{|r'(t)|^2}\n\\end{aligned}\n$$\nusing $T(t) = \\frac{r'}{|r'|}$. This also gives (from the second line)\n$$\nr''(t) = T'(t)s'(t) + T(t)s''(t)\n$$\nand (from $T(t) = \\frac{r'}{s'}$)\n$$\nr'(t) = T(t)s'(t)\n$$\nyielding\n$$\n|r'(t) \\times r''(t)| = |s'(t)|^2 |(T\\times T')| = |r'(t)|^2|(T\\times T')|\n$$\nas $T \\times T = 0$ by property of the cross product. As shown above, $T$ and $T'$ are orthogonal, so their cross product has magnitude\n$$\n\\begin{aligned}\n|T \\times T'| &= |T||T'|\\sin \\theta \\\\\n&= |T'| \\\\\n&= \\frac{|r'(t) \\times r''(t)|}{|r'(t)|^2}\n\\end{aligned}\n$$\nas $T$ is a unit vector, and the last line originates from above. Recall that curvature is defined\n$$\n\\frac{|T'(t)|}{|r'(t)|}\n$$\nwhich finally equals\n$$\n\\frac{|r'(t) \\times r''(t)|}{|r'(t)|^3}\n$$\nas desired.\n\n## Osculating planes\n\nNot to be confused with *oscillating plane*, which was New York's local weather forecast on the morning of 9/11/2001, or *ovulating crane*, which is what I affectionately refer to my mother-in-law as, the **osculating plane** of a curve is defined as\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **osculating plane** of a curve $r(t)$ at point $t$ is the plane (uniquely) determined by the tangent and normal vectors $T(t)$ and $N(t)$ defined above.\n\nAlternatively, the osculating plane results from the following limiting definition:\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. Suppose that $t-h, t, t+h$ represent three points on a curve $r(t)$; a plane is uniquely determined obtained by the two vectors joining the points $(r(t-h),r(t))$ and $r(t), r(t+h)$. The osculating curve to $r(t)$ at $t$ is thus given when $h \\to 0$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> (or at least a desperate justification). \n\nFrom the above definition we know that the osculating curve contains the tangent and normal vectors, which are in the directions $r'(t)$ and $r''(t)$ respectively. \n\nWhen $h \\to 0$ as above, the vector joining $r(t)$ and $r(t+h)$ has direction\n$$\n\\lim_{h\\to 0}\\frac{r(t+h)-r(t)}{h} = r'(t)\n$$\nwhich is the tangent to $r(t)$ at $t$; furthermore the plane also contains the vector in the direction\n$$\n\\lim_{h \\to 0}\\frac{r(t)-r(t-h)}{h}\n$$\nwhich is also $r'(t)$. If the plane contains both vectors, however, it must contain the difference of the two vectors; which is\n$$\n\\lim_{h \\to 0}\\frac{r(t+h)-2r(t)+r(t-h)}{h}\n$$\nwhich, by definition, equals $r''(t)$ and is thus in the direction of $N(t)$.\n\n****\n\nAs the tangent vector to a curve indicates its direction and the normal vector indicates the direction it bends/curves in (as it is in the direction $T'(t)$), the osculating plane is the plane that **most closely touches the curve**:\n\n![alt text](./assets/images/image-9.png)\n\nAs previously stated, the binormal vector is simultaneously perpendicular to the unit normal and unit tangent vectors; and thus it is the normal vector to the osculating plane.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The normal vector to the osculating plane at $t$ for a curve $r(t)$ is given by\n$$\n\\frac{r'(t) \\times r''(t)}{|r'(t)\\times r''(t)|}\n$$\n\nNote that $r'(t)\\times r''(t)$ is never zero because the two are perpendicular, aside from when $r''(t)$ is itself zero.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. The normal vector arises from $N(t) \\times B(t)$, which are in the direction of $r''(t)$ and $r'(t)$ respectively. Normalizing the vector leads to the above result.\n\nFurther define\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **normal plane** to the curve, uniquely defined by the normal and binormal vectors $B(t)$ and $N(t)$ at a point $t$; it contains all vectors normal to the curve at $t$.\n\n## Serret-Frenet equations\n\nThe Serret-Frenet equations concisely restate our above conclusions about curves. They are chiefly concerned with the **arc length parameterization** $r(s)$ of a curve, and provide relationships between tangent, normal, and binormal vectors. \n\nThe arc length parameterization make these expressions more natural given that the tangent vector $r'(s)$ is a unit vector. Denote this unit vector by $\\mathbf{u}$; as proven previously, the curvature\n$$\n\\kappa = \\frac{|r'(t)\\times r''(t)|}{|r'(t)|^3} = |r''(s)| \\geq 0\n$$\nwhen $|r'(s)| = 1$. This yields the first Serret-Frenet equation:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **First Serret-Frenet equation: tangent and normal**. \n$$\n\\frac{d^2\\mathbf{r}}{ds^2} = \\frac{d\\mathbf{u}}{ds} = \\kappa\\mathbf{p}\n$$\n> Where $\\mathbf{u}$ is the unit tangent vector and $\\mathbf{p}$ is the **principal normal vector** at $s$, which exists if $|r''(s)|>0$. It is a unit vector due to the above definition of curvature.\n\nThis is the same result as the one proven above. As before, define the **binormal vector** as the vector perpendicular to both the unit normal and tangent vectors; for the arc length parameterization it conveniently equals\n$$\n\\mathbf{b} = \\frac{\\mathbf{u \\times p}}{|\\mathbf{u\\times p}|} = \\frac{\\mathbf{r'(s) \\times r''(s)}}{|\\mathbf{r}''(s)|} = \\frac{\\mathbf{r'(s)\\times r''(s)}}{\\kappa}\n$$\nas above. Curvature is useful especally in physical scenarios, such as calculating forces, moments, or the total testicular torsion resulting from a spicy kick to the crotch. This leads us to\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Second Serret-Frenet equation: torsion**. For unit binormal vector $\\mathbf{b}$ and principal normal vector $\\mathbf{p}$, both functions of $s$, we have\n\n$$\n\\frac{d\\mathbf{b}}{ds} = -\\tau \\mathbf{p}\n$$\n\n> where $\\tau$ denotes the **torsion**.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> As $\\mathbf{b}$ is a unit vector, we have \n$$\n\\mathbf{b} \\cdot \\mathbf{b}' = 0\n$$\n> which was previously proven with the derivative of the unit tangent vector. However, as $\\mathbf{p} \\cdot \\mathbf{b}$ is also zero due to the binormal vector being perpendicular to the normal vector, $\\mathbf{p}$ and $\\mathbf{b}'$ are in the same direction excepting a sign; this results in\n$$\n\\frac{d\\mathbf{b}}{ds} = -\\tau \\mathbf{p}.\n$$\n\nThe intuition behind torsion originates from the fact that the binormal vector represents the unit normal vector to the osculating plane; thus, the rate at which the osculating plane \"twists\" and changes is given by $\\frac{d\\mathbf{b}}{ds}$ - and measured by the parameter $\\tau$.\n\nFinally, we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The **Third Serret-Frenet equation** describes the vanishing magnitude of my will to live. It represents the derivative of the principal normal vector as a linear combination of curvature, torsion, tangent, and binormal:\n$$\n\\frac{d\\mathbf{p}}{ds} = \\tau\\mathbf{b} - \\kappa \\mathbf{u} \n$$\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. By definition of the unit binormal we have $\\mathbf{b \\times u} = |\\mathbf{b}||\\mathbf{u}|\\mathbf{p} = \\mathbf{p}$ because both $|\\mathbf{b}|$ and $|\\mathbf{u}|$ are $1$, and $\\mathbf{p}$ is orthogonal to both vectors. Thus\n$$\n\\begin{aligned}\n\\frac{d\\mathbf{p}}{ds} &= \\frac{d}{ds}(\\mathbf{b \\times u}) \\\\\n&= \\frac{d\\mathbf{b}}{ds}\\times \\mathbf{u} + \\mathbf{b}\\frac{d\\mathbf{u}}{ds} \\\\\n&= -\\tau(\\mathbf{p} \\times \\mathbf{u}) + \\kappa (\\mathbf{b} \\times \\mathbf{p}) \\\\\n&= \\tau \\mathbf{b} - \\kappa \\mathbf{u}\n\\end{aligned}\n$$\n> making use of the cross-product derivative rule and the properties of the moving trihedral.\n\nAlternative forms of writing the Serret-Frenet equation involve either the cross-product properties of the three vectors (e.g. $\\frac{d\\mathbf{b}}{ds} = -\\tau(\\mathbf{b \\times u})$) or the following matrix representation:\n$$\n\\begin{bmatrix}\n\\mathbf{u}' \\\\\n\\mathbf{b}' \\\\\n\\mathbf{p}'\n\\end{bmatrix} = \n\\begin{bmatrix}\n0 & 0 & \\kappa \\\\\n0 & 0 & -\\tau \\\\\n-\\kappa & \\tau & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{u} \\\\\n\\mathbf{b} \\\\\n\\mathbf{p}\n\\end{bmatrix}\n$$\nwhich is skew-symmetric. The values for curvature and torsion $\\kappa$ and $\\tau$ contain the full scope of information for the geometry of a curve, excepting its position in space and its rotational angle; simultaneously, the moving trihedral of vectors $\\mathbf{b, u}$ and $\\mathbf{p}$ have important physical significance in various system modelled by curves, e.g. velocity, acceleration and speed.\n","n":0.018}}},{"i":44,"$":{"0":{"v":"Linear Algebra","n":0.707},"1":{"v":"### Created by Turbo Huang\n<hr>\nAnd so we begin! I suppose this is a better opportunity than any for me to share a profound foreword to the erudite, learned journey of mathematical enlightenment we are about to embark on with linear algebra - namely, to answer the question: why learn linear algebra at all? There are only three reasons. First, you're a massive sci-fi movie nerd and have accidentally stumbled upon the far-inferior version of the Matrix. Second, you're a massive basketball fan and received the worst surprise of your entire life when you searched \"Jordan form\" on YouTube in hopes of basketball enlightenment. Third, you're a massive German and have taken linear algebra for the sole purpose of pronouncing *eigenvalue* \"the German way\". Either way, I'm glad you're here with me; after all, if you're destined to become a machine learning dev earning seven figures and sunbathing in a luxury yacht, then - in the eternal words of wisdom of r/animememes - \"don't say you love the anime if you haven't read the manga\".<br/><br/>\n(Of course, besides all these completely unhinged things I'm talking about, I suppose there's also a few nuggets of mathematical beauty to be found in these curious morsels we call vectors and matrices here and there.)\n<Newline>","n":0.069}}},{"i":45,"$":{"0":{"v":"Vectors","n":1},"1":{"v":"## Introduction, in brief\nOver the myriad centuries of our blessed lifetimes, we've known vectors as many things: they can be really tall numbers, really squiggly arrows, the only place where a mathematician understands the meaning of the word \"bold\", and, above all else, the orange jumpsuit guy from Despicable Me who embodies the biological fact that rhombus-shaped human bodies are peak evolution. But now comes a time where we study vectors not as actual *things* - arrays of numbers, directions, arrows - but as abstract math objects, just like what numbers are to us. \n\n","n":0.103}}},{"i":46,"$":{"0":{"v":"Vector Product","n":0.707},"1":{"v":"Similar to the scalar product, we can also define a binary operation known as the *vector product* over a vector space that takes two vectors and outputs another vector:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Vector product). For vectors $\\mathbf{a, b} \\in \\mathbb{R^3}$, define the *vector product* between them as\n$$\n\\mathbf{a\\times b} = |\\mathbf{a}||\\mathbf{b}|\\sin \\theta\\ \\mathbf{\\hat{n}}\n$$\n> Where $\\mathbf{\\hat{n}}$ is the unit vector in the direction normal to both $\\mathbf{a}$ and $\\mathbf{b}$, and $\\theta$ is the angle between the two vectors.\n\nEssentially, the vector product takes two vectors and outputs another vector with length equal to the product of their lengths times $\\sin \\theta$, and in a direction normal to both of them. To make such an output unique, we restrict this direction to be the normal direction to $\\mathbf{a, b}$ in a *right-handed sense*:\n\n![alt text](./assets/images/image.png)\n\nThe vector product satisfies the following properties:\n1. $\\mathbf{a\\times b = b\\times a}$.\n2. $\\mathbf{a \\times a = 0}$.\n3. $\\mathbf{a \\times b = 0}\\iff \\mathbf{a} = \\lambda \\mathbf{b}$ for some $\\lambda \\in \\mathbb{R}$, or $\\mathbf{b}= 0$\n4. $\\mathbf{a \\times}\\lambda\\mathbf{b} = \\lambda(\\mathbf{a\\times b})$\n5. $\\mathbf{a}\\times \\mathbf{(b + c)} = \\mathbf{a\\times b + a \\times c}$.\n\nThe following sections will demonstrate three nifty applications of the vector product.\n### Finding normal vectors\nAs mentioned, the vector product outputs a vector normal in direction to both input vectors. We propose that it can be calculated as follows:\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>: $\\mathbf{a \\times b} = \\begin{vmatrix}\\hat{\\mathbf{i}} & \\hat{\\mathbf{j}} & \\hat{\\mathbf{k}} \\\\ a_1 & a_2 & a_3 \\\\ b_1 & b_2 & b_3 \\end{vmatrix}$, in which $a_1, a_2, a_3$ are the components of $\\mathbf{a}$, $b_1, b_2, b_3$ of $\\mathbf{b}$, and $\\mathbf{i, j, k}$ are the unit vectors in the three axes.\n\n### Area of a triangle\nIf vectors $\\mathbf{a}$ and $\\mathbf{b}$ are two sides a triangle (with $\\mathbf{b-a}$ as the third side), then we know that the area of the triangle is half the product of the lengths of two sides times the sine of the angle between them: \n$$\nS = \\frac{1}{2}|\\mathbf{a}||\\mathbf{b}|\\sin \\theta.\n$$\nHowever, the magnitude of the vector product itself is simply\n$$\n|\\mathbf{a\\times b}| = ||\\mathbf{a}||\\mathbf{b}|\\sin \\theta\\ \\mathbf{\\hat{n}}| = |a||b|\\sin \\theta |\\mathbf{\\hat{n}}| = |a||b|\\sin \\theta\n$$\nas $\\hat{n}$ is a unit vector. Thus, the area of the triangle is half the magnitude of the vector product.\n\n### Area of a parallelpiped\n(Derived from the Ancient Greek prefix *parallel*, meaning parallel, and *pipos*, meaning pipes.)\n\n![alt text](./assets/images/image-1.png)\n\nA parallelpiped is just a really jiggly parallelogram. (This should be a proposition.) We first define\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Scalar triple product). Write $[\\mathbf{a,b,c}]$ for three vectors $\\mathbf{a,b,c}$ to denote the product $\\mathbf{a\\cdot(b\\times c)}$. As this is a vector product first and a scalar product second, this outputs a scalar.\n\nWe propose that \n><span style=\"background-color: #ffb812; color: black;\"> Proposition</span>. ~~A parallelpiped is just a really jiggly parallelogram.~~ A parallelpiped with sides $\\mathbf{a,b,c}$, which form a **right-handed system** as shown above, has volume equal to $[\\mathbf{a,b,c}]$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof.</span> The volume of this parallelopiped is the area of the parallelogram encompassed by $\\mathbf{b,c}$ multiplied by its height. \n\n\n> First, the area of that parallelogram is $|\\mathbf{b\\times c}|$ - double the area of a triangle, which has area $\\frac{1}{2}|\\mathbf{b\\times c}|$ as shown above.\n\n> We note that the height of the parallelpiped is $|\\mathbf{a}|\\cos \\theta$, where $\\theta$ is the angle between $\\mathbf{a}$ and the direction normal to the base - thus, the direction normal to both $\\mathbf{b}$ and $\\mathbf{c}$. \n\n> This direction is given by the dot product $\\mathbf{a} \\cdot (\\mathbf{b\\times c})$, as it equals $|\\mathbf{a||b\\times c|}\\cos \\theta$ where $\\theta$ is exactly the angle we are looking for. This completes the proof.\n\nAs long as we choose an order of the vectors $\\mathbf{a,b,c}$ such that they still form a right-handed system, the value of $[\\mathbf{a,b,c}]$ should not be affected because it outputs the volume of the parallelpiped:\n$$\n\\mathbf{[a,b,c]=[b,c,a]=[c,a,b]}\n$$\nHowever, if the three vectors form a left-handed system instead, then the value of the triple product is made negative:\n$$\n\\mathbf{[a,b,c] = -[a,c,b] = -[b,a,c] = -[c,b,a]}\n$$\n### A final note on the vector product in 2D and 3D\nIn 3D (vectors in $\\mathbb{R^3}$), the vector product is all fine and dandy; it has a geometric interpretation as a vector perpendicular to two vectors, and it outputs a vector. This definition completely collapses when we reach a different dimension, like my mental health after doing too many seeded Minecraft speedruns in an hour. As such, in 2D we simply define\n$$\n\\mathbf{a\\times b} = |\\mathbf{a}||\\mathbf{b}|\\sin\\theta.\n$$\n","n":0.037}}},{"i":47,"$":{"0":{"v":"Vector Geometry","n":0.707},"1":{"v":"## Spherical trigonometry\nPicking up where we left off:\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. $\\mathbf{(a\\times b)\\cdot (a\\times c)= (a\\cdot a)(b\\cdot c)-(a\\cdot b)(a\\cdot c)}$.\n\n![alt text](./assets/images/image-2.png)\n\nConsider points $A$, $B$, $C$ on the surface of a unit sphere with radius 1 and origin $O$. Let $\\mathbf{a,b,c}$ denote the position vectors of $A$, $B$ and $C$ relative to the origin $O$ respectively. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Let $\\delta(A,B)$ denote the angle $\\angle AOB$, in radians; $\\delta(A,B)$ is also the arc length $\\widehat{AB}$. We also have $\\mathbf{a\\cdot b} = |\\mathbf{a||b|}\\cos \\delta(A,B)=\\cos \\delta(A,B)$ as $\\mathbf{a,b}$ are unit vectors and have $|\\mathbf{a|=|b|=1}$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span> (Spherical cosine rule). For points $A$, $B$, $C$ on this sphere, we have $\\cos \\alpha \\sin\\delta(A,B) \\sin\\delta(A,C)=\\cos\\delta(B,C)-\\cos\\delta(A,B)\\cos\\delta(A,C)$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Notice that the dot product\n$$\n(a\\times b)\\cdot (a\\times c)\n$$\n> represents the angle $\\alpha$ by means of\n$$\n\\cos \\alpha = \\frac{(a\\times b)\\cdot (a\\times c)}{|a\\times b||a\\times c|}\n$$\n> as $\\alpha$ is the angle between the vectors outputted by the two cross products. Using the proposition above, we have\n$$\n\\cos \\alpha = \\frac{(a\\cdot a)(b\\cdot c)-(a\\cdot b)(a\\cdot c)}{|a\\times b||a\\times c|}\n$$\n> with $a\\cdot a = 1$ due to $a$ being a unit vector. This expression can be rewritten as\n$$\n\\cos \\alpha = \\frac{\\cos\\delta(B,C)-\\cos\\delta(A,B)\\cos\\delta(A,C)}{\\sin\\delta(A,B)\\sin\\delta(A,C)}\n$$\n> resulting in the desired cosine rule.\n\nThis is the fundamental rule governing trigonometry on the surface of a sphere, and it has interesting implications for what triangles would look like in such a circumstance. For instance, say that the triangle is equilateral: $\\cos\\delta(A,B)=\\cos\\delta(B,C)=\\cos\\delta(A,C)=\\delta$. We thus have\n$$\n\\cos \\alpha =\\frac{\\delta - \\delta^2}{1-\\delta^2}=1-\\frac{1}{1+\\delta}\n$$\nwhich is smaller than $\\frac{1}{2}$, with $\\alpha$ thus being greater than $60^{o}$ - except if $\\delta = 1$, where $\\delta(A,B)$ and every other angle would be 0 degrees (degenerating the triangle into a point). This also implies that the sum of angles in a spherical triangle exceeds 180 degrees.\n\n## Lines\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The line through a point with position vector $\\mathbf{a}$ and parallel to vector $\\mathbf{t}$ has equation $\\mathbf{x} = \\mathbf{a} + \\lambda \\mathbf{t}$, where $\\lambda$ is a constant. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. If $P$ denotes the point on the line, then the position vector of $P$ $\\vec{OP}$ equals $\\vec{OA} + \\vec{AP}$ where $\\vec{OA} = \\mathbf{a}$ and $\\vec{AP}$ is some vector in the direction of the line; hence, $\\lambda \\mathbf{t}$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The equation of the same line can also be rewritten $\\mathbf{(x-a)\\times t = 0}$ where $\\times$ denotes the cross product. This can be intuitively seen because the only case when two vectors have a cross product of zero is if one is zero, or if they are parallel; thus either $\\mathbf{x=a}$ or $\\mathbf{x-a}$ is in the direction of $\\mathbf{t}$.\n\n## Planes\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The equation of a plane perpendicular to some unit vector $\\mathbf{n}$ and passing through a point $\\mathbf{a}$ is $\\mathbf{(x-a)\\cdot n} = 0$. This simply expresses the notion that a vector lying in the plane, $(\\mathbf{x-a})$, is perpendicular to the normal vector $\\mathbf{n}$.\n\nThis can also be rewritten $\\mathbf{x\\cdot n = a\\cdot n}$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The distance from the origin to the plane is equal to $\\mathbf{x \\cdot n = a \\cdot n}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Let $Q$ be the point on the plane such that $\\vec{OQ}$ is parallel to  $\\mathbf{n}$. This is the perpendicular line extending from the origin to the plane, and thus it represents the minimum distance from the origin to the plane. Let $\\vec{OQ} = d\\mathbf{n}$ for some $d$. As $Q$ is on the plane, it satisfies\n$$\n(\\vec{OQ}-\\mathbf{a}) \\cdot \\mathbf{n} = (d\\mathbf{n-a}) \\cdot \\mathbf{n} = d|\\mathbf{n}| - a\\cdot \\mathbf{n} = 0\n$$\n> with $|\\mathbf{n}|=1$ by $\\mathbf{n}$ being a normal vector. Thus, $\\mathbf{a\\cdot n} = d$.\n\nNote also that if $\\mathbf{l}$ and $\\mathbf{m}$ are two linearly independent vectors that lie in the plane, then any point on the plane can be expressed as\n$$\n\\mathbf{x}=\\mathbf{a}+\\mu \\mathbf{l}+\\lambda \\mathbf{m}\n$$\nwhere $\\lambda$ and $\\mu$ are real.\n\n## Spheres and circles\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The equation of a generalized hypersphere in $n$ dimensions, with radius $r$ and center $\\mathbf{a}$, (in 2D: a circle, in 3D: a sphere, in 4D and beyond: a hypersphere) is $\\mathbf{|x-a|}=r,\\ \\mathbf{x} \\in \\mathbb{R^n}$. This encompasses the set of all points in $n$-space a distance $r$ away from $\\mathbf{a}$.\n\n## Vector equations\n\nEquations involving vectors, the dot product and the cross product, such as $\\mathbf{x - (x\\times a)\\times b = c}$, are often best solved through vector manipulation:\n1. To make a vector $\\mathbf{a}$ disappear on one side of the equation, take the cross product with $\\mathbf{a}$ on both sides, as $\\mathbf{a\\times a} = 0$.\n2. If two vectors are normal to one another, take their dot product.\n3. Use vector identities such as the vector triple product to expand more complicated expressions.\n\n","n":0.036}}},{"i":48,"$":{"0":{"v":"Tensor Notation","n":0.707},"1":{"v":"> <span style=\"background-color: #12ffd7; color: black;\">Notation</span> (Suffix notation). For vectors $\\mathbf{v}$, the suffix $\\mathbf{v}_i$ is understood to be the $i$th component of $\\mathbf{v}$; this enables us to convert vector equations to scalar form, e.g. $\\mathbf{a=\\lambda b}$ for two vectors can be written $\\mathbf{a_i=\\lambda b_i}$. <br/><br/>\nBuilding on this, we introduce a notation that allows us to remove the summation symbol in equations such as e.g. $\\mathbf{a\\cdot b}=\\sum a_i b_i$. We simply write, for instance, $\\mathbf{a \\cdot b} = a_i b_i$; if the suffix $i$ is repeated twice in a term, that means \"sum over $i$\" - $a_i b_i$ is understood to be $\\sum_{i=1}^n a_i b_i$, $x_k y_k z_c$ is understood to be summing over $k$ and not $c$, etc. If a suffix appears only once in a term (e.g. $a_{ij}b_{jk}$, for $i$ and $k$), they are not summed over.\n\nThe last example - $a_{ij}b_{jk}$ - is an example of *tensors*, an important class of mathematical object prevalent in linear algebra:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Tensors). Tensors are items with more than one index: for instance, $a_{ij}$ or $b_{ijk}$. They may represent entries in a matrix, or other things entirely. For instance:\n\n> <span style=\"background-color: #03cafc; color: black;\">Defintion</span> (Kronecker delta). Define the tensor $\\delta_{ij}$ to be such that \n$$\n\\begin{cases}\n\\delta_{ij}=1,\\ i =j \\\\\n0 \\text{ otherwise}\n\\end{cases}\n$$\nIf the Kronecker delta represents the entries of a matrix, then that matrix is zero for $i\\neq j$ (entries not on the diagonal) and one for $i = j$ (on the diagonal). Thus, the matrix represented by the Kronecker delta is an identity matrix.\n\nThe Kronecker delta can often be useful in writing suffix notation for vector sums; if it is present in a sum, it will eliminate the terms with unequal suffixes and leave only the terms with equal suffixes. For instance:\n1. $a_i \\delta_{ij} = a_j$ (note that this is a sum, not a single term; we are summing over $i$ per the above notation)\n2. $a_p \\delta_{pq} b_q = a_p b_p$ summing over both $p$ and $q$. This expression looks confusing, but remember that $a_p b_p$ denotes a sum over $p$, not a single term.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Alternating symbol). For numbers $(i,j,k)$, define an **even permutation** as a permutation of $(i, j, k)$ that can be reached with an even number of swaps of its elements (zero or two swaps): $(i, j, k)$ itself (zero swaps), $(k,i,j)$, $(j,k,i)$. Define an **odd permutation** analogously as a permutation that can be reached with an odd number of swaps (one swap): $(i,k,j)$, $(j,i,k)$, $(k,j,i)$. Thus define $\\epsilon_{ijk}$, the **alternating symbol**, as\n$$\n\\begin{cases}\n\\epsilon_{ijk}=1, \\text{ $(i,j,k)$ is an even permutation} \\\\\n\\epsilon_{ijk}=-1, \\text{ $(i,j,k)$ is an odd permutation} \\\\\n\\text{0 otherwise (repeated suffixes)}\n\\end{cases}\n$$\nFor instance, $\\epsilon_{231}=1$ and $\\epsilon_{213}=-1$. $\\epsilon_{112}=0$ as it is not a valid permutation.\n\nWe can use these tensors to simplify a few proofs:\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. $(\\mathbf{a\\times b})_i=\\epsilon_{ijk}a_j b_k$ (note the suffix notation here: $j$ and $k$ are both present twice and are thus summed over, while $i$ is present once and is thus fixed). \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n$$\n\\begin{aligned}\n\\mathbf{a \\times b} &= \\begin{vmatrix} i & j & k \\\\ a_1 & a_2 & a_3 \\\\ b_1 & b_2 & b_3 \\end{vmatrix} \\\\\n&= (a_2b_3-a_3b_2)i-(a_1b_3-a_3b_1)j+(a_1b_2-a_2b_1)k \\\\\n&= \\begin{bmatrix} \na_2b_3-a_3b_2 \\\\\na_1b_3-a_3b_1 \\\\\na_1b_2-a_2b_1\n\\end{bmatrix}\n\\end{aligned}\n$$\n> As such, we can write $(\\mathbf{a\\times b})_1 = a_2b_3-a_3b_2=\\epsilon_{123}a_2b_3+\\epsilon_{132}a_3b_2$, with the terms containing $j$ and $k$ not equal to 2 or 3 in any order evaluating to zero by the definition of $\\epsilon$. Examining the other two terms demonstrates the validity of the statement. $\\square$\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. $\\epsilon_{ijk}\\epsilon_{ipq}=\\delta_{jp}\\delta_{kq}-\\delta_{jq}\\delta_{kp}$ where $\\delta$ denotes the Kronecker delta.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. $\\mathbf{a\\cdot (b\\times c)=b\\cdot (c\\times a)}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Using the previous proposition, we have $\\mathbf{(b \\times c)}_i=\\epsilon_{ijk}b_j c_k$ and thus the dot product $\\mathbf{a} \\cdot \\mathbf{(b \\times c)} = \\epsilon_{ijk}a_i b_j c_k$. On the other hand, $\\mathbf{b\\cdot (c\\times a)} = \\epsilon_{ijk}b_i c_j a_k = \\epsilon_{ijk}a_k b_i c_j = \\epsilon_{kij}a_i b_j c_k$ as all of $i$, $j$, $k$ are simply dummy subscripts that can be replaced by other variables. $\\epsilon_{kij}$ represents the permutation $(k,i,j)$, which is two swaps away from $(i,j,k)$ and thus $\\epsilon_{kij}=\\epsilon_{ijk}$. $\\square$\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. $\\mathbf{a\\times(b\\times c) = (a\\cdot c)b - (a\\cdot b)c}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. We have $\\mathbf{b\\times c} = \\begin{bmatrix}\\epsilon_{1jk}b_j c_k \\\\ \\epsilon_{2jk}b_j c_k \\\\ \\epsilon_{3jk}b_j c_k\\end{bmatrix}$. Thus, we have \n$$\n\\begin{aligned}\n\\mathbf{(a\\times (b\\times c))}_i &= \\epsilon_{ijk}a_j(\\mathbf{b\\times c})_k \\\\\n&= \\epsilon_{ijk}a_j\\epsilon_{kpq}b_pc_q\n\\end{aligned}\n$$\n> Using the above theorem, we have\n$$\n\\begin{aligned}\n\\epsilon_{ijk}a_j\\epsilon_{kpq}b_pc_q &= \\epsilon_{kij}\\epsilon_{kpq}a_jb_pc_q \\\\\n&= (\\delta_{ip}\\delta_{jq}-\\delta_{iq}\\delta_{jp})a_j b_p c_q \\\\\n&= a_j b_i c_j - a_j b_j c_i\n\\end{aligned}\n$$\nas the $\\delta$ terms are only nonzero when $p=i, q=j$ and when $q=i, p=j$ respectively. Note that this is the $i$th term and $i$ is fixed; both terms are a sum over $j$. By the definition of the dot product, this is thus $\\mathbf{(a\\cdot c)b_i - (a\\cdot b)c_i}$. $\\square$\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span> (Spherical geometry). $\\mathbf{(a\\times b)\\cdot (a\\times c)= (a\\cdot a)(b\\cdot c)-(a\\cdot b)(a\\cdot c)}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. The left-hand side can be expanded in scalar form as\n$$\n\\begin{aligned}\n\\mathbf{(a\\times b)\\cdot (a\\times c)} &= (\\epsilon_{ijk}a_jb_k)(\\epsilon_{ipq}a_p c_q) \\\\\n&= \\epsilon_{ijk} \\epsilon_{ipq} a_j b_k a_p c_q \\\\\n&= (\\delta_{jp}\\delta_{kq} - \\delta_{jq}\\delta_{kp})a_jb_ka_pc_q \\\\\n&=a_j^2 b_k c_k - a_ja_kb_kc_j\n\\end{aligned}\n$$\n> The right-hand side can be expanded in scalar form as\n$$\n\\begin{aligned}\n\\mathbf{(a\\cdot a)(b\\cdot c)-(a\\cdot b)(a\\cdot c)} &= (a_i a_i)(b_j c_j) - (a_i b_i)(a_j c_j) \\\\\n&=a_j^2 b_k c_k - a_j a_k b_k c_j\n\\end{aligned}\n$$\n> And thus the two expressions are equal. $\\square$\n\n","n":0.034}}},{"i":49,"$":{"0":{"v":"Subspaces","n":1},"1":{"v":"> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Vector subspace). We say that $U$ is a subspace of a vector space $V$ if $U \\subseteq V$ and $U$ is also a vector space under the same operations (addition and scalar multiplication). <br/><br/>\nOf the subspaces of $V$, $V$ and $\\{0\\}$ are trivially subspaces; the other subspaces are **proper subspaces**. <br/><br/>\nIf $U$ is a subspace of $V$, then by the definition of a vector space, $U$ must also be **closed under addition and scalar multiplication**:\nthat is, for $\\mathbf{a,b}\\in U$ and some scalar $\\lambda$, we have\n1. $\\mathbf{a+b}\\in U$\n2. $\\lambda\\mathbf{a} \\in U$\n3. $\\mathbf{0} \\in U$. \n<br/><br/>\nIn other words, $U$ is closed under linear transformation $\\lambda\\mathbf{a}+\\mu\\mathbf{b}$.\n\n","n":0.096}}},{"i":50,"$":{"0":{"v":"Scalar Product","n":0.707},"1":{"v":"In *addition* (ba dum-tss) to addition and multiplication, we define a third operation in a vector space $V$ - the scalar product - that takes two vectors $\\mathbf{a, b}$ as inputs and returns a scalar.\n\nThus far, the one instance of a scalar product we are familiar with is the dot product; but the dot product is not the only scalar product that can be defined. We know that the dot product in $\\mathbb{R^2}$ and $\\mathbb{R^3}$ between vectors $\\mathbf{a}$ and $\\mathbf{b}$ is defined $\\mathbf{a\\dot b = |a||b|}\\cos\\theta$ where $\\theta$ is the angle between the two, and satisfies:\n\n1. $\\mathbf{a\\cdot b = b \\cdot a}$\n2. $\\mathbf{a \\cdot a} = |\\mathbf{a}| \\geq 0$\n3. $\\mathbf{a\\cdot a} = 0$ $\\iff$ $\\mathbf{a} = 0$\n4. If $\\mathbf{a \\cdot b}$ = 0, then $\\mathbf{a, b}$ are perpendicular.\n\nFrom the definition $\\mathbf{a\\dot b = |a||b|}\\cos\\theta$, we know that the dot product gives us information on the angle between two vectors; indeed, from the definition of cosine in a triangle, we know that this expression represents the component of $\\mathbf{a}$ parallel to $\\mathbf{b}$, also known as the *projection*:\n\n![alt text](./assets/images/LA_ch1_dotproduct.png)\n\nHowever, this is only one of many possible scalar products, or *inner products*, $\\langle\\mathbf{x|y}\\rangle$, that can be defined. Similar to a dot product, they are defined by the following properties:\n\n1. Commutativity: $\\mathbf{\\langle x|y\\rangle=\\langle y|x\\rangle}$\n2. Distributive and associative properties: $\\langle\\mathbf{x}|(\\lambda\\mathbf{y}+\\mu\\mathbf{z})\\rangle=\\lambda\\langle\\mathbf{x|y}\\rangle+ \\mu\\langle\\mathbf{x|z}\\rangle$\n3. Identity: if $\\mathbf{\\langle x|x\\rangle=0}$, then $\\mathbf{x}=0$; $\\mathbf{\\langle x|x\\rangle}\\geq 0$.\n\nThese are the same properties as the ones satisfied by the dot product above.\n\nAccordingly, we define the *norm* of a vector $\\mathbf{x}$ as follows:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Norm). The *norm* of a vector $\\mathbf{x}$ is defined as \n$$\n        |\\mathbf{x}|=\\sqrt{\\langle\\mathbf{x}|\\mathbf{x}\\rangle}\n$$\nThe above definitions encompasse far more than the dot product; for instance, if we define the vector space $V$ as the set of all real integrable functions $f(x)$, then we can verify that the operation \n$$\n    \\langle f(x)|g(x) \\rangle = \\int_{0}^{1} f(x)g(x)\\ dx\n$$\nis a valid inner product. This generality will be useful in establishing the power of the following result.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span> (Cauchy-Schwarz inequality). For all vectors $\\mathbf{x,y}\\in\\mathbb{R^n}$, we have \n$$\n        |\\langle \\mathbf{x | y}\\rangle| \\leq |\\mathbf{x}||\\mathbf{y}|.\n$$\n> In other words, the norm of the inner product is greater than or equal to the product of the norms. This applies to any inner product on any real vector space, as the proof below will demonstrate.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof.</span> Consider the norm $|(\\mathbf{x}-\\lambda\\mathbf{y})|$, which is non-negative by definition. Thus its square\n$$\n        (\\mathbf{x}-\\lambda\\mathbf{y})\\cdot (\\mathbf{x}-\\lambda\\mathbf{y})\n$$\n> is also non-negative, leading to \n$$\n        |\\mathbf{x}| - 2\\lambda \\mathbf{x\\cdot y} +\\lambda^2 |\\mathbf{y}| \\geq 0\n$$\n> where $\\cdot$ denotes any inner product. Reorganizing this as a quadratic equation in $\\lambda$ and recognizing that it has at least one real solution, we have its discriminant\n$$\n        \\Delta = b^2-4ac=(4\\mathbf{(x\\cdot y)}^2) - 4|\\mathbf{x||y|}\\geq 0\n$$\n> leading to the desired result.\n\nWe derived this proof entirely based on the axiomatic properties of inner products instead of a specific inner product (e.g. the dot product), so this inequality is applicable across any inner product in any real vector space - like the integral-based one defined above.\n","n":0.045}}},{"i":51,"$":{"0":{"v":"Linear Independence and Basis","n":0.5},"1":{"v":"> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Linear combination). Let $V$ be a vector space and $\\mathbf{v_1,v_2, ..., v_n} \\in V$ be vectors. A **linear combination** of these $n$ vectors is another vector of the form\n$$\na_1v_1+a_2v_2+...+a_nv_n\n$$\n> where $a_1,a_2,...,a_n \\in \\mathbb{R}$ or $\\mathbb{C}$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Span). The **span** of $n$ vectors $v_1, v_2, ..., v_n$ is the set of all vectors $v$ that are linear combinations of $v_1, v_2, ..., v_n$:\n$$\nv = \\{v\\ |\\ v = a_1v_1+a_2v_2 + ... + a_nv_n\\}\n$$\n> The span of an empty list () is defined to be $\\{0\\}$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Spanning set). If the span of $v_1,v_2,...,v_n$ is the vector space $V$, then we say that $v_1,v_2,...,v_n$ **spans** $V$. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Linear independence). We say that a list of vectors $(v_1,v_2,...,v_n)$ are **linearly independent** if the only coefficients $a_1, a_2, a_3, ..., a_n$ which satisfy\n$$\na_1v_1+a_2v_2+...+a_nv_n = 0\n$$\n> are $a_1=a_2=a_3=...=a_n=0$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. If $(v_1, v_2, ..., v_n)$ are linearly independent and span $V$ such that every element $v \\in V$ can be written as $v = a_1v_1+a_2v_2+...+a_nv_n$, a linear combination of the $v_i$s, then the coefficients $a_1, a_2, ..., a_n$ are unique.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Assume for the sake of contradiction that $v \\in V$ can be written two ways: $v = \\sum a_i v_i,$ and $v = \\sum b_i v_i$ where $a_i \\neq b_i$ for $i = 1, 2, ..., n$. Then $v-v=\\sum(a_i-b_i)v_i=0$. However, as $(v_1,v_2,...,v_n)$ are linearly independent, there is no other combination of coefficients other than $(a_1-b_1)=(a_2-b_2)=...=(a_n-b_n)=0$. This yields a contradiction.\n\nLet's see an example for the simplest vector space, $\\mathbb{R^2}$. Suppose that two vectors $\\mathbf{a, b}$ span $\\mathbb{R^2}$. If they are linearly independent, then $\\alpha = 0, \\beta = 0$ are the only coefficients that make $\\alpha \\mathbf{a}+\\beta \\mathbf{b}=0$ possible; in other words, $\\mathbf{a}$ is not a scalar product of $\\mathbf{b}$, and they do not lie on the same line. This means that the angle between them is $\\pi$, and their vector product (as defined in 2D) is 0:\n$$\n\\mathbf{a\\times b = |a||b|}\\sin \\pi = 0\n$$\nTo prove the above theorem, we can also utilize the properties of the vector product:\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Assume for the sake of contradiction that there exists some $\\lambda' \\neq \\lambda$, $\\mu' \\neq \\mu$, all real numbers, such that for vectors $(v_1,v_2)$ and a vector $v \\in \\mathbb{R^2}$, $v = \\lambda v_1 + \\mu v_2 = \\lambda'v_1 +\\mu'v_2$. Performing the vector product of both sides with $v_1$ yields\n$$\n\\lambda v_1 \\times v_1 + \\mu v_2 \\times v_2 = \\lambda' v_1 \\times v_1 + \\mu' v_2 \\times v_2\n$$\n> or\n$$\n(\\mu-\\mu')v_1\\times v_2 = 0\n$$\n> as $v_1 \\times v_1 = v_2 \\times v_2 = 0$ and $v_1 \\times v_2 = v_2 \\times v_1$. If $v_1$ and $v_2$ are linearly independent, then $v_1 \\times v_2 \\neq 0$. Thus $\\mu - \\mu' = 0, \\mu - \\mu'$, and we reach a contradiction.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Basis). A set of vectors $\\{v_1,v_2,...,v_n\\}$ is a basis of a vector space $V$ if they are linearly independent and span $V$. For example, $\\{(0,1),(1,0)\\}$ is a basis of $\\mathbb{R^2}$.\n\nLet's show two examples in $\\mathbb{R^2}$ and $\\mathbb{R^3}$.\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Two vectors $v_1, v_2 \\in \\mathbb{R^2}$ form a basis of $\\mathbb{R^2}$ if they are linearly independent.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Let $r$ be any vector in $\\mathbb{R^2}$ and suppose that $r = \\alpha v_1 + \\beta v_2$. Taking the vector product with $v_1$ on both sides yields $r\\times v_1 = \\beta v_2 \\times v_1$ and $\\beta = \\frac{r\\times v_1}{v_2\\times v_1}$; similarly, $\\alpha = \\frac{r\\times v_2}{v_2 \\times v_1}$. As $v_1,v_2$ are linearly independent, $v_1\\times v_2 \\neq 0$ and thus $\\alpha$ and $\\beta$ always exist for any $r$. Therefore, $v_1$ and $v_2$ form a basis of $\\mathbb{R^2}$ by definition.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Three vectors $v_1,v_2,v_3\\in \\mathbb{R^3}$ form a basis of $\\mathbb{R^3}$ if they are linearly independent. The condition for linear independence in $\\mathbb{R^3}$ is the triple product: $[v_1,v_2,v_3]\\neq 0$.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Observation.</span> The scalar triple product $[v_1,v_2,v_3]$ provides a way of determining whether the three vectors $v_1,v_2,v_3$ are coplanar (share a plane). If the triple product is zero, they share a plane; otherwise, they do not share a plane. <br/><br/> To explain this, let's take a look at what the triple product means; $[v_1,v_2,v_3]=v_1\\cdot(v_2\\times v_3)$. $v_2\\times v_3$ is the direction normal to both $v_2$ and $v_3$. If the dot product between $v_1$ and a vector in this direction is zero, $v_1$ is normal to this normal direction, meaning that $v_1$ is in the same plane as $v_2$ and $v_3$. <br/><br/>\nThus we have:\n$$\n\\begin{cases}\nv_1,v_2,v_3 \\text{ coplanar if $[v_1,v_2,v_3] = 0$} \\\\\nv_1,v_2,v_3 \\text{ not coplanar if $[v_1,v_2,v_3] \\neq 0$}\n\\end{cases}\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Similar to the $\\mathbb{R^2}$ case, suppose that for any vector $r \\in \\mathbb{R^3}$, there exist $\\alpha, \\beta, \\gamma \\in \\mathbb{R}$ such that $r = \\alpha v_1 + \\beta v_2 + \\gamma v_3$. Taking the dot product with $v_2 \\times v_3$ on both sides gives\n$$\nr\\cdot (v_2 \\times v_3) = \\alpha[v_1,v_2,v_3] + 0 + 0\n$$\n> as $[v_2,v_2,v_3]=v_2\\cdot(v_2\\times v_3) = [v_3, v_2, v_2] = v_3(v_2\\times v_2) = 0$, and by a similar logic the last two terms are both zero. Thus we have $\\alpha = \\frac{r\\cdot (v_2 \\times v_3)}{[v_1,v_2,v_3]}$ and a similar process can determine $\\beta$ and $\\gamma$. As the triple product is not equal to zero, $\\alpha,\\beta$ and $\\gamma$ exist for all vectors $r\\in\\mathbb{R^3}$ and thus $v_1,v_2,v_3$ is a basis for $\\mathbb{R^3}$.\n\n> When $r$ is the zero vector, we find that $\\alpha=\\beta=\\gamma=0$ uniquely and thus $v_1,v_2,v_3$ are linearly independent.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Standard basis). The standard basis for the set $\\mathbb{R^n}$ is $\\{\\mathbf{e_1,e_2,...,e_n}\\}$ such that $\\mathbf{e_1}=(1,0,...,0), \\mathbf{e_2}=(0,1,...,0), ..., \\mathbf{e_n}=(0,0,...,0,1)$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Orthonormal basis). Call a basis $\\{\\mathbf{e_1,e_2,...,e_n}\\}$ **orthonormal** if the dot product of any two distinct elements $\\mathbf{e_i\\cdot e_j} = 0$ (i.e. any two vectors in the basis are perpendicular), and the magnitude of any vector in the basis $\\mathbf{e_i \\cdot e_i} =1$. In other words, any two vectors in the basis are normal and every vector in the basis is a unit vector. The standard basis for $\\mathbb{R^n}$ is orthonormal.\n\nWe now generalize the results found in $\\mathbb{R^2}$ and $\\mathbb{R^3}$ to $\\mathbb{R^n}$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. If ${v_1, v_2, ..., v_n}$ is a set of $n$ linearly independent vectors in the vector space $\\mathbb{R^n}$, then the set forms a basis of $\\mathbb{R^n}$. Indeed, this is true not only for $\\mathbb{R^n}$; it is true for any $n$-dimensional vector space.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Dimension). The **dimension** of a vector space $V$, denoted $\\dim V$, is the number of elements in its basis.\n\nThis definition only makes sense if every possible basis of a vector space is of the same size, which we will prove below:\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Suppose that two sets of vectors $u = {u_1,u_2,...,u_m}$ and $v = {v_1,v_2,...,v_n}$ form bases for a vector space $V$. <br/><br/>\nWithout loss of generality, and for the sake of contradiction, let $n > m$. As $\\{v_1, v_2, ..., v_n\\}$ forms a basis, the vector $u_1 \\in V$ can be written as a linear combination of vectors in $v$: $a=a_1v_1 + a_2v_2 + ... + a_nv_n$ for scalars $a_1,a_2,...,a_n$. <br/><br/>\nThus, we have $v_1=u_1-a_2v_2-a_3v_3-...-a_nv_n$ and thus $v_1$ can be written as a linear combination of $v'=\\{u_1, v_2, ..., v_n\\}$. As such, $v'$ also forms a basis. This process can be repeated for $u_2, u_3, ..., u_m \\in u$, which eventually leads to the basis $w=\\{u_1,u_2,u_3,...,u_m,v_{m+1},v_{m+2},...,v_{n}\\}$. <br/><br/>\nThe steps above imply that the vectors in $w$ are linearly independent, but as $\\{u_1,u_2,...,u_m\\}$ is already a basis, all of $v_{m+1},v_{m+2},...,v_n$ can be written as linear combinations of $u_1,...,u_m$. Thus, $w$ is not linearly independent and we reach a contradiction.\n\n","n":0.028}}},{"i":52,"$":{"0":{"v":"Definitions and Properties","n":0.577},"1":{"v":"> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Vector space). A *vector space* $V$ over $\\mathbb{R}$ or $\\mathbb{C}$ is a collection of entities (\"vectors\") $\\mathbf{v} \\in V$, in which we define two operations: addition of two vectors and scalar multiplication with a vector.\n\nThese \"vectors\" (notice that I am stubbornly using quotation marks here like a conspiracy theorist who believes that Big Archimedes fabricated vectors to steal your money, because technically, we aren't supposed to know what vectors are yet; we're defining them here) must satisfy the following properties under addition and scalar multiplication:\n\n### Addition\n1. Commutativity: $\\mathbf{a + b = b + a}$\n2. Associativity (order does not change result): $\\mathbf{(a+b)+c=a+(b+c)}$\n3. Identity: there is a vector $\\mathbf{0}$ such that $\\mathbf{a+0=a}$.\n4. Existence of inverses: for all vectors $\\mathbf{a}$, there exists another vector in $V$ which we denote $-\\mathbf{a}$ such that $\\mathbf{a+(-a)=0}$.\n\n### Scalar Multiplication\nHere $\\lambda, \\mu$ denote scalars and all bolded letters represent vectors.\n1. Scalar distributive property: $\\lambda\\mathbf{(a+b)}=\\lambda\\mathbf{a}+\\lambda\\mathbf{b}$.\n2. Vector distributive property: $\\mathbf{a}(\\lambda+\\mu)=\\mathbf{a}\\lambda+\\mathbf{a}\\mu$\n3. Associativity: $\\lambda(\\mu\\mathbf{a})=\\mu(\\mathbf{a}\\lambda)$\n4. Identity: $1\\mathbf{a}=\\mathbf{a}$.\n\nIt is implicitly stated that the sum of two \"vectors\" in $V$ (suspicious name, that) will be another \"vector\" (also in $V$), and the scalar multiplication of a vector will give another vector; thus we can say that vector spaces are *closed* under addition and scalar multiplication; that is, these two operations map elements of $V$ to $V$.\n\nNotice also that vectors (as we understand them; the matrix kind) of any size will all satisfy these properties, whether they have two or three or ten entries; in fact, vectors with a single entry - which are just scalars - also satisfy these properties. Thus, any $\\mathbb{R}^n$ - the set of $n$-tuples of real numbers - are vector spaces; all lines through the origins are vector spaces (as in, all points on these lines), but lines not through the origin are not - they do not contain (0,0), the zero vector.\n\n\nThe geometric meaning of vectors, of course, are more familiar to us: objects $\\mathbf{v}$ with a length denoted $|\\mathbf{v}|$, and a direction which remains unchanged under scalar multiplication (either parallel or antiparallel).\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Unit vector). A unit vector, usually denoted with a little hat like this - $\\mathbf{\\hat{v}}$ - is a vector with length 1.\n\n\n","n":0.052}}},{"i":53,"$":{"0":{"v":"Complex Vector Spaces","n":0.577},"1":{"v":"> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. (Complex vector space) Define the complex vector space $\\mathbb{C}^n$ as encompassing all vectors $(z_1,z_2,...,z_n), z_i \\in C$. This vector space shares many properties with $\\mathbb{R^n}$: the same standard basis, for instance. However, we will need a different definition of the scalar product because vectors such as $u=(0,i)$ will have negative norms $u\\cdot u = i^2=-1$, violating the definition of a norm. <br/><br/>\nThus, we define the dot product $\\mathbf{u\\cdot v}$ in the complex plane as $\\sum \\bar{u_i} v_i$ where $\\bar{u_i}$ denotes the complex conjugate. This ensures that the norm is always positive ($u\\cdot u=\\bar{z_1}z_1 + \\bar{z_2}z_2 + ... + \\bar{z_n}z_n=|z_1|+|z_2|+...+|z_n|$, which is a sum of magnitudes and thus always positive).<br/><br/>\nThis dot product does not satisfy $u\\cdot v = v\\cdot u$, but instead satisfies $u\\cdot v = \\bar{(v \\cdot u)}$ (due to the properties of complex conjugation).\n\n","n":0.084}}},{"i":54,"$":{"0":{"v":"Transformation Groups","n":0.707},"1":{"v":"Apparently there's a fun little pattern with how these Cum Breech Tripod courses are designed: they give you the world's most microscopic glimpse of the next course you'll be studying, but like Vince Gilligan, they end it right before the good part to pad viewer engagement. Cowards (Unvravo Bince).\n\n## Defining groups\n> ~~<span style=\"background-color: #03cafc; color: black;\">Definition</span>. I'm done with this shit, yo. Just go take **Group Theory** or something.~~\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **group** is a set $G$ with a binary operation $*$ that satisfies the following four axioms:\n1. **Closure**. If $a, b \\in G$, $a * b \\in G$ for all $a$, $b$.\n2. **Associativity**. For all $a, b, c \\in G$, $(a * b) * c = a * (b * c)$.\n3. **Identity**. There exists an **identity element** $e\\in G$ such that $a * e = e * a = a$ for all  $a \\in G$.\n4. **Inversibility**. For all $a \\in G$, there exists an **inverse element** $a^{-1}$ such that $a * a^{-1} = a^{-1} * a = e$, the identity element.\n\n## Matrix groups\n\n### Orthogonal matrices\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The set $G$ of all $n\\times n$ orthogonal matrices $Q$ such that $Q^T Q = I$ forms a group.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nDefine the binary operation $*$ over this group as matrix multiplication.\n\n1. **Closure**. If $P$, $Q$ are both orthogonal ($P, Q \\in G$), then $(PQ)^{T}(PQ) = Q^{T}P^{T}PQ = Q^{T}IQ = Q^{T}Q = I$. Thus $PQ \\in G$.\n\n2. **Associativity**. True by associativity of matrix multiplication.\n\n3. **Identity**. True by existence of identity matrix $I$.\n\n4. **Inversibility**. By definition, if $Q \\in G$, then $Q^T = Q^{-1}$; if $Q$ exists, then $Q^T$ must exist. $Q^{-1}$ is also in $G$, as $(Q^{-1})^{-1} (Q^{-1}) = (Q^T)^{-1} (Q^{T}) = I$ by $Q^T = Q^{-1}$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Denote the group containing all $n\\times n$ orthogonal matrices as $O(n)$.\n\n### Orthogonal matrices with $\\det$ 1\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The set $SO(n)$ of all $n\\times n$ orthogonal matrics which have determinant 1 forms a group.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n1. **Closure**. Let $P, Q$ be orthogonal matrices with $\\det 1$. Then $PQ$ is orthogonal, as shown above, and $\\det (PQ) = \\det P \\det Q = 1 \\cdot 1 = 1$.\n\n2. **Associativity**. True by associativity of matrix multiplication.\n\n3. **Identity**. True by existence of identity matrix $I$.\n\n4. **Inversibility**. $P^{-1}$ is orthogonal as shown above; we also have $\\det P^{-1} = \\frac{1}{\\det P} = \\frac{1}{1} = 1$.\n\n### Length-Preserving Matrices\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Let $P$ be a real $n\\times n$ matrix. Then the following statements are equivalent:\n1. $P$ is orthogonal.\n2. The columns of $P$ are orthonormal.\n3. $P$ preserves the scalar product: $\\mathbf{x\\cdot y} = (P\\mathbf{x})\\cdot (P\\mathbf{y})$ for vectors $\\mathbf{x,y}\\in \\mathbb{R}^n$.\n4. $P$ is a *linear isometry*, i.e. it preserves the length of vectors: $|\\mathbf{x}| = |P(\\mathbf{x})|$.\n5. If $\\{v_1, ..., v_n\\}$ form an orthonormal list, then $\\{Pv_1, ..., Pv_n\\}$ form an orthonormal list (and thus $P$ is a transformation matrix between two orthonormal bases).\n\nNote that any two of the statements above form an \"if-and-only-if\"; one implies the other, and vice versa.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n**1 -> 2**. Let $P$ be an orthogonal matrix; we want to prove that its columns are orthonormal, i.e. if $P$ has columns \n$$\nP = \\begin{bmatrix}\np_1 & p_2 & ... & p_n\n\\end{bmatrix}\n$$\nthen $p_i \\cdot p_j = 0$ and $|p_i| = p_i \\cdot p_i = 1$ for all $i \\neq j = 1, 2, ..., n$. We also have $PP^T = I$, or, in suffix notation,\n$$\nP_{ik}P_{jk} = (p_i)_k (p_j)_k = p_i \\cdot p_j = \\delta_{ij}.\n$$\n\n\n**2 -> 3**. We know that $P$ has orthonormal columns; we want to prove that it preserves the scalar product with $(P\\mathbf{x})\\cdot (P \\mathbf{y}) = \\mathbf{x\\cdot y}$ for all vectors $\\mathbf{x,y}\\in\\mathbb{R^n}$. From above, we know that if the columns of $P$ are orthonormal, then $P$ is orthogonal. We thus have\n$$\n(P\\mathbf{x})\\cdot (P\\mathbf{y}) =(P\\mathbf{x})^T (P\\mathbf{y}) = \\mathbf{x}^T P^T P \\mathbf{y} \n$$\nwhere $P^T P = I$ by definition of orthogonality, which means\n$$\n(P\\mathbf{x})\\cdot (P\\mathbf{y}) = \\mathbf{x}^T \\mathbf{y} = \\mathbf{x\\cdot y}\n$$\nby definition of the real scalar product.\n\n**3 -> 4**. For any vector $\\mathbf{x} \\in \\mathbb{R^n}$, we have\n$$\n|P(\\mathbf{x})| = (P\\mathbf{x})\\cdot (P\\mathbf{x}) = \\mathbf{x\\cdot x} = |\\mathbf{x}|\n$$\nby preservation of the scalar product.\n\n\n**4 -> 5**. If $P$ preserves lengths, then $Pv_1$ has the same length as $v_1$, $Pv_2$ as $v_2$, etc. Thus if $v_1, ..., v_n$ are each of unit length, then $Pv_1, ..., Pv_n$ are also each of unit length. Now consider\n$$\n(Pv_i) \\cdot (Pv_j)\n$$\nwhich, by scalar product preservation, is equal to $v_i \\cdot v_j = 0$ by orthonormality. Thus $Pv_i$ form an orthonormal list.\n\n**5 -> 1**. Proven in \"Hermitian Matrices and their Eigenvalues\" (transformation matrices between orthonormal bases are orthogonal).\n\n****\n\nThese equivalent statements directly lead to\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The set of all (real) $n\\times n$ length-preserving transformation matrices form a group.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n By the previous theorem, real length-preserving transformation matrices are orthogonal matrices and vice versa. As orthogonal matrices form a group, so must length-preserving transformation matrices.\n\n ****\n\nIn fact, particularly in the 2-dimensional group of orthogonal matrices $O(n)$ - and their subgroup $SO(n)$, which are such matrices with determinant 1 -  length-preserving/orthogonal matrices serve very specific purposes:\n\n > <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. A two-dimensional length-preserving (orthogonal) matrix $P \\in O(2)$ is equivalent to either one of the following: the rotation matrix\n$$\nP = \\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{bmatrix}\n$$\n> or the reflection matrix\n$$\nP = \\begin{bmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{bmatrix}\n$$\n> which is a reflection in the line $y \\cos  (\\theta/2) = x\\sin (\\theta/2)$. \n\nA little tangent on the reflection matrix. To derive it, consider the following. If a reflection about the line\n$$\ny =x \\tan \\theta\n$$\n(which encompasses all real lines through the origin, as $\\tan \\theta$ has a range over all the reals) sends a general vector \n$$\n\\mathbf{x} = \\begin{bmatrix}\nx \\\\ y\n\\end{bmatrix}\n$$\nto \n$$\n\\mathbf{x'} = \\begin{bmatrix}\nx' \\\\ y'\n\\end{bmatrix}\n$$\nwhat are $x'$ and $y'$ in terms of $x$ and $y$? A reflection of $\\mathbf{x}$ in a line with direction vector $\\mathbf{d}$ can be understood as $\\mathbf{x}$ minus two times its perpendicular vector onto the line. This perpendicular vector is given by $\\mathbf{x}$ minus its projection onto the direction $\\mathbf{d}$:\n$$\n\\mathbf{x' = x} - 2(\\mathbf{x} - P_{\\mathbf{d}}(\\mathbf{x})) = 2P_{\\mathbf{d}}(\\mathbf{x}) - \\mathbf{x}\n$$\nAs previously found, this projection is equivalent to\n$$\nP_{\\mathbf{d}}(\\mathbf{x}) = \\frac{\\mathbf{d\\cdot x}}{\\mathbf{d\\cdot d}}\\mathbf{d}\n$$\nAnd in the case $y = x \\tan \\theta$, which has direction vector \n$$\n\\mathbf{d} = \\begin{bmatrix}\n1 \\\\\n\\tan \\theta\n\\end{bmatrix}\n$$\nwe thus have \n$$\n\\begin{aligned}\nP_{\\mathbf{d}}(\\mathbf{x}) &= \\frac{\\mathbf{d\\cdot x}}{\\mathbf{d\\cdot d}}\\mathbf{d} \\\\\n&= \\frac{x+(\\tan \\theta) y}{1^2 + \\tan^2 \\theta}\\begin{bmatrix}\n1 \\\\\n\\tan \\theta\n\\end{bmatrix} \\\\\n&= \\frac{x+(\\tan \\theta) y}{\\sec^2 \\theta}\\begin{bmatrix}\n1 \\\\\n\\tan \\theta\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n(\\cos^2 \\theta)x + (\\sin \\theta \\cos \\theta)y \\\\\n(\\sin \\theta \\cos \\theta)x + (\\sin^2 \\theta)y\n\\end{bmatrix}\n\n\\end{aligned}\n$$\nPlugging this into\n$$\n\\mathbf{x'} = 2P_{\\mathbf{d}}(\\mathbf{x}) - \\mathbf{x}\n$$\ngives\n$$\n\\mathbf{x}' = \\begin{bmatrix}\n(2\\cos^2 \\theta - 1)x + 2(\\sin\\theta \\cos\\theta) y \\\\\n2(\\sin\\theta \\cos\\theta) x + (2\\sin^2 \\theta - 1)y\n\\end{bmatrix} = \\begin{bmatrix}\n(\\cos 2\\theta) x + (\\sin 2\\theta) y\\\\\n(\\sin 2\\theta) x + (-\\cos 2\\theta)y\n\\end{bmatrix} \n$$\nwhich can be written in matrix form as\n$$\n\\mathbf{x'} = \\begin{bmatrix}\n\\cos 2\\theta & \\sin 2\\theta \\\\\n\\sin 2\\theta & -\\cos 2\\theta\n\\end{bmatrix} \\mathbf{x}\n$$\ngiving the reflection matrix\n$$\nP = \\begin{bmatrix}\n\\cos 2\\theta & \\sin 2\\theta \\\\\n\\sin 2\\theta & -\\cos 2\\theta\n\\end{bmatrix}.\n$$\n\n****\n\nWith this in mind, let's prove that all $2\\times 2$ orthogonal matrices are either reflections or rotations.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> by brute force.\n\nLet\n$$\nP = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n$$\nwith $a, b, c, d$ real, i.e. $P$ is any general real matrix. Suppose that $P \\in O(2)$, and thus that $P$ satisfies $PP^T = I$:\n\n$$\n\\begin{aligned}\nPP^T &= \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\na^2 +b^2 & ac + bd \\\\\nac + bd & c^2 + d^2\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}.\n\\end{aligned}\n$$\nThus, $a, b, c, d$ must satisfy\n$$\n\\begin{cases}\na^2 + b^2 = 1 \\\\\nac+bd = 0 \\iff ac = -bd\\\\\nc^2 + d^2 = 1\n\\end{cases}\n$$\nHowever, $P$ must also satisfy $PP^T = I$, yielding the equations\n$$\n\\begin{cases}\na^2 + c^2 = 1 \\\\\nab+cd = 0 \\iff ab = -cd\\\\\nb^2 + d^2 = 1\n\\end{cases}\n$$\nIf $a^2 + c^2 = 1$, we can parametrize $a = \\cos \\theta$ and $c = \\sin \\theta$ for some real $\\theta$ (this is representative of every possible pair $(a,c)$.) Thus, as $a^2 + b^2 = 1$ and $c^2 + d^2 = 1$, we also have $b = \\pm \\sin \\theta$ and $d = \\pm \\cos \\theta$. However, as $ac = -bd$, $b$ and $d$ have opposite signs; thus either $b = \\sin \\theta, d = -\\cos\\theta$, resulting in\n$$\nP = \\begin{bmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n\\sin \\theta & -\\cos \\theta\n\\end{bmatrix}\n$$\nwhich is a reflection, or $b = -\\sin\\theta, d = \\cos\\theta$, resulting in\n$$\nP = \\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{bmatrix}\n$$\nwhich is a rotation.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. The determinant of a rotation matrix (as above) is $1$; the determinant of a reflection matrxi is $-1$. Thus we conclude that if $P \\in SO(2)$, the group of orthogonal matrices with determinant 1, then it is a rotation matrix.\n\n","n":0.026}}},{"i":55,"$":{"0":{"v":"Metrics","n":1},"1":{"v":"### Metrics (or how to confuse me completely about scalar products)\n\nLet $\\{u_1, ..., u_n\\}$ be a basis of $\\mathbb{R}^n$, **not necessarily orthogonal or orthonormal**. If we were to write the components of vectors $\\mathbf{x,y}\\in\\mathbb{R^n}$ in terms of $u_1, u_2, ..., u_n$, we would have\n$$\n\\begin{cases}\n\\mathbf{x} = \\sum_{i=1}^n x_i \\mathbf{u}_i \\\\\n\\mathbf{y} = \\sum_{i=1}^n y_i \\mathbf{u}_i\n\\end{cases}\n$$\nfor components $x_i$ of $\\mathbf{x}$ and $y_i$ of $\\mathbf{y}$. Thus, we write the dot product $\\mathbf{x\\cdot y}$ as\n$$\n\\begin{aligned}\n&\\mathbf{x}\\cdot \\mathbf{y} \\\\\n&= (\\sum_{i=1}^n x_i \\mathbf{u}_i)\\cdot (\\sum_{j=1}^n y_i \\mathbf{u}_i) \\\\\n&= \\sum_{i=1}^n\\sum_{j=1}^n x_i y_j (\\mathbf{u_i} \\cdot \\mathbf{u_j}) \\\\\n&= \\sum_{i=1}^n\\sum_{j=1}^n x_i G_{ij} y_j\n\\end{aligned}\n$$\nwhere we define the matrix $G$ as $G_{ij} = \\mathbf{u_i \\cdot u_j}$; note that $G$ is symmetric by commutativity of the real dot produdct. In matrix form, we can also write\n$$\n\\mathbf{x}\\cdot\\mathbf{y} = \\mathbf{x}^T G \\mathbf{y}\n$$\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. For the basis $\\{u_1,...,u_n\\}$ defined above, call $G$ its **metric**. A **metric** is - at least in the context of this course - an application of changes of basis upon scalar products; call the scalar product $\\mathbf{x}^T G \\mathbf{y}$ a scalar product **with respect to the metrix $G$**.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. If $\\mathbf{u_1, u_2, ..., u_n}$ are orthonormal, then $\\mathbf{u_i\\cdot u_j} = \\delta_{ij}$, implying that $G = I$. Thus, orthonormal bases have their scalar products with respect to the metric $I$ (as expected); from previous results, we know that the group of matrices that preserve the scalar product with respect to $I$ form a group (orthogonal matrices).\n\n## Lorentz transformations \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **Minkowski metric** in $\\mathbb{R}^2$ to be\n$$\nJ = \\begin{bmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{bmatrix}\n$$\n> and the corresponding **Minkowski inner product** with respect to this metric as\n$$\n\\langle\\mathbf{x, y}\\rangle  = \\mathbf{x}^T J \\mathbf{y} = x_1y_1 - x_2y_2.\n$$\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The set of $2\\times 2$ transformations $M$ that preserve the Minkowski inner product, i.e. that ensure\n$$\n\\langle\\mathbf{x, y}\\rangle = \\langle M\\mathbf{x}, M\\mathbf{y} \\rangle\n$$\n> for vectors $\\mathbf{x}$ and $\\mathbf{y}$ in $\\mathbb{R^2}$, are of the form\n$$\nJ = M^T J M\n$$\n> where $J$ is as above.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nBy definition of the Minkowski inner product, we have\n$$\n\\langle M\\mathbf{x}, M\\mathbf{y} \\rangle = (M\\mathbf{x})^T J (M\\mathbf{y}) = \\mathbf{x}^T (M^T J M) \\mathbf{y}\n$$\nwhich equals $\\mathbf{x}^T J \\mathbf{y}$, leading to $M^T J M = J$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. All such matrices $M$ come in one of the following two forms:\n$$\nM = \\begin{bmatrix}\n\\cosh u & \\sinh u \\\\\n\\sinh u & \\cosh u\n\\end{bmatrix},\\ M = \\begin{bmatrix}\n\\cosh u & -\\sinh u \\\\\n\\sinh u & -\\cosh u\n\\end{bmatrix}\n$$\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLet $M$ be the general $2\\times 2$ matrix\n$$\nM = \\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}.\n$$\nIf $M$ satisfies $J = M^T J M$, then\n$$\n\\begin{aligned}\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{bmatrix} &= \n\\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{bmatrix}\n\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\\begin{bmatrix}\na & b \\\\\n-c & -d\n\\end{bmatrix} \\\\\n&=\n\\begin{bmatrix}\na^2 - c^2 & ab - cd \\\\\nab - cd & b^2 - d^2\n\\end{bmatrix}\n\\end{aligned}\n$$\nAllowing for\n$$\n\\begin{cases}\na^2-c^2 = 1 \\\\\nab = cd \\\\\nb^2 - d^2 = 1\n\\end{cases}\n$$\nWhere we can parametrize $a^2-c^2 = 1$ as $a = \\cosh u, c=\\sinh u$ (which encompasses all real solutions to $a^2-c^2=1$.) As $ab$ and $cd$ have the same sign, we must have either $b = \\sinh u, d = \\cosh u$ (both positive) or $b = -\\sinh u, d = -\\cosh u$, matching the two cases shown above.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. The above two forms for $M$ look remarkably similar to rotation matrices and reflection matrices, except for the fact that they use hyperbolic cosines and sines instead of regular cosines and sines; they represent rotations and reflections respectively for numbers parametrized by hyperbolic angles.\n\nDenote the **hyperbolic rotation** counterclockwise by hyperbolic angle $u$ as\n$$\nH_u = \\begin{bmatrix}\n\\cosh u & \\sinh u \\\\\n\\sinh u & \\cosh u\n\\end{bmatrix}\n$$\nand the **hyperbolic reflection** over the line $\\sinh(u/2)x = \\cosh(u/2)y$ as\n$$\nH_{u/2.} = \\begin{bmatrix}\n\\cosh u & -\\sinh u \\\\\n\\sinh u & -\\cosh u\n\\end{bmatrix}\n$$\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **Lorentz boost** is a transformation represented by a matrix of the form\n$$\nB_v = \\frac{1}{\\sqrt{1-v^2}}\\begin{bmatrix}\n1 & v \\\\\nv & 1\n\\end{bmatrix}.\n$$ \n> Note that a Lorentz boost by **velocity** $\\tanh u$, denoted $B_{\\tanh u}$, is identicala to a hyperbolic rotation by $u$:\n$$\n\\begin{aligned}\nB_{\\tanh u} &= \\frac{1}{\\sqrt{1-\\tanh^2 u}}\\begin{bmatrix}\n1 & \\tanh u  \\\\\n\\tanh u & 1\n\\end{bmatrix} \\\\\n&= \\frac{1}{\\text{sech } u}\n\\begin{bmatrix}\n1 & \\tanh u  \\\\\n\\tanh u & 1\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n\\cosh u & \\sinh u  \\\\\n\\sinh u & \\cosh u\n\\end{bmatrix} \\\\\n\\end{aligned}\n$$\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The set of Lorentz boosts form a group.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLorentz boosts are identical to hyperbolic rotations of the form $\\begin{bmatrix}\n\\cosh u & \\sinh u  \\\\\n\\sinh u & \\cosh u\n\\end{bmatrix}$.\n\n**Closure**. \n$$\n\\begin{aligned}\n&\\begin{bmatrix}\n\\cosh u & \\sinh u  \\\\\n\\sinh u & \\cosh u\n\\end{bmatrix}\n\\begin{bmatrix}\n\\cosh w & \\sinh w \\\\\n\\sinh w & \\cosh w\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n\\cosh u \\cosh w + \\sinh u \\sinh w & \\cosh u \\sinh w + \\sinh u \\cosh w  \\\\\n\\cosh u \\sinh w + \\sinh u \\cosh w   & \\cosh u \\cosh w + \\sinh u \\sinh w\n\\end{bmatrix}  \\\\\n&= \n\\begin{bmatrix}\n\\cosh (u+w) & \\sinh (u+w) \\\\\n\\sinh (u+w) & \\cosh (u+w)\n\\end{bmatrix}\n\\end{aligned}\n$$\nwhich is also a hyperbolic rotation by angle $u+w$.\n\n**Associativity** by corresponding associativity matrix multiplication.\n\n**Identity** by $B_0 = \\frac{1}{\\sqrt{1-0^2}}\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} = I$. And finally,\n\n**Inverse** by $H_{u}H_{-u} = I$: two rotation matrices of opposite angles cancel out.\n\n$\\square$\n\n> Fin!","n":0.034}}},{"i":56,"$":{"0":{"v":"Systems of Linear Equations","n":0.5},"1":{"v":"studying this after doing the wackiest suffix notation mumbo jumbo in the world is like becoming champion of the kiddie pool 25-meter freestyle swim meet after competing in the olympics\n\n","n":0.183}}},{"i":57,"$":{"0":{"v":"Matrix Rank","n":0.707},"1":{"v":"## Matrix Rank 2: Electric Boogaloo\nIn the context of a linear map, we defined the rank of a matrix $A$ as the dimension of its image: $r(A)=\\dim A(\\mathbb{R}^n)$. We previously derived two conclusions: \n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. \n1. In the context of $A$ representing a linear map from $\\mathbb{R}^n$ to $A(\\mathbb{R}^n)$ under the standard basis $\\{e_j\\} = \\{(1,0,...,0),...,(0,0,...,1)\\}$, the vectors that $e_j$ are mapped to under $A$ - $A(e_j)$ - are represented by the columns of $A$. \n2. $A(e_1), A(e_2), ..., A(e_n)$ spans the image of $A$.\n\nBuilding upon the second statement and the notion of rank, we further note that\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. Within the spanning set $\\{A(e_1),A(e_2),...,A(e_n)\\}$ spanning the image of $A$, the number of vectors which are linearly independent to each other must equal $r(A)$. This derives organically from the definition of dimension and rank.\n\nAs Lemma 1 notes that $A(e_i)$ are the columns of $A$, this is equivalent to stating that $r(A)$ is the number of linearly independent columns of $A$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **row rank** of a matrix as its number of linearly independent rows; define the **column rank** as its number of linearly independent columns. \n\nBut sike! You've been successfully duped, tricked, deceived, and possibly even bamboozled, because\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The row rank of any matrix is equal to its column rank. Denote this rank by $\\text{rank} A$, or if $A$ also represents a linear map, $r(A)$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Let $A$ be a $m\\times n$ matrix, where $m$ need not equal $n$. Suppose that $A$ has row rank $r$; that is, it has $r$ linearly independent rows. Let $A^T_{(i)}$ denote the $i$th row of $A$ and $A_{(j)}$ denote the $j$th column of $A$. <br/><br/>\nIf $A$ has row rank $r$, then there is a set of row vectors $v = \\{v^T_{(1)}, v^T_{(2)}, ..., v^T_{(r)}\\}$ corresponding to $r$ different rows of $A$ that are linearly independent. All rows of $A$ can be written as a linear combination of the elements of $v$ by definition of linear independence; these rows are either in $v$, or are linearly dependent with the elements of $v$. Thus, we write\n$$\nA^T_{(i)} = \\sum_{k=1}^r a_{ik} v^T_{(k)}\n$$\n> for certain scalars $a_i$. In scalar form, this becomes\n$$\nA_{ij} = \\sum_{k=1}^r a_{ik} v^T_{(k)j}\n$$\n> and if $j$ is fixed, this becomes\n$$\n\\begin{aligned}\n\\begin{bmatrix}\nA_{1j} \\\\ A_{2j} \\\\ \\vdots \\\\ A_{mj}\n\\end{bmatrix} &= \\begin{bmatrix}\n\\sum_{k=1}^r a_{1k} v^T_{(k)j} \\\\\n\\sum_{k=1}^r a_{2k} v^T_{(k)j} \\\\\n\\vdots \\\\\n\\sum_{k=1}^r a_{mk} v^T_{(k)j}\n\\end{bmatrix}  \\\\\n&= v^T_{(1)j}\\begin{bmatrix}a_{11} \\\\ a_{21} \\\\ \\vdots \\\\ a_{m1} \\end{bmatrix} + \nv^T_{(2)j}\\begin{bmatrix}a_{12} \\\\ a_{22} \\\\ \\vdots \\\\ a_{m2} \\end{bmatrix} + ... + \nv^T_{(r)j}\\begin{bmatrix}a_{1r} \\\\ a_{2r} \\\\ \\vdots \\\\ a_{mr} \\end{bmatrix}\n\\end{aligned}\n$$\n> and thus any column of $A$ can be written as a linear combination of $r$ distinct column vectors. We deduce that the column rank of $A$ is less than or equal to $r$ by definition. <br/><br/>\nAs the exact same argument can be applied to $A^T$ to prove that the row rank is less than or equal to the column rank, we conclude that equal to each other. $\\square$\n\nTo calculate the rank of a matrix, Gaussian elimination once again comes in handy; the number of linearly independent row vectors does not change under elementary row operations or reordering of columns, as adding a linear multiple of one row to another retains linear independence. Thus, the rank of a matrix is the number of non-empty rows it has after Gaussian elimination.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Observation</span>. As previously proven, the determinant of a square matrix is the product of the elements on its diagonal after Gaussian elimination multiplied by a sign. Thus if $\\det A = 0$, then at least one of these elements must be zero; this would indicate that at least one row is zero after Gaussian elimination, and that the rows of $A$ are linearly dependent.\n\n## Homogeneous systems of linear equations\n\nThis section will analyze $n\\times n$ matrices $A$, which represent linear systems of the form $A\\mathbf{x=0}$, deemed **homogeneous systems**. As the inverse of a square matrix $A$ involves the reciprocal of $\\det A$, the condition for invertibility is $\\det A \\neq 0$; thus if $\\det A \\neq 0$, then $A$ is invertible and $\\mathbf{x=0}$ is the unique solution to the system. If $\\det A = 0$, however, we're screwed.\n\nWait, hang on, we're not screwed. Let's take a closer look at $A\\mathbf{x=0}$, under the condition $\\det A = 0$ - now with a renewed understanding of rank!\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. If $\\det A = 0$, which implies the non-existence of $A^{-1}$, then the matrix $A$ is **non-invertible** or **singular** (not to be confused with **single**, which describes\n\n> A **non-singular** matrix is such that $\\det A \\neq 0$ (an inverse exists).\n\n### The Geometrical View\n\nConsider the special case where $A$ is a **real** $3\\times 3$ matrix, such that our realm of existence is $\\mathbb{R}^3$ and our puny brains can actually comprehend what we're doing. Denote the rows of $A$ as $r^T_{(i)}$, such that\n$$\nA = \\begin{bmatrix}\nr^T_{(1)} \\\\\nr^T_{(2)} \\\\\nr^T_{(3)}\n\\end{bmatrix}\n$$\nwhere each $r_{(i)}$ is a row matrix rather than a single component. Thus, $A\\mathbf{x=0}$ can be rewritten in scalar form as\n$$\nr_{(i)}^T \\mathbf{x} = r_{(i)} \\cdot \\mathbf{x} = 0\n$$\nwhich represent three planes perpendicular to the vector $r_{(i)}$ in $\\mathbb{R^3}$. As such, the set of solution vectors $\\mathbf{x}$ lies at the intersection of these three planes, which takes one of the following three forms:\n1. $r(A) = 3 \\iff$ The planes intersect at a single point. As $\\mathbf{x=0}$ is a solution to $A\\mathbf{x=0}$, the point of intersection is necessarily the origin $O$.\n\n    As stated previously, this is contingent upon $\\det A \\neq 0$, implying that Gaussian elimination on $A$ results in a full diagonal; this would mean that $r(A) = 3$ (all of the rows are linearly independent). This can also be demonstrated by the determinant being equivalent to the triple product $(r_{(1)}^T \\times r_{(2)}^T)\\cdot r_{(3)}^T$ in $\\mathbb{R}^3$, which implies linear independence.\n\n    If $\\mathbf{x}$ lies in the intersection of the three planes, then we simultaneously have\n    $$\n    \\begin{cases}\n    r_{(1)} \\cdot \\mathbf{x} = 0 \\\\\n    r_{(2)} \\cdot \\mathbf{x} = 0\n    \\end{cases}\n    $$\n    which implies that $\\mathbf{x}$ is perpendicular to both $r_{(1)}$ and $r_{(2)}$, and thus $\\mathbf{x} = \\lambda (r_{(1)}\\times r_{(2)})$, the cross-product of the two vectors. We also have\n    $$\n    r_{(3)}\\cdot \\mathbf{x} = r_{(3)}\\cdot (\\lambda(r_{(1)}\\times r_{(2)})) = 0\n    $$\n    which implies $\\lambda = 0$, as $\\det A \\neq 0$ implies that the above triple product is nonzero. Thus $\\mathbf{x = 0}$ only, and $n(A)$, the nullity of $A$, is the dimension of the zero vector (which is zero). \n\n    (As an added bonus, we confirm that $r(A)+n(A)=\\dim A = 3$.)\n\n2. $r(A) = 2 \\iff$ The planes intersect in a line. $r(A) = 2$ implies that exactly two rows of $A$ are linearly independent; without loss of generosity, assume that $r_{(1)}$ and $r_{(2)}$ are linearly independent, meaning that they do not lie on the same line and $(r_{(1)}\\times r_{(2)}) \\neq 0$. \n\n    In this case $\\det A = 0$, because Gaussian elimination results in a diagonal with one zero. Following upon the above, we have $\\mathbf{x} = \\lambda (r_{(1)}\\times r_{(2)})$; as $\\det A = 0$, the triple product $(r_{(1)}\\times r_{(2)})\\cdot r_{(3)} = 0$, meaning that $\\mathbf{x}\\cdot r_{(3)}$ is already satisfied for all $\\mathbf{x}$. Thus, $\\mathbf{x}$ need only lie on the line $\\mathbf{x} = \\lambda (r_{(1)}\\times r_{(2)})$. This would imply that $n(A)$, the dimension of the set of vectors $\\mathbf{x}$, is 1 (a line). ($r(A) + n(A)$ is again 3).\n\n3. $r(A) = 1 \\iff$ The planes are all the same plane; all the rows of $A$ are linearly independent, implying that they are all parallel. This would mean that $r_{(i)} \\times \\mathbf{x}$ all describe the same plane, and thus the solution space is a plane with dimension $2$ ($n(A) = 2$ and $r(A)+n(A)=3$).\n\n### The Linear Map View\n\nAs previously defined, the kernel or null-space of a matrix $A$ (now a $n\\times n$ matrix, no longer restricted to the particular $3\\times 3$ case) is the set of all vectors $\\mathbf{x}$ such that\n$$\nA\\mathbf{x} = 0,\n$$\nmatching our definition for the homogeneous system of equations. As such $K(A)$ is the solution space of the system, with dimension given by $n(A)$, the nullity of $A$; by the rank-nullity theorem, $r(A) + n(A) = n$. This can be demonstrated through the following two cases; in essence, this constitutes a proof of the rank-nullity theorem.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. **Alternate proof of the rank-nullity theorem**.\n\n**Case 1**. $n(A) = 0$; the solution space for $A\\mathbf{x=0}$ is a single vector. We will claim that $r(A) = n$ in this case, meaning that all the columns (and, therefore, all the rows) of $A$ are linearly independent. If $\\{e_j\\}$ is the standard basis for $\\mathbb{R}^n$, then, as shown previously, the $A(e_j)$ represent the columns of $A$.\n\n**Claim**. The columns of $A$ are linearly independent: if $\\sum_{j=1}^{n} a_{j} A(e_j) = \\mathbf{0}$, then all of $a_1, ..., a_n = 0$.\n\n**Proof**. \n$$\n\\begin{aligned}\n\\sum_{j=1}^{n} a_{j} A(e_j) &= \\mathbf{0} \\\\\nA(\\sum_{j=1}^{n} a_{j} (e_j))&= \\mathbf{0} \\\\\n\\sum_{j=1}^{n} a_{j} (e_j) &= \\mathbf{0}\n\\end{aligned}\n$$\nand thus all of $a_j$ are zero, as the $e_j$ form a basis and are thus linearly independent. This implies that $r(A) = n$.\n\n**Case 2**. $0< n(A) = r \\leq n$. We claim that $r(A) = n-r$. Let $\\{u_j\\}$ for $j = 1, ..., r$ form a basis for the solution space of $A$, or equivalently the kernel of $A$, $K(A)$; let $\\{v_i\\}$ for $i=1,...,n$ form a basis for $A$ itself. By the extension lemma given in \"Rank and Nullity\", we extend $\\{u_j\\}$ to the set of vectors\n$$\nu=\\{u_1, u_2, ..., u_r, v_{r+1}, ..., v_n\\}\n$$\nwhich is a basis of $\\mathbb{R}^n$, as proven previously. (The details of this extension are as follows: take every one of $v_1, ..., v_n$ and proceed in steps. For step $i$, append $v_i$ onto $u$ if it is not in $text{span} u$, and ignore $v_i$ otherwise).\n\nWe then claim that $\\{A(v_{r+1}), A(v_{(r+2)}), ..., A(v_n)\\}$ is a basis of the image of $A$, $A(\\mathbb{R^n})$; this necessitates proving linear independence and span. \n\n**Span**. \nAs\n$$\nu=\\{u_1, u_2, ..., u_r, v_{r+1}, ..., v_n\\}\n$$\nis a basis of $\\mathbb{R^n}$, any vector $\\mathbf{x} \\in \\mathbb{R}^n$ can be written as a linear combination of these vectors:\n$$\n\\mathbf{x} = a_1 u_1 + a_2 u_2 + ... + a_r u_r + a_{r+1} v_{r+1} + ... + a_n v_n\n$$\nand thus any $A(\\mathbf{x})$ can be written\n$$\nA(\\mathbf{x}) = A(a_1 u_1 + a_2 u_2 + ... + a_r u_r + a_{r+1} v_{r+1} + ... + a_n v_n)\n$$\nhowever, as $\\{u_1, u_2, ..., u_r\\}$ form a basis for the null-space of $A$, we have\n$$\n A(a_1 u_1 + a_2 u_2 + ... + a_r u_r) = 0\n$$\nand thus all $A(\\mathbf{x})$ can be written\n$$\nA(\\mathbf{x}) = A(a_{r+1} v_{r+1} + ... + a_n v_n)\n$$\nfor some $a_{r+1}, ..., a_n$.\n\n**Linear independence**. Suppose that $A(v_{i})$ for some $i = r+1, ..., n$ can be written as a linear combination of the other $A(v_{j})$:\n$$\nA(v_{i}) = \\sum_{j\\neq i} a_j A(v_{j}).\n$$\nThis would imply that\n$$\nA(\\sum_{j\\neq i} a_j(v_{j}) - v_i) = 0\n$$\nand thus that $\\sum_{j\\neq i} a_j(v_{j}) - v_i \\in K(A)$. As $\\{u_i\\}$ span $K(A)$, it is possible to find $a_i$ for $i = 1, ..., r$ such that\n$$\na_1 u_1 + ... + a_r u_r = \\sum_{j\\neq i} a_j(v_{j}) - v_i\n$$\nwhich contradicts the set \n$$\nu=\\{u_1, u_2, ..., u_r, v_{r+1}, ..., v_n\\}\n$$\nbeing a linearly independent basis. $\\square$\n\n## Inhomogeneous systems of linear equations\nFor a $n\\times n$ matrix $A$ and a column vector $\\mathbf{d}$ with $n$ rows, \n$$\nA\\mathbf{x} = \\mathbf{d}\n$$\ndescribes an inhomogeneous system of linear equations. We've already commented on the **invertible case**: if $\\det A \\neq 0$, $A$ has an inverse and $\\mathbf{x}$ has a unique solution $\\mathbf{x} = A^{-1}\\mathbf{d}$. However, if $\\det A = 0$, then $n(A) > 0, r(A) < n$ and either:\n1. $\\mathbf{d}$ is not in the image of $A$; as such, there are no solutions and the system is *inconsistent*.\n2. $\\mathbf{d}$ is in the image of $A$. There are at least one solution and the system is *consistent*.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. If $\\mathbf{d} \\in I(A)$, the image of $A$, then all solutions to $A\\mathbf{x}=\\mathbf{d}$ can be written in the form\n$$\n\\mathbf{x} = \\mathbf{x_0+y}\n$$\n> where $\\mathbf{x_0}$ is a **particular solution** of the system and $\\mathbf{y}$ is the solution to the corresponding homogeneous equation, $A\\mathbf{x=0}$.\n\nNote the similarities to homogeneous and inhomogeneous differential equations!\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. If $\\mathbf{y}$ satisfies $A\\mathbf{y=0}$ and $\\mathbf{x_0}$ satisfies $A\\mathbf{x_0=d}$, then\n\n$$\nA(\\mathbf{x_0+y})=A\\mathbf{x_0}+A\\mathbf{y}=\\mathbf{d}+\\mathbf{0}=\\mathbf{d}.\n$$\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>.\n1. If $n(A) = 0$, then the solution is unique (by definition). For a system in $\\mathbb{R}^3$, this implies that the solution is a single point in space.\n2. If $n(A) = 1$, the solution space is a line $\\mathbf{x_0 +} \\lambda\\mathbf{t}$.\n3. If $n(A)=2$, the solution space is a plane.\n","n":0.022}}},{"i":58,"$":{"0":{"v":"Matrix Inverses and Linear Equations","n":0.447},"1":{"v":"## Systems of $2\\times 2$ equations\nConsider a general system of linear equations in two variables $x_1$ and $x_2$:\n$$\n\\begin{cases}\nA_{11} x_1 + A_{12} x_2 = d_1 \\\\\nA_{21} x_1 + A_{22} x_2 = d_2\n\\end{cases}\n$$\nThis can be rewritten in matrix form as\n$$\nA\\mathbf{x=d}\n$$\nwhere column vectors $\\mathbf{x} = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$ and $\\mathbf{d}=\\begin{bmatrix}d_1\\\\d_2\\end{bmatrix}$. The general solution of this system can arise from substitution, e.g. writing $x_2 = \\frac{d_1-A_{11}x_1}{A_{12}}$ and substituting into the second equation:\n$$\n\\begin{aligned}\nA_{21}x_1 + A_{22}x_2 &= A_{21}x_1 + A_{22} \\frac{d_1-A_{11}x_1}{A_{12}} = d_2 \\\\\n&= (A_{21} - \\frac{A_{22}A_{11}}{A_{12}})x_1 +\\frac{A_{22}}{A_{12}}d_1 \\\\\n\\frac{A_{21}A_{12}-A_{11}A_{22}}{A_{12}}x_1 &= d_2 - \\frac{A_{22}}{A_{12}}d_1 \\\\\n\\frac{-\\det A}{A_{12}}x_1 &= \\frac{A_{12}d_2 - A_{22}d_1}{A_{12}} \\\\\nx_1 &= \\frac{A_{22}d_1 - A_{12}d_2}{\\det A}\n\\end{aligned}\n$$\nwith $\\det A = A_{22}A_{11}-A{12}A_{21}$, as previously defined. Analogously, substituting $x_1$ back into the equations gives \n$$\nx_2 = \\frac{- A_{21}d_1 + A_{11}d_2}{\\det A}\n$$\nThis solution is unique as long as $\\det A \\neq 0$. Thus, the linear system of two equations in two variables represented by $A\\mathbf{x=d}$ has solution \n$$\n\\begin{aligned}\n\\mathbf{x} &= \\begin{bmatrix}\n\\frac{A_{22}d_1 - A_{12}d_2}{\\det A} \\\\\n\\frac{- A_{21}d_1 + A_{11}d_2}{\\det A}\n\\end{bmatrix} \\\\\n&= \\frac{1}{\\det A}\\begin{bmatrix}\nA_{22}d_1 - A_{12}d_2 \\\\\n- A_{21}d_1 + A_{11}d_2\n\\end{bmatrix} \\\\\n&= \\frac{1}{\\det A} \\begin{bmatrix}\nA_{22} &- A_{12} \\\\\n- A_{21} & A_{11}\n\\end{bmatrix}\\begin{bmatrix}\nd_1 \\\\\nd_2\n\\end{bmatrix} = \\frac{1}{\\det A} \\begin{bmatrix}\nA_{22} &- A_{12} \\\\\n- A_{21} & A_{11}\n\\end{bmatrix}\\mathbf{d}\n\\end{aligned}\n$$\nHowever, matrix algebra also gives $\\mathbf{x = A^{-1}d}$. Thus we conclude that:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The inverse of a $2 \\times 2$ matrix $A$ is given generally by:\n$$\nA^{-1} = \\frac{1}{\\det A} \\begin{bmatrix}\nA_{22} &- A_{12} \\\\\n- A_{21} & A_{11}\n\\end{bmatrix}.\n$$\nNote that this only makes sense if $\\det A \\neq 0$ (more on this later!)\n## Inverse of a general $n\\times n$ matrix\n\nRecall the *Laplace expansion formulae* for determinants of an $n\\times n$ matrix, $A=\\{A_{ij}\\}$:\n\n$$\n\\det A = \\sum_{j}A_{ij}\\Delta_{ij} = \\sum_{i}A_{ij}\\Delta_{ij}\n$$\nNow suppose that we replace the $I$th row of $A$ with the $i$th row of $A$, $I\\neq i$, such that we create a new matrix $A'$ with two identical rows (row $I$ = row $i$). Per a previously proven property, the matrix $A'$ has zero determinant as it has two identical rows; however, the cofactors $\\Delta_{Ij}$ will remain unchanged as they involve the rest of the matrix aside from row $I$, which were not affected. Using row $I$ to take the determinant via the Laplace expansion formula gives\n$$\n\\sum_{j}A'_{Ij}\\Delta_{Ij}=\\sum_{j}A_{ij}\\Delta_{Ij}=0\n$$\nSwapping the $I$th column of $A$ with the $i$th column produces an identical result:\n$$\n\\sum_{j}A'_{jI}\\Delta_{jI}=\\sum_{j}A_{ji}\\Delta_{jI}=0\n$$\nThus we have, for any $I$ and using suffix notation:\n$$\n\\begin{cases}\nA_{ji}\\Delta_{jI}=0 \\\\\nA_{ij}\\Delta_{Ij}=0\n\\end{cases}\n$$\nif $i \\neq I$. However, if $i = I$, nothing has been swapped and we simply have\n$$\n\\begin{cases}\nA_{ji}\\Delta_{jI}=\\det A \\\\\nA_{ij}\\Delta_{Ij}=\\det A\n\\end{cases}\n$$\nby Laplace expansion; alternatively, using the Kronecker delta, we have\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>.\n$$\n\\begin{cases}\nA_{ji}\\Delta_{jI}=\\delta_{iI}\\det A, \\text{ summing over $j$} \\\\\nA_{ij}\\Delta_{Ij}=\\delta_{iI}\\det A, \\text{ summing over $j$}\n\\end{cases}\n$$\nThis leads finally to\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **General inverse for $n\\times n$ matrices**. For a $n\\times n$ square matrix $A$ with $\\det A \\neq 0$, its inverse is given by \n$$\nA^{-1} = \\frac{1}{\\det A}\\{\\Delta_{ji}\\} = \\frac{1}{\\det A} \\begin{bmatrix}\n\\Delta_{11} & \\Delta_{12} & ... & \\Delta_{1n} \\\\\n\\Delta_{21} & \\Delta_{22} & ... & \\Delta_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\Delta_{n1} & \\Delta_{n2} & ... & \\Delta_{nn}\n\\end{bmatrix}^T\n$$\n(Note the tiny little transpose there!)\n> We assert that $A^{-1} A = AA^{-1} = I$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n$$\n\\begin{aligned}\nA^{-1}A &= \\frac{1}{\\det A} \\Delta_{ik}A_{jk} \\\\\n&= \\frac{\\det A}{\\det A} \\delta_{ij} \\text{ by the above lemma} \\\\\n&= \\delta_{ij}, \\text{ matching the identity matrix} \\\\ \\\\\nAA^{-1} &= \\frac{1}{\\det A} A_{ik}\\Delta_{jk}  \\\\\n&= \\frac{\\det A}{\\det A} \\delta_{ij} \\text{ by the above lemma} \\\\\n&= \\delta_{ij}, \\text{ matching the identity matrix} \\\\ \\\\\n\\end{aligned}\n$$\n> as $AA^{-1} = I$ and $A^{-1}A=I$, $A^{-1}$ is by definition the inverse of $I$. (A little on-the-nose, maybe, but it's true!)\n\nEssentially, this tells us that taking an inverse of a $n\\times n$ matrix is as easy as listening to Mozart sped up in your left ear while listening to the Chinese version of the Bible in reverse in your right: all you need to do is to form a matrix of cofactors $\\Delta$ from $A$ by taking the cofactor of each term and leaving it there, then transpose it, then dividing it by $\\det A$. What? What do you mean that's not easy? What do you mean we need to take a bajillion determinants and end up with something with time complexity $O(n\\cdot n!)$?\n\n(Okay, so let's take a rain check on that one for now. Moving on!)\n\n## Solving linear equations\n\nBuckle your seatbelts, guzzle down three twelve-packs of Red Bull, and PREPARE THYSELVES for the spectactular extravaganza that lies ahead, for I will demonstrate to you for the first time in your puny lives how to neutralize the terrifying, hellish devils the likes of which your minds have never comprehended. Dare you brave $3x+4=5$, for instance? Or - ***shudder*** - dare you penetrate the horrors of (gasp) $\\frac{x}{2} + 3 = 6$? Or -\n\n(*What? Wrong linear equations?*)\n\nOkay, so never mind. Let's take a look at the above $2\\times 2$ example of systems of linear equations, and see how we can generalize it:\n$$\nA\\mathbf{x=d}\n$$\nThis forms a valid system of $n$ equations in $n$ variables if $A$ is $n\\times n$, $\\mathbf{x}$ is $n \\times 1$, and $\\mathbf{d}$ is $n \\times 1$. \n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. If $\\mathbf{d}$ is the zero vector, the system of equations is deemed **homogeneous**; otherwise, it is **inhomogeneous**.\n\nAs demonstrated, linear algebra leads to\n$$\n\\mathbf{x=}A^{-1}\\mathbf{d}\n$$\nif the system is inhomogeneous. Trying to calculate the solution vector $\\mathbf{x}$ through directly calculating the inverse of $A$ would lead to a very tired hand and a thoroughly punched calculator. Exactly how punched? Consider just the step of taking $\\det A$: using Laplace expansion, $\\det A$ can be written as a sum of $n$ determinants of $(n-1)\\times (n-1)$ matrices (cofactors of $A$), which, in turn, can each be written as a sum of $(n-1)$ determinants of $(n-2) \\times (n-2)$ matrices, etc. \n\nIn other words, if $f_n$ represents the number of operations required to calculate $\\det A$, the we have\n$$\nf_n = n f_{n-1} + 2n - 1\n$$\nrecursively, as an additional $n-1$ additions and $n$ multiplications must be made between the determinants. This leads to the $O(n\\cdot n!)$ time-complexity mentioned above - a shudderingly-large number, one that must not be disturbed with a ten-kilometer iron pole while wearing ten hazmat suits and a blast jacket.  \n\nThat's no good. Clearly, we need:\n\n## Gaussian elimination\n\nWe're going to throw matrices into the trash bin like the pieces of scrapyard junk they are; they're cramping our style. Instead, we're going back to treating systems of equations like systems of equations:\n\n$$\n\\begin{cases}\nA_{11}x_1 + A_{12}x_2 + ... + A_{1n}x_n = d_1 \\\\\nA_{21} x_1 + A_{22} x_2 + ... + A_{2n} x_n = d_2 \\\\\n\\vdots \\\\\nA_{m1} x_1 + A_{m2} x_2 + ... + A_{mn} x_n = d_m\n\\end{cases}\n$$\n\n(Note that this is a system of $m$ equations with $n$ variables, where $m$ need not equal $n$).\n\nReally, who needs matrices when you have thirty minutes of free time and a truly messed-up fetish for writing the letter $x$?\n\nThe process is familiar. First insist that $A_{11}$, the leading coefficient, is nonzero; if it is, reorder the rows such that the first row has a nonzero leading coefficient. If that isn't possible, i.e. every row has a zero leading coefficient, ~~blow your piece of paper up with a homemade nuke~~ relabel $x_2, ..., x_n$ to $x_1, ..., x_{n-1}$ and start over.\n\nWe begin by eliminating a single variable, e.g. $x_1$. In order to erase every trace of $x_1$ from the miserable existence of this prison we call a system, we will need to subtract every other row by a certain multiple of the first row: if row 1 is denoted $(1) = A_{11}x_1 + A_{12}x_2 + ... + A_{1n}x_n = d_1$, row 2 is denoted $(2) = A_{21}x_1 + A_{22}x_2 + ... + A_{2n}x_n = d_2$ and so on, then $(2) - \\frac{A_{21}}{A_{11}}(1)$ eliminates $x_1$ from $(2)$, $(3)-\\frac{A_{31}}{A_{11}}(1)$ eliminates $x_1$ from $(3)$, ..., $(m)-\\frac{A_{m1}}{A_{11}}(1)$ eliminates $x_1$ from $(m)$. We are thus left with the system\n$$\n\\begin{cases}\n(1), \\\\\n(2) - \\frac{A_{21}}{A_{11}}(1), \\\\\n\\vdots \\\\\n(m) - \\frac{A_{m1}}{A_{11}}(1).\n\\end{cases}\n$$\nThis system does not involve $x_1$ in any equation except the first. And now we do it again; first insist that the leading coefficient of $x_2$ is nonzero (and take the necessary measures to ensure it is so), then eliminate $x_2$ from every equation below the second (the third, the fourth, etc.), and so on - where, after $x_2$ is eliminated, only the first and second equations will contain $x_2$. And so will only the first three equations contain $x_3$, the first four $x_4$, etc. etc. This leaves us with a system of the form\n$$\n\\begin{cases}\nA_{11}x_1 + A_{12}x_2 + ... + A_{1n}x_n = d_1 \\\\\n0x_1 +A_{22}'x_2 + ... + A_{2n}'x_n = d_2' \\\\\n0x_1 + 0x_2 + A_{33}'x_3 + ... + A_{3n}'x_n = d_3' \\\\\n0x_1 + 0x_2 + 0x_3 + A_{44}'x_4 + ... + A_{4n}'x_n = d_4' \\\\\n\\vdots \\\\\n0x_1 + 0x_2 + 0x_3 + ... + A_{mn}'x_m = d_m' \\\\\n\\end{cases}\n$$\nwhere $d'$ and $A'$ denote the remaining coefficients after elimination. Note that every subsequent row will have less variables than the last; thus, if a row is zero on the left-hand side, all the next rows will be zero as well. There are now three possible cases for the system.\n1. **The overdetermined case**. If there exists an $r < n$ such that the left-hand side of equation $(r)$ is zero and the right-hand side $d_r' \\neq 0$, then the system is *inconsistent* and there are no solutions.\n2. **Unique solution**. If there only exists an $r = n+1$ such that the left-hand side of equation $(r)$ is zero (i.e. the previous $n$ equations are nonzero, and only from the $n+1$th equation onwards are the equations zero), and none of $d'_{n+1}, ..., d'_{m}$ are zero, then there is a unique solution that can be found by taking the value of $x_n$ and substituting into equation $(n-1)$, then taking $x_{n-1}$ and substituting into $(n-2)$, etc.\n3. **Infinite solutions**. If there exists an $r<n$ such that the left-hand side of equation $(r)$ is zero and all of $d'_r, d'_{r+1}, ..., d'_m$ are zero, then there are no inconsistencies in the system, but there aren't enough equations to uniquely determine a single solution.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remarks</span>.\n1. It can be convenient to reorder the equations such that $A_{11}$ has the maximum absolute value of $(A_{11},A_{21},...,A_{n1})$.\n2. Gaussian elimination can be achieved in $O(n^3)$ - much more feasible than finding matrix inverses; however, algorithms also exist to find determinants, and thus inverses, in $O(n^3)$ (which are based on Gaussian elimination).\n3. A linear system is consistent if it has at least one solution, and inconsistent if it has none.\n4. The above Gaussian elimination algorithm involves the following **elementary row operations**:\n    - Swapping two rows\n    - Adding a constant multiple of one row to another\n    - Multiplying a row by a scalar constant\n5. Two systems of linear equations are row-equivalent if they can each be obtained from elementary row operations of the other; these systems have the same solutions. \n6. Swapping two rows and adding a constant multiple of one row to another, alongside swapping two columns, change the determinant of the system's associated matrix by only a sign (as previously shown). Thus, performing Gaussian elimination on a matrix and transforming it into\n\n$$\nA' = \\begin{bmatrix}\nA_{11} & A_{12} & A_{13} & ... & A_{1n} \\\\\n0 & A'_{22} & A'_{23} & ... & A'_{2n} \\\\\n0 & 0 & A'_{33} & ... & A'_{3n} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & ... & A'_{nn}\n\\end{bmatrix}\n$$\n> with $\\det A' = (-1)^k \\det A$ where $k$ is the number of column and row swaps performed, leads to $\\det A' = A_{11}A'_{22}A'_{33}...A'_{nn}$ (using the first column for Laplace expansion). As Gaussian elimination can be achieved in $O(n^3)$, the determinant can also be found in $O(n^3)$ (as what remains is simply a multiplication of the diagonal elements).","n":0.023}}},{"i":59,"$":{"0":{"v":"Matrices and Linear Maps","n":0.5},"1":{"v":"\n## Introduction, in briefs\n\ni forgot what i was going to write and spent half an hour reading The Matrix fanfiction instead please send help","n":0.204}}},{"i":60,"$":{"0":{"v":"Rank and Nullity","n":0.577},"1":{"v":"\n## Rank, Kernel and Nullity\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **rank** of a linear map $T: V \\to W$ is the dimension of the image: $r(T) = \\dim T(V)$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **kernel** or **null-space** of a linear map is the set of all vectors $\\mathbf{v} \\in V$ such that $T(\\mathbf{v}) = 0$: essentially, it is the \"zeroes\" of the linear map. Denote the kernel as $K(T) = \\{v \\in V: T(v) = 0\\}$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. $K(T)$ is a subspace of $V$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. For any two elements $\\mathbf{a,b}\\in K(T)$, we have:\n1. $T(a+b)=T(a)+T(b)=0+0$, and as such $a+b\\in K(T)$ by definition.\n2. $T(\\lambda a) = \\lambda T(a) = \\lambda \\times 0 = 0$, and as such $\\lambda a \\in K(T)$.\n3. $\\mathbf{0} \\in K(T)$ because $T(0)=0$ for any $T$. Thus $K(T)$ is closed under linear combinations and is a subspace of $V$ by definition.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **nullity** of $T$ is the dimension of the kernel: $n(T) = \\dim K(T)$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. (**Rank-Nullity Theorem**). ~~First proposed by famous mathematicians John Rank and Joe Nullity in 1869.~~ <br/><br/>\nFor any linear map $T$, we have:\n$$\nr(T) + n(T) = \\dim V\n$$\n> or: the rank and the nullity of $T$ sum to the dimension of its domain. (This is personal for me, because $\\dim T$ used to be my nickname in high school.)\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. ~~Left as an exercise to the reader.~~ (Actually, not really. I have way too much free time on my hands, so I'll do it now.)<br/><br/>\nLet $\\dim V = n$ and $n(T) = m$, with $m \\leq n$ as the null space is a subspace of the domain. We will discuss the following two cases to prove the theorem.<br/><br/>\n**Case 1: $n=m$.** Then the null space of $T$ and the domain of $T$ are the same vector space; both vector spaces are real vector spaces, and if they have the same dimension, then they are necessarily both $\\mathbb{R}^n = \\mathbb{R}^m$. Thus inputting any vector $\\mathbf{x}$ into $T$ gives 0, and the rank of $T$ is 0, as the image of $T$ only contains $\\{0\\}$.<br/><br/>\n**Case 2: $n>m$**. Let $\\{e_1, e_2, ..., e_m\\}$ be the basis of the null space. As they are a linearly independent set of vectors, we can extend the basis to $\\{e_1, e_2, ..., e_m, e_{m+1}, ..., e_n\\}$: a set of $n$ vectors, all linearly independent. The details of this extension are as follows.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. (The **extension lemma**). Let $L = \\{e_1, e_2, ..., e_m\\}$ be a set of $m$ linearly independent vectors in $V$ with $\\dim V = n \\geq m$, and some basis $\\{v_1, v_2, ..., v_n\\}$. Then a basis of $V$ can be constructed which contains $L$, as follows:\n1. Begin with $L$.\n2. For every $i = 1, 2, ..., n$: if $v_i \\in \\text{span}(L)$, keep $L$ as is. Otherwise, addd $v_i$ to the set $L$.\n<br/><br/>\nWe claim that $L$ is both linearly independent and spans $V$. <br/><br/>\nFirst, linear independence: at every step, we ensure that the linear independence of $L$ is preserved because $v_i$ is only added to $L$ if it lies outside the span of $L$. <br/><br/>\nSecond, $L$ spans $V$; every time a $v_i$ is added to $L$ for $i = 1, 2, ..., n$ (Step 2), $v_i$ is in the span of $L$ because either (1) it already was before adding $v_i$ to $L$, or (2) if it wasn't, $v_i$ is added to $L$. Thus, as $\\{v_1, v_2, ..., v_n\\}$ forms a basis of $V$ and $L$ spans all $v_i$, $L$ spans $V$.\n\n> As previously proven, any set of linearly independent vectors of length $n$ forms a basis for $V$ (a vector space with dimension $n$). <br/><br/>\nIn order to prove that $r(T) = \\dim V - n(T) = n - m$, we need to prove that a set of $n-m$ linearly independent vectors spans the image of $T$. <br/><br/>\n**Claim.** $\\{T(e_{m+1}), T(e_{m+2}), ..., T(e_n)\\}$ is such a set. We will first demonstrate that it spans $T(V)$, then demonstrate that it is linearly independent. <br/><br/>\n**Span**. First, as $\\{e_1, e_2, ..., e_m, ..., e_n\\}$ is a basis of $V$, any $v \\in V$ can be written as some linear combination $a_i e_i$. Thus any $T(v) \\in T(V)$ can be written\n$$\n\\begin{aligned}\nT(a_i e_i) &= a_i T(e_i) \\\\\n &= a_1T(e_1) + ... + a_m T(e_m) + a_{m+1}T(e_{m+1}) ... + a_nT(e_n)\n\\end{aligned}\n$$\n> Where every term from $a_1T(e_1)$ to $a_mT(e_m)$ is zero because $e_1, ..., e_m$ lie in the null space of $T$. Thus, $T(v)$ is a linear combination of $T(e_{m+1}), ..., T(e_n)$. <br/><br/>\n**Linear independence**. Suppose for the sake of contradiction that some linear combination \n$$\na_{m+1} T(e_{m+1}) + a_{m+2} T(e_{m+2}) + ... + a_n T(e_n)\n$$\n> equals zero. Then by the definition of a linear map, we have\n$$\nT(a_{m+1}e_{m+1} + a_{m+2} e_{m+2} + ... + a_n e_n) = 0\n$$\n> Or, in other words, the vector $v = a_{m+1} e_{m+1} + ... + a_n e_n$ lies in the null space of $T$. Thus, as $\\{e_1, ..., e_m\\}$ span the null space of $T$, there exist some $a_1, ..., a_m$ such that\n\n$$\na_{m+1} e_{m+1} + ... + a_n e_n = a_1 e_1 + ... + a_m e_m\n$$\n> which implies\n$$\na_1 e_1 + ... + a_m e_m - a_{m+1} e_{m+1} - ... - a_{n}e_n = 0 \n$$\n> i.e. that $e_1, ..., e_{n}$ are not linearly independent. This forms a contradiction with the fact that these vectors form a basis for $V$.\n\n---\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. If ${e_1, e_2, ..., e_n}$ is a basis for $V$, then the image of the linear map $T: V \\to W$, $T(V)$, is spanned by $T(e_1), T(e_2), ..., T(e_n)$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. By definition, every vector $v \\in V$ can be written as a linear combination of the $e_i$s. Thus $T(v)=T(a_1e_1 + a_2e_2 + ... + a_ne_n)$ for some scalars $a_1, a_2, ..., a_n$, and by the properties of a linear map, the right-hand side results in $a_1 T(e_1) + a_2 T(e_2) + ... + a_n T(e_n)$. Thus, every possible $T(v)$ can be written as a linear combination of $T(e_1)$, $T(e_2)$, ..., $T(e_n)$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span> (**Existence and uniqueness**).  If $\\{\\mathbf{e_j}\\}$ ($j = 1, 2, ..., n$) is a basis of $V$, and $\\{\\mathbf{w_j}\\}$ ($j=1,2,...,n$) is a set of $n$ vectors in $W$, then there is a unique linear map $T$ mapping $\\{\\mathbf{e_j}\\}$ to $\\{\\mathbf{w_j}\\}$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. **Existence**. Define the linear map $T: V \\to W$ as $T(c_j e_j) = c_j w_j$ (using suffix notation), for any scalars $c_1, ..., c_n$. Then $T(e_j) = w_j$ for any $j$, as shown by setting $c_j = 1$ and all other $c_i = 0$, and the domain of T covers the entirety of $V$ as $\\{e_j\\}$ form a basis. Thus, $T$ satisfies the desired properties. <br/><br/>\nIt may also be verified that $T$ is closed under linear combinations:\n\n$$\n\\begin{aligned}\nT(c_1 v_1 + c_2 v_2) \\text{ for any $v_1, v_2 \\in V$ and $c_1, c_2 \\in R$} \\\\\n= T(c_1\\sum_{i=1}^{n} a_i e_i + c_2\\sum_{i=1}^{n} b_i e_i) \\text{ for some $a_i, b_i$} \\\\\n= T(\\sum_{i=1}^{n} (c_1a_i + c_2b_i)e_i) \\\\\n= \\sum_{i=1}^{n} (c_1a_i + c_2b_i)w_i \\text{ by definition} \\\\\n= c_1\\sum_{i=1}^{n} a_i w_i + c_2 \\sum_{i=1}^{n} b_i w_i \\\\\n= c_1T(v_1) + c_2 T(v_2) \\text{ by definition}\n\\end{aligned}\n$$\n> where Step 2 is due to $\\{e_i\\}$ being a basis. Thus, as $T$ is closed under linear combinations and the domain of $T$ is $V$ (and $T(0)=0$), $T$ is a linear map from $V$ to $W$ that satisfy the given properties.\n\n> **Uniqueness**. Suppose another linear map $S \\neq T$ exists that maps $e_j$ to $w_j$. Then $S(e_j)=w_j$; by the definition of a linear map, for scalars $c_j$ we have $c_jS(e_j) = c_jw_j$ and thus, $S(c_j e_j)=c_jw_j$ using suffix notation. This matches the original definition of $T$, and so $S = T$, reaching a contradiction. Therefore $T$ is the unique linear map that maps $e_j$ to $w_j$ in $\\text{span}(e_1, e_2, ..., e_n)$, and as the $e_j$s span $V$, $T$ is unique in $V$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Let $\\mathcal{L}(V,W)$ denote all linear maps from $V \\to W$. Then $\\mathcal{L}(V,W)$ is a vector space, under the following definitions of addition and scalar multiplication for $S, T \\in \\mathcal{L}$:\n1. $(S+T)(\\mathbf{x}) = S(\\mathbf{x})+T(\\mathbf{x})$. \n2. $(\\lambda S)(\\mathbf{x}) = \\lambda(S(\\mathbf{x}))$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. It can be shown that both $(S+T)(\\mathbf{x})$ and $(\\lambda S)$ satisfy the properties of a linear map as well, demonstrating that $\\mathcal{L}$ is closed under addition and scalar multiplication.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. For two linear maps $S: U \\to V$ and $T: V \\to W$, the **composite** map $TS: U \\to W$ is such that $TS(\\mathbf{x}) = T(S(\\mathbf{x}))$ for $\\mathbf{x} \\in U$. Note that order matters here: $S$ acts first, then $T$.\n\nFor $TS$ to be well defined, $S(\\mathbf{x})$ must be in the domain of $T$; in other words, the image of $S$ must be a subset of the domain of $T$. Furthermore, when $S$ and $T$ are linear maps, they obey the following identities:\n1. **Associativity.** For linear maps $S$, $T$, $U$, $S(TU)$ = $(ST)U$ whenever the products are well-defined.\n2. **Identity.** Suppose that the linear map $I$ maps vectors onto themselves. Then $TI = IT = T$.\n3. **Distributivity.** $(S+T)(U) = SU+TU$, and $S(T+U)=ST+SU$ if all the products are well-defined.\n\nHowever, linear maps do not satisfy commutativity; $ST$ does not necessarily equal $TS$.\n\nAll the above leads to the following result...","n":0.025}}},{"i":61,"$":{"0":{"v":"Permutations","n":1},"1":{"v":"## Permutation notation\n\nPermutations are ways to arrange the order of $n$ distinct objects without stealing one of them, eating one of them, trying to make your newborn son blend in among them, or otherwise changing them in any way. They contain many of the most shocking results in mathematics; for instance, do you know how many distinct permutations the list $\\{1,2,3,...,25\\}$ has? only 25!\n\n> <span style=\"background-color: #03cafc; color: black;\">Disclaimer</span>. No, the factorial jokes do not get better from this point onwards.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Notation</span>. Let $\\rho$ be a permutation of the set $\\{1,2,3,...,n\\}$, such that the number in position 1 is $\\rho(1)$, position 2 is $\\rho(2)$, and so on. Then we write\n$$\n\\rho = \n\\begin{bmatrix}\n1 & 2 & ... & n \\\\\n\\rho(1) & \\rho(2) & ... & \\rho(n)\n\\end{bmatrix}\n$$\n> where the order of the columns do not matter.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **inverse** of $\\rho$ as \n$$\n\\rho^{-1} = \n\\begin{bmatrix}\n\\rho(1) & \\rho(2) & ... & \\rho(n) \\\\\n1 & 2 & ... & n\n\\end{bmatrix}\n$$\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define a **fixed point** $k \\in \\{1,2,...,n\\}$ as a point where $\\rho(k) = k$; it is unchanged by the permutation.\n\nWhen these absurdly unsubstantiated definitions start coming, they don't stop coming. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Two permutations are **disjoint** if a point moved by one permutation is fixed by the other permutation; no point is moved by both permutations. In otherwords, if for permutations $\\rho_1$ and $\\rho_2$, $\\rho_1(k)\\neq  k$ implies $\\rho_2(k) = k$ and vice versa, the two permutations are disjoint.\n\nFor instance, the permutations\n$$\n\\rho_1=\\begin{bmatrix}\n1 & 2 & 3 \\\\\n3 & 1 & 2\n\\end{bmatrix}\n$$\nand \n$$\n\\rho_2=\\begin{bmatrix}\n4 & 5 & 6 \\\\\n6 & 4 & 5\n\\end{bmatrix}\n$$\nare disjoint over $\\{1,2,3,4,5,6\\}$ as they do not ever move the same number.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. Disjoint permutations are commutative; $\\rho_1 \\rho_2 = \\rho_2 \\rho_1$. This can be intuitively felt, as they do not apply to the same numbers, meaning that applying them in any order yields the same result.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Cycles** of length $q$ (\"q-cycles\") refer to the permutation\n$$\n\\rho_q  = \\begin{bmatrix}\na_1 & a_2 & a_3 & ... & a_{q-1} & a_q \\\\\na_2 & a_3 & a_4 & ... & a_{q} & a_1\n\\end{bmatrix}\n$$\n> In other words, the top row is shifted to the left by one unit. We also denote this cycle by $(a_1, a_2, ..., a_q)$.\n\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **transposition** is a 2-cycle $(a_1,a_2)$ that acts simply to swap the positions of $a_1$ and $a_2$. A transposition is its own inverse.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **The standard representation**. Any permutation can be written as a product of disjoint cycles, or a product of transpositions; this simply expresses the notion that you can arrange a list of items in any order by swapping the positions of elements one pair of a time. \n\nNote that such a representation is non-unique.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **sign** of a permutation as corresponding to the number of transpositions required to express it; if $\\rho$ is a product of $r$ transpositions (i.e. we need $r$ swaps to bring about $\\rho$), then the sign of $\\rho$, $\\epsilon(\\rho)$, is defined to be $(-)^r$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. It follows that for permutations $\\rho, \\sigma$, $\\epsilon(\\rho\\sigma)=\\epsilon(\\rho)\\epsilon(\\sigma)$ and $\\epsilon(\\rho^{-1})=\\epsilon(\\rho)$, as inverses simply reverse the permutation with the same amount of steps.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>.  **Generalized $\\epsilon$ Symbol**, or the **Levi-Civita Symbol**. If $(j_1,j_2,...,j_n)$ is a permutation, define $\\epsilon_{(j_1,j_2,...,j_n)}$ as the sign of the permutation; otherwise, it is zero.\n\n","n":0.042}}},{"i":62,"$":{"0":{"v":"Matrix Operations","n":0.707},"1":{"v":"## Matrix algebra\n\nIn addition *(ba-dum tss)* to the matrix addition, scalar multiplication, and multiplication operations defined previously, we define the following operations on matrices:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Matrix transpose**. For any matrix $A = \\{A_{ij}\\}$, define its transpose $A^T$ as $\\{A_{ji}\\}$: $A$ with rows and columns swapped.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. **Properties of the matrix transpose**:\n1. $(A^T)^T = A$.\n2. Transposing a column vector yields a row vector.\n3. $(AB)^T = B^T A^T$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n$$\n\\begin{aligned}\n(AB)^T_{ij} &= (AB)_{ji} \\\\\n&= A_{jk} B_{ki} \\\\\n&= B_{ki} A_{jk} \\\\\n&= (B^T)_{ik} (A^{T})_{kj} \\\\\n&= B^T A^T\n\\end{aligned}\n$$\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Hermitian conjugate**. For any matrix $A = \\{A_{ij}\\}$, define its Hermitian conjugate $A^{\\dagger}$ as $\\{A_{ji}^*\\}$: in other words, $A^{\\dagger} = (A^T)^*$ - the conjugate of $A$ transpose.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The Hermitian conjugate obeys the above three properties of the matrix transpose.\n\n## Matrix classifications\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Symmetric matrices** $A$ are such that $A = A^T$: symmetry arises across the matrix's diagonal.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Anti-symmetric** or **skew-symmetric** matrices are such that $A = -A^T$. This necessitates that the diagonal elements of the matrix are zero, as any $A_{ii}$ lying on the diagonal remain unchanged by the transpose; as such, $A = -A^T$ implies $A_{ii} = -A_{ii} = 0$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Hermitian** matrices are such that $A = A^{\\dagger}$. This implies that the diagonal elements of $A$ are all real, as real numbers are their own conjugate.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Skew-Hermitian** matrices are such that $A = -A^{\\dagger}$. The diagonal elements of such a matrix must be purely imaginary.\n\n## Matrix operations\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Trace**. The trace of a matrix $A$, denoted $Tr(A)$, is the sum of its diagonal elements $A_{ii}$. \n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. For two matrices $B$ and $C$ whose products are square matrices, $Tr(BC) = Tr(CB)$ even though $BC$ and $CB$ are not necessarily equal. This can be seen by simply expanding the diagonal terms of the product.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Identity matrix**. The $n \\times n$ identity matrix $I_{n\\times n}$ is the matrix which provides an analogy to $1$ for matrices: multiplication with $I$ does not change the original matrix, i.e. $AI = IA = A$ for any matrix $A$ (as long as the product is well-defined). We have\n$$\nI_{n\\times n} = \\begin{bmatrix}\n1 & 0 & ... & 0 \\\\\n0 & 1 & ... & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & ... & 1\n\\end{bmatrix} = \\{\\delta_{ij}\\}\n$$\n> with all $1$s on the diagonal and all $0$s everywhere else.\n\n## Matrix decompositions\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>.\nEvery matrix can be written as the sum of a symmetric and an anti-symmetric matrix.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Let $A = \\{A_{ij}\\}$. Observe that for any $A_{ij}$, we have\n$$\nA_{ij} = \\frac{A_{ij}+A_{ji}}{2} + \\frac{A_{ij}-A_{ji}}{2}\n$$\n> Thus define the symmetric matrix $S^{+} = \\{\\frac{A_{ij}+A_{ji}}{2} \\}$ and the antisymmetric matrix $S^{-} = \\{\\frac{A_{ij}-A_{ji}}{2}\\}$, such that $A = S^+ + S^-$. Note that $S^+ = \\frac{A+A^T}{2}$ and $S^- = \\frac{A-A^T}{2}$.\n\nFollowing from this, we also have:\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. Every square matrix can be written as a sum of a symmetric *trace-free* matrix (trace is zero), an antisymmetric matrix and an isotropic matrix.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Isotropic matrices** are scalar multiples of the identity matrix $I$... for now. ~~chuckles ominously~~\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Using the preceding decomposition, we have $A = S^+ + S^-$. The trace of $S^+$ is $\\sum A_{ii}$, which is equal to the trace of $A$; let this trace be $n\\sigma$, where $n$ is the dimension of $A$.<br/><br/>\nIn order to make $S^+$ trace-free, we need to subtract all elements on the diagonal of $S^+$ by $\\sigma$; this results in the matrix $S^{+} - \\sigma I = E$. Thus we have\n$$\nA = E + S^{-} + \\sigma I\n$$\n> where the first matrix $E$ is symmetric and trace-less, the second is antisymmetric, and the third is isotropic.\n\nThis decomposition finds applications in various physical and mathematical scenarios; for instance, a point in a solid body deformed from point with position vector $\\mathbf{x}$ to $A\\mathbf{x}$ for some matrix $A$ will have the following:\n1. *Average expansion/contraction* $\\sigma$.\n2. *Strain* $E$.\n2. *Average rotation* $S^{-}\\mathbf{x}$.\n\n## Matrix inverses\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. For a $m\\times n$ matrix $A$, the $n\\times m$ matrix $B$ deemed the **left inverse** of $A$ is such that\n$$\nBA = I\n$$\n> where $I$ is the $n\\times n$ identity matrix; simultaneously, the $n\\times m$ matrix $C$ deemed the **right inverse** of $A$ is such that\n$$\nAC = I\n$$\n> where $I$ is now the $m \\times m$ identity matrix.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. For a square matrix of dimensions $n \\times n$, the left and right inverses are identical. In fact, this is true for any matrix as long as both the left and right inverses exist.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. By the above, $BA=I \\iff BAC = C \\iff B(AC)=C$ by associativity. As $AC = I$ by definition, we have $B = C$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Invertible matrices** are square matrices whose inverse exists. \n\nNote that if an inverse of a square matrix, denoted $A^{-1}$ for the matrix $A$, does exist, then it is necessarily unique (as the right inverse equals the left inverse).\n\n> <span style=\"background-color: #ffb812; color: black;\">Properties</span>. \n1. $A = (A^{-1})^{-1}.$\n2. $(AB)^{-1} = B^{-1} A^{-1}$. This can be demonstrated through associativity: $ABB^{-1}A^{-1}=A(B B^{-1})A^{-1}=AIA^{-1}=AA^{-1}=I$.\n\nThe inclusion of the inverse allows us to define two more categories of matrices:\n\n## Orthogonal and unitary matrices\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Orthogonal matrices** $A$ are such that the inverse of $A$ is equal to its transpose: $A^{T} = A^{-1}$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. **The rows and columns of an orthogonal matrix form orthonormal sets**.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Let $A_{(i,*)}$ and $A_{(j,*)}$ denote the $i$th and $j$th row of the $n\\times n$ orthonormal matrix $A$, respectively, where $i, j$ are integers from $1$ to $n$. Then the sum of the products of their terms equal \n$$\nA_{(i,*)} (A_{(j,*)})^T = (AA^{T})_{ij} \n$$\n> which is $1$ if $i = j$ and $0$ if $i \\neq j$, as $AA^T = I$. Thus, the dot product between two rows of $A$ are $0$ if they are two different rows and $1$ if they are the same row, matching the definition of an orthonormal set. The proof for columns proceeds similarly.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. If $A$ is an orthogonal matrix representing a linear map from an orthonormal basis $\\{e_i\\}$ to $\\{Ae_i\\}$, then the set $\\{Ae_i\\}$ is also orthonormal.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\nAs previously observed, the columns of a matrix representing a linear map are $Ae_1, Ae_2, ..., Ae_n$. As the columns of an orthogonal matrix are orthonormal, the set $\\{Ae_i\\}$ must also be orthonormal.\n\nExamples of orthonormal matrices include the rotation matrix and the reflection matrix, given in the last section (Examples of Linear Maps).\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. A transformation by a linear map represented by orthogonal matrix $A$ from two vectors $\\mathbf{x, y}$ to $\\mathbf{x', y' = Ax,\\ Ay}$ preserve the real scalar product $\\mathbf{x'\\cdot y' = x\\cdot y}$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Using suffix notation, we write $\\mathbf{x\\cdot y} = x_i y_i$ and $\\mathbf{x'}_i, \\mathbf{y'}_i = A_{ij}x_j, A_{ij}y_j$. Thus we have $\\mathbf{x'}\\cdot \\mathbf{y'} = A_{ij}x_j A_{ik}y_k$. Note that $A_{ij}A_{ik} = (AA^T)_{kj} = \\delta_{kj}$ by $AA^T = I$. This leads to \n$$\n\\mathbf{x'\\cdot y'} = \\delta_{kj}x_jy_k = x_j y_j = x_i y_i = \\mathbf{x\\cdot y}\n$$ \n> as the Delta symbol restricts us to $k = j$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. The same linear map preserves the distance between $\\mathbf{x}$ and $\\mathbf{y}$: $|\\mathbf{x-y}|$. Such a property is termed an **isometry**.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. $|\\mathbf{x'-y'}|^2=(\\mathbf{x'-y')\\cdot(x'-y'}) = \\mathbf{(x-y)\\cdot(x-y)}=|\\mathbf{x-y}|^2$ by the above property.\n\nAs an added note, length-preserving transformations in $\\mathbb{R}^3$ are either orthogonal matrices or outside the bounds of linear maps (e.g. translations).\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **unitary matrix** is the complex analogue to orthogonal matrices: if $A$ is unitary, then $U^{\\dagger} = U^{-1}$. The above properties all hold for unitary matrices, except that they preserve the complex scalar product (with conjugates) instead of the real scalar product.\n\n","n":0.027}}},{"i":63,"$":{"0":{"v":"Matrices as Linear Maps","n":0.5},"1":{"v":"\n## Matrices as linear maps\n\nSuppose $T$ is a linear map from $V$ to $W$, each with basis $\\{\\mathbf{v_i}\\}$ ($i = 1, ..., n$) and $\\{\\mathbf{w_j}\\}$ ($j=1, ..., m$).\n\nConsider the set of vectors that $\\mathbf{v_i}$ maps to under $T$: $\\{T(\\mathbf{v_i})\\}.$ As shown above, this set spans the image $T(V)$; furthermore, as $T(\\mathbf{v_i}) \\in W$, every $T(\\mathbf{v_i})$ can be written uniquely as the linear combination\n$$\nT(\\mathbf{v_i}) = \\sum_{i=1}^{m} A_{ij} \\mathbf{w}_i\n$$\nby definition of a basis. It thus follows that for any general $\\mathbf{x} = \\sum_{j=1}^{n} x_i \\mathbf{v}_i \\in V$, we have \n$$\nT(\\mathbf{x}) = \\sum_{j=1}^{n} \\mathbf{x}_i \\sum_{i=1}^{m} A_{ij} \\mathbf{w}_i\n$$\nand thus\n$$\nT(\\mathbf{x}) = \\sum_{i=1}^{m} (\\sum_{j=1}^{n} x_i A_{ij}) \\mathbf{w}_i\n$$\nby swapping order of summation.\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **matrix** of the linear map $T$ as $\\{A_{ij}\\}$. \n\nAs the above equations hold for any $\\mathbf{x} \\in V$, the matrix of $T$ - uniquely defined, by definition of a basis - is sufficient for determining all the mappings of $T$. \n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. First notice that $T(\\mathbf{x})$ as defined above is equivalent to writing $T(\\mathbf{x}) = \\mathbf{x}' = \\mathbf{x}'_i \\mathbf{w}_i$, where $\\mathbf{x}_i' = \\sum_{j=1}^{n} A_{ij} x_j$. Thus we have\n$$\n\\begin{aligned}\n\\mathbf{x}' &= \\begin{bmatrix}\n\\mathbf{x}'_1 \\\\\n\\mathbf{x}'_2 \\\\\n\\vdots \\\\\n\\mathbf{x}'_n\n\\end{bmatrix}\n\\end{aligned} = \\begin{bmatrix}\nA_{1j}\\mathbf{x}_j \\\\\nA_{2j}\\mathbf{x}_j \\\\\n\\vdots \\\\\nA_{nj}\\mathbf{x}_j\n\\end{bmatrix} = A\\mathbf{x}\n$$\n> by the definition of matrix multiplication found above. Therefore, the linear map $\\mathbf{x} \\to \\mathbf{x}': T$ can be written in **matrix form** as $\\mathbf{x}' = A\\mathbf{x}$. Note that $A$ is a $m \\times n$ matrix.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Observation</span>. $V$ and $W$ are not restricted to a single basis; when different bases are used, the matrix form of $T$ will be different. It is sometimes necessary to specify which two bases $T$ is mapping between: $\\{v_i\\} \\to \\{w_i\\}$.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Observation</span>. If we take the basis $\\{w_i\\}$ to be the standard basis $\\{(1,0,...,0), (0,1,...,0), ..., (0,0,...,1)\\}$ of $\\mathbb{R}^m$, we have\n$$\nT(\\mathbf{v}_i)=\\begin{bmatrix}\nA_{1i} \\\\\nA_{2i} \\\\\n\\vdots \\\\\nA_{mi}\n\\end{bmatrix}\n$$\n> which is the $i$th column of the matrix form of $T$. \n\nThe crucial takeaway from this section is that matrices are a way to **express linear maps**; everything, from the definition of matrix multiplication to all the operations we will see in the following section, is derived from this fact.","n":0.053}}},{"i":64,"$":{"0":{"v":"Higher-Dimensional Determinants","n":0.707},"1":{"v":"> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Using the permutation sign and the precedent of the $3\\times 3$ determinant, define the determinant of a $n \\times n$ matrix $A$ as \n$$\n\\det A = \\epsilon_{j_1 j_2 ... j_n}A_{j_1 1} A_{j_2 2} ... A_{j_n n}\n$$\n> corresponding to the $3\\times 3$ and $2\\times 2$ determinants. The epsilon sign is only nonzero when $j_1, j_2, ..., j_n$ are all chosen to be distinct, i.e. they are a permutation of $\\{1,2,...,n\\}$. As such, we can also rewrite the above as\n\n$$\n\\det A = \\sum_{\\sigma(n)}\\epsilon(\\sigma(n))A_{\\sigma(1) 1}A_{\\sigma(2) 2} ... A_{\\sigma(n) n}\n$$\n> summing over all possible permutations $\\sigma(n)$ of $\\{1,2,3,...,n\\}$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. For any square matrix $A$, $\\det A = \\det A^T$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. We have\n$$\n\\det A^T = \\sum_{\\sigma(n)}\\epsilon(\\sigma(n))A_{1 \\sigma(1)}A_{2 \\sigma(2)} ... A_{n \\sigma(n)}\n$$\n> with subscripts reversed from $\\det A$. For every permutation $\\sigma(n)$, the product $A_{1\\sigma(1)}A_{2\\sigma(2)}...A_{n\\sigma(n)}$ is equivalent to $A_{\\rho(1) 1}A_{\\rho(2) 2}...A_{\\rho(n) n}$, as it is simply a rearrangement of the order of the terms. Thus, $\\det A^T = \\det A$. (For instance, we could choose the permutation $\\sigma(\\sigma^{-1}(n))$.)\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. Let $A'$ be the square matrix $A$ with a **single** row or column multiplied by a scalar $\\lambda$. Then $\\det A' = \\lambda \\det A$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. As $\\det A = \\det A'$, it suffices to prove this property for rows. Suppose that row $j$ for $j \\in \\{1,2,...,n\\}$ is multiplied by $\\lambda$. Then \n\n$$\n\\det A' = \\sum_{\\sigma(n)}\\epsilon(\\sigma(n))A_{1 \\sigma(1)}A_{2 \\sigma(2)} ... (\\lambda A_{j \\sigma(j)}) ... A_{n \\sigma(n)}\n$$\n> which is equal to $\\lambda \\det A$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. Following from the above, $\\det(\\lambda A)=\\lambda^n \\det A$ for a $n\\times n$ matrix $A$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. If $A'$ is $A$ with two rows or columns interchanged, then $\\det A' = - \\det A$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Suppose that rows $i$ and $j$ are swapped, with $i < j$; once again, due to $\\det A = \\det A^T$, it suffices to prove this property for rows. Then we have\n$$\n\\det A' = \\sum_{\\sigma(n)}\\epsilon(\\sigma(n))A'_{1 \\sigma(1)}A'_{2 \\sigma(2)} ... (A'_{i\\sigma(i)})...(A'_{j\\sigma(j)})...A'_{n\\sigma(n)}\n$$\n> All the other $A'$s are left unchanged except $A'_{i\\sigma(i)}$, which becomes $A_{j\\sigma(i)}$ (and vice versa), so we have\n$$\n\\det A' = \\sum_{\\sigma(n)}\\epsilon(\\sigma(n))A_{1\\sigma(1)}A_{2\\sigma(2)}...A_{i\\sigma(j)}...A_{j\\sigma(i)}...A_{n\\sigma (n)}\n$$\n> Let the permutation $\\{\\sigma(1), \\sigma(2),..,\\sigma(j),...\\sigma(i),...\\sigma(n)\\}$ be denoted $\\sigma'$. $\\sigma'$ is equal to $\\sigma$ with one transposition ($\\sigma(i) \\to \\sigma(j)$), and so $\\epsilon(\\sigma')=(-)\\epsilon(\\sigma)$. Thus $\\det A' = \\sum \\epsilon(\\sigma') ... = - \\det A$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. Following from the above, if $A$ has two identical rows or columns, $\\det A = 0$ because swapping the identical rows results in $\\det A' = \\det A = - \\det A$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. If $A'$ is $A$ with a scalar multiple of one row added to another row (or a scalar multiple of one column added to another column), then $\\det A' = \\det A$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Let row $r$ multiplied by a scalar $\\lambda$ be added to row $s$ with $r \\neq s$. By the above, we have\n$$\n\\begin{cases}\nA'_{ij}=A_{ij},\\ i \\neq s \\\\\nA'_{ij} = A_{ij} + \\lambda A_{rj},\\ i = s\n\\end{cases}\n$$\n> and as such\n$$\n\\begin{aligned}\n\\det A' &= \\sum_{\\sigma(n)}A_{1\\sigma(1)}...(A_{s\\sigma(s)}+\\lambda A_{r\\sigma(s)})...A_{n\\sigma (n)} \\\\ \n&= \\det A + \\lambda\\sum_{\\sigma(n)}A_{1\\sigma(1)}...(A_{r\\sigma(r)}A_{r\\sigma(s)})...A_{n\\sigma (n)}\n\\end{aligned}\n$$\n> where the latter term is the determinant of a matrix which has two identical rows $A_r$ and $A_r$, and thus zero according to the above properties. Therefore $\\det A' = \\det A$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. If the rows or columns of $A$ are linearly dependent, then $\\det A = 0$. ","n":0.042}}},{"i":65,"$":{"0":{"v":"Examples of Linear Maps","n":0.5},"1":{"v":"## Examples of matrices as linear maps\n\n1. **Rotation** by $\\theta$ about $z$-axis in $\\mathbb{R}^3$ ($z$-coordinate fixed):\n$$\nR=\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta & 0 \\\\\n\\sin \\theta & \\cos \\theta & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\n2. **Stretch** by a factor of $\\lambda$, $\\mu$, $\\nu$ in the $x$-, $y$- and $z$-axis respectively:\n$$\nS = \\begin{bmatrix}\n\\lambda & 0 & 0 \\\\\n0 & \\mu & 0 \\\\\n0 & 0 & \\nu\n\\end{bmatrix}\n$$\n3. **Reflection **in the plane $\\mathbf{x \\cdot n} = 0$ in $\\mathbb{R}^3$, where $\\mathbf{n}$ is a unit normal vector. Suppose we are reflecting a point with position vector $\\mathbf{p} = \\vec{OP}$ about the plane. Then its image after reflection, $P'$, is connected to $P$ by two times the normal vector from $P$ to the plane. Call the foot of this normal $N$. Then:\n$$\n\\vec{OP'} = \\vec{OP} + \\vec{PN} + \\vec{NP'} = \\vec{OP} - 2 \\vec{NP}\n$$\nwhich equals\n$$\n\\mathbf{p} - 2\\vec{NP}\n$$\nwhere $\\vec{NP}$ is $(\\mathbf{p\\cdot n})\\mathbf{n}$.\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. $\\vec{NP}$ must be parallel to $\\mathbf{n}$ as it is normal to the plane; suppose it is equal to $d\\mathbf{n}$ for some scalar $d$. Then as $N$ lies on the plane, we have $\\vec{ON} = \\vec{OP} - \\vec{NP} = \\mathbf{p} - d\\mathbf{n}$ and $\\vec{ON}\\cdot \\mathbf{n} = 0$ by the equation of the plane. Thus $(\\mathbf{p}-d\\mathbf{n})\\cdot \\mathbf{n} = 0$, yielding $d = \\mathbf{p \\cdot n}$.<br/><br/>\nUsing suffix notation, $\\mathbf{p} - 2(\\mathbf{p\\cdot n})\\mathbf{n}$ can be expanded as follows:\n$$\n\\begin{aligned}\n\\mathbf{p} - 2(\\mathbf{p\\cdot n})\\mathbf{n} &= p_{i} - 2(p_jn_j)n_i \\\\\n&= \\delta_{ij}p_j - 2(p_jn_j)n_i \\\\\n&= p_j(\\delta_{ij}-2n_jn_i) \\\\\n&= \\mathbf{p}'\n\\end{aligned} \n$$\n> Note the trick of using $\\delta_{ij}$ to transform the suffix $i$ into $j$!\n>\nTherefore, the matrix representing a transformation from $\\mathbf{p}$ to $\\mathbf{p}'$, its reflection about a plane with normal vector $\\mathbf{n}$, can be expressed by\n$$\nR = \\{\n\\delta_{ij} - 2n_j n_i\n\\}.\n$$\n\n4. **Shear** in any axis (e.g. the $x$-axis) in $\\mathbb{R}^3$. A shear is any transformation that transforms a vector $(x,y,z)$ onto, for instance, $(x+\\lambda y, y, z)$, preserving the two other coordinates while adding a scalar multiple of another coordinate onto one coordinate. This can be simply expressed as\n$$\nS = \\begin{bmatrix}\n1 & \\lambda & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$","n":0.054}}},{"i":66,"$":{"0":{"v":"Determinants","n":1},"1":{"v":"## Alternative names for determinants\n- The Black Death\n- It That Lurks In The Darkness\n- Satan's Hemmorhoids\n- The Antichrist in Human Clothing \n> This post was made by the #SheldonAxler gang. Let not the evil influences of these unholy idols defile the sanctity of your linear algebra studies; do linear algebra right, stay strong, and stay ~~determined~~ - wait, no, sorry, I didn't mean to -\n\n## Determinants: An Extermination Guide for the Home\n\nConsider the linear map $A$ mapping the standard basis $\\{\\mathbf{e_1, e_2, e_3}\\} = \\{(1,0,0),(0,1,0),(0,0,1)\\}$ in $\\mathbb{R}^3$ to $A\\mathbf{e_1}, A\\mathbf{e_2}, A\\mathbf{e_3}$. The original solid willed into existence by these basis vectors is a unit cube, volume 1; what will be the volume of the new solid formed under the wicked distortions of the almighty $A$?\n\nThe new solid under transformation will be a parallelpiped, enclosed by $A\\mathbf{e_1}, A\\mathbf{e_2}, A\\mathbf{e_3}$; a result from several lifetimes ago tells us that the volume of such a parallel pipi is given by the scalar triple product $A\\mathbf{e_1} \\cdot (A\\mathbf{e_2} \\times A\\mathbf{e_3})$. This, in turn, can be expanded in suffix notation as\n$$\n\\begin{aligned}\nA\\mathbf{e_1} \\cdot (A\\mathbf{e_2} \\times A\\mathbf{e_3}) &= \\epsilon_{ijk} (Ae_1)_i (Ae_2)_j (Ae_3)_k \\\\\n&= \\epsilon_{ijk}A_{ia}(e_1)_{a}A_{jb}(e_2)_{b}A_{kc}(e_3)_{c} \\\\\n&= \\epsilon_{ijk}A_{ia}\\delta_{1a}A_{jb}\\delta_{2b}A_{kc}\\delta_{3c} \\\\\n&\\text{(as $e_1=(1,0,0), e_2=(0,1,0), e_3=(0,0,1)$)} \\\\\n&= \\epsilon_{ijk}A_{i1}A_{j2}A_{k3}\n\\end{aligned}\n$$\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **determinant** of a $3\\times 3$ matrix $A$ to equal the above expression. As shown, it is also equal to the (signed) volume of a parallelpiped produced under transformation of a unit cube by the linear map $A$. \n\n> <span style=\"background-color: #bc42f5; color: black;\">Notation</span>. Denote the determinant by $|A|$, $\\det A$, \"The Black Hand\", or simply \"The Darkness Which Prowls The Night\".\n\n> <span style=\"background-color: #ffb812; color: black;\">Properties</span>.\n1. A linear map $A$ preserves the volume of a solid if and only if its determinant is $\\pm 1$.\n2. If $\\{e_i\\}$ is a set of orthonormal vectors in the right-handed sense, then the set of vectors it transforms to is right-handed if $\\det A > 0$ and left-handed if $\\det A < 0$.\n3. The cross-product $\\mathbf{a} \\times \\mathbf{b}$ can be written in determinant form as \n$$\n\\begin{vmatrix}\n\\mathbf{i} & \\mathbf{j} & \\mathbf{k}\\\\\na_1 & a_2 & a_3 \\\\\nb_1 & b_2 & b_3\n\\end{vmatrix}\n$$\n> Warning: the unironic usage of this formula in any context is a sign of clinical psychopathy.\n\n## Determinants and their demonic offspring\n\nA matrix in $\\mathbb{R}^3$ is effectively a $2\\times 2$ matrix acting in $\\mathbb{R}^2$ if it looks something like this:\n$$\nA = \\begin{bmatrix}\n... & ... & 0 \\\\\n... & ... & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\nIn effect, it changes only the $x$- and $y$-components and does nothing to the $z$-component; it enlarges bodies across the horizontal axes and makes them chunkier, but it fails to affect their height. It may horrify you to know that I was subjected to nearly ten years of transformation under $A$ during my teenage years.\n\nFor such a matrix, we recall the above definition of the determinant \n$\\det A = \\epsilon_{ijk}A_{i1}A_{j2}A_{k3} = \\epsilon_{ij3}A_{i1}A_{j2}$ as $A_{k3}$ is only nonzero with $k=3$. This leads simply to \n$$\n\\det A = \\epsilon_{123}A_{11}A_{22} + \\epsilon_{213}A_{21}A_{12} = A_{11}A_{22}-A_{21}A_{12}\n$$\nas any terms $A_{31}, A_{32}$ etc. are zero. For a $2 \\times 2$ matrix\n$$\n\\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix}\n$$\nthis is simply the product of terms on one diagonal minus the product of terms on the other.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Observation</span>. Harvesting this demonic offspring of the $3\\times 3$ determinant allows us to rewrite the $3\\times 3$ determinant more clearly as\n$$\n\\det A = A_{11}\\begin{vmatrix}\nA_{22} & A_{23} \\\\\nA_{32} & A_{33}\n\\end{vmatrix} -\nA_{21}\\begin{vmatrix}\nA_{12} & A_{13} \\\\\nA_{32} & A_{33}\n\\end{vmatrix}\n+\nA_{31}\\begin{vmatrix}\nA_{12} & A_{13} \\\\\nA_{22} & A_{23}\n\\end{vmatrix}\n$$\n> noting the sign pattern.\n\n\n","n":0.042}}},{"i":67,"$":{"0":{"v":"Determinants of Products","n":0.577},"1":{"v":"## Determinant of a product\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. **Switching subscripts of a sum of permutations.** The determinant, expressed as the sum\n$$\n\\sum_{\\sigma \\in S}\\epsilon(\\sigma)A_{\\sigma(i) i}\n$$\n> is equivalent to\n$$\n\\sum_{\\sigma \\in S}\\epsilon(\\sigma)A_{\\sigma(\\rho(i)) \\rho(i)}\n$$\n> where $\\rho$ is another permutation. This is because the two sums are simply a reordering of one another ($\\sigma(\\rho(i))\\rho(i)$ encompasses the same numbers $1,2,3,...,n$ as $\\sigma(i)i$)\n\n(Note: $A_{\\sigma(i)i}$ is shorthand for the product of all $A_{\\sigma(i) i}$s. This is a horrific abuse of notation and probably a war crime cf. Geneva Convention Clause 48 Article 9.2, but do I look like someone who cares?)\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. For a square matrix $A$ and a permutation $\\rho$, $\\epsilon(\\rho)\\det A = \\sum_{\\sigma \\in S}\\epsilon(\\sigma)A_{\\sigma(i) \\rho(i)}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof.</span> To get from the first sequence to the second sequence, we want\n$$\n\\begin{aligned}\n\\epsilon(\\rho)\\det A &= \\epsilon(\\rho)\\sum_{\\sigma \\in S}\\epsilon(\\sigma)A_{\\sigma(i)i} \\\\\n&=\\sum_{\\sigma \\in S}\\epsilon(\\sigma (\\rho))A_{\\sigma(i)i} \\\\\n&= \\sum_{\\sigma \\in S}\\epsilon(\\sigma (\\rho))A_{\\sigma(\\rho(i))\\rho(i)} \\text{ by the above lemma} \\\\\n&= \\sum_{\\tau \\in S}\\epsilon(\\tau)A_{\\tau(i)\\rho(i)}\n\\end{aligned}\n$$\n> where $\\tau(i) = \\sigma(\\rho(i))$. We can also write in subscript notation\n\n$$\n\\epsilon_{j_1 j_2 ... j_n} \\det A = \\sum_{i_1, i_2,...,i_n}\\epsilon_{i_1 i_2 ... i_n}A_{i_kj_k}.\n$$\n\nThis gives us enough firepower to prove:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. For any two matrices $A$ and $B$, $\\det (AB) = \\det A \\det B$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n$$\n\\begin{aligned}\n\\det (AB) &= \\sum_{\\sigma \\in S}\\epsilon(\\sigma) (AB)_{\\sigma(i) i} \\\\\n&= \\sum_{\\sigma \\in S}\\epsilon(\\sigma) (A_{\\sigma(i)j_i)}B_{j_i i}) \\\\\n&= \\sum_{\\sigma \\in S}\\epsilon(\\sigma) A_{\\sigma(1)j_1}B_{j_1 1}A_{\\sigma(2)j_2}B_{j_2 2}...A_{\\sigma(n)j_n}B_{j_n n} \\\\\n&= \\sum_{i_1, i_2, ..., i_n}\\epsilon_{i_1 i_2 ... i_n}A_{\\sigma(1)j_1}A_{\\sigma(2)j_2}...A_{\\sigma(n)j_n}B_{j_1 1}B_{j_2 2}...B_{j_n n} \\\\\n&= \\sum_{i_1, i_2, ..., i_n}\\epsilon_{j_1 j_2 ... j_n}(\\det A)B_{j_1 1}B_{j_2 2}...B_{j_n n} \\text{ by the above lemma}\\\\\n&= \\det A \\det B.\n\n\\end{aligned}\n\n$$\n\n> <span style=\"background-color: #ffb812; color: black;\">Corollary</span>. If $A$ is orthogonal, then $\\det(A) = \\pm 1$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. By definition, $AA^T = I$; as such $\\det (AA^T) = \\det A \\det(A^T) = \\det I = 1$. as $\\det A^T = \\det A$, we have $(\\det A)^2 = 1$ and $\\det A = \\pm 1$.\n\n## Minors and cofactors\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A minor is a minor scale based  on the notes A, B, C, D, E, F, and G. Its key signature has no flats or sharps; it is often regarded as the simplest minor key, and as such many cornerstones of Western classical music [1] have been composed in this key, such as Dr. Dre's \"Still D.R.E.\" and so on \n\nSorry, my hand slipped. Let's try that again:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define a **minor matrix** $A^{ij}$ of a square matrix $A$ to be the matrix composed of every element except those in the $i$th row and $j$th column. Accordingly, define the **minor** $M_{ij}$ as the determinant of the matrix: $M_{ij} = \\det A^{ij}$.\n\nEssentially, to get the minor of an element in the matrix, drive a lawnmower armed with a bazooka straight through it and fire the bazooka upwards. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **cofactor** of element $A_{ij}$, denoted $\\Delta_{ij}$, as $(-1)^{i-j}M_{ij} = (-1)^{i-j}\\det A^{ij}$.\n\nThe introduction of minors and cofactors allows us to obtain a far more articulate expression for the determinant. \n\n> <span style=\"background-color: #bc42f5; color: black;\">Notation</span>. If we ever need to emphasize that a number is missing from a sequence of numbers - e.g. $j$ is missing from $1, ..., n$, or $a_i$ is missing from $a_1, ..., a_n$ - we write these sequences as $1, ..., \\bar{j}, ..., n$ and $a_1, ..., \\bar{a_i}, ..., a_n$ respectively. In general, a bar over something, e.g. $\\bar{A}_{ij}$, indicates that it is missing rather than present within a sequence.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. $\\epsilon_{j_1 j_2 ... j_I ... j_n} = (-1)^{I - j_I} \\epsilon_{j_1 j_2 ... \\bar{j_I} ... j_n}$. In accordance with the above notation, the second sequence is the first sequence with the term $j_I \\in \\{1,2,...,n\\}$ removed.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. To get from $\\{1,2,...,n\\}$ to $\\{j_1,...,j_I,...j_n\\}$ (with $j_I$ included), we need to do two things:\n1. Get $j_I$ to the $I$th position, with the order of all other elements $\\{1,2,...,\\bar{j_I},...,n\\}$ unchanged. Let this permutation be represented by $\\sigma$.\n2. With $j_I$ fixed, reorder the other $n-1$ elements $\\{1,2,...,\\bar{j_I},...,n\\}$ (without $j_I$, this time) to correspond with $\\{j_1,...,\\bar{j_I}...,j_n\\}$. Let this permutation be represented by $\\rho$.<br/><br/>\nLet's deal with $\\sigma$ first. To get $j_I$ to the $I$th position without disturbing the order of the other elements, we will need to a series of swaps. <br/><br/>\nIf $j_I > I$ - i.e. $j_I$ needs to reach a lower position, we first swap $j_I$ with $j_I - 1$, then $j_I$ (now in the position of $j_I - 1$) with $j_I - 2$, ..., until $j_I$ and $I$ swap places:<br/><br/>\n![\n](d4c4027ef73c928b12036f7bcfc3fd8.jpg)<br/><br/>\nIf $j_I < I$, we will need to swap upwards instead in the same way:<br/><br/>\n![alt text](3fe6e60ebf3c6b6d452563010780a82.jpg)<br/><br/>\nAs such, $\\sigma$ requires a total of $I - j_I$ swaps and has sign $(-)^{I-j_I}$. <br/><br/>\n$\\rho$ is the permutation that reorders the other $n-1$ elements (without $j_I$) to $j_1, ..., \\bar{j_I}, ..., j_n$. As such, its sign is $\\epsilon_{j_1j_2...\\bar{j_I}...j_n}$. Thus, the sign of $\\sigma \\rho$, which permutes $\\{1,2,...,n\\}$ to $\\{j_1,...,j_n\\}$ is $\\epsilon_{j_1j_2...j_n}=\\epsilon(\\sigma)\\epsilon(\\rho)=(-1)^{I-j_{I}}\\epsilon_{j_1j_2...\\bar{j_I}...j_n}$.\n\nThis lemma allows us to show the following:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem.</span>. **Laplace expansion formulae**. $\\det A = \\sum_{k=1}^{n}A_{1k}\\Delta_{1k}$ for a $n\\times n$ square matrix $A$, where $\\Delta$ denotes the cofactor. This is also valid for $A_{2k}\\Delta_{2k}$, ..., $A_{nk}\\Delta_{nk}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n$$\n\\begin{aligned}\n\\det A &= \\sum_{j_1,j_2,...,j_n}\\epsilon_{j_1j_2...j_n}A_{1j_1}A_{2j_2}...A_{nj_n} \\\\\n&=\\sum_{j_I = 1}^{n} A_{I j_I} \\sum_{j_1, j_2, ..., \\bar{j_I}, ..., j_n} (-1)^{I-j_I}\\epsilon_{j_1j_2...\\bar{j_I}...j_n}A_{1j_1}A_{2j_2}...\\bar{A}_{I j_I}A_{nj_n} \\\\\n&\\text{using the above lemma} \\\\\n&=  \\sum_{j_I = 1}^{n} A_{I j_I} (-1)^{I-j_I} M_{Ij_I}\n\\end{aligned}\n$$\n\n> as the second sum represents a determinant of the matrix with row $I$ and column $j_I$ removed. Note that the proof is equivalent for columns, and leads to the conclusion that you can use any column's cofactors to find the determinant, not just any row.\n\nThis is usually the method to use for calculating determinants in practice, not summing over every possible permutation of $\\{1,...,n\\}$; it helps to choose a row or column with as many zeros as possible to simplify calculations. Ideally, the row or column we choose has only one non-zero entry. Indeed, we can manipulate affairs to achieve that goal, using the following properties demonstrated previously:\n1. If you add a scalar multiple of one row to another row, the determinant is unchanged.\n2. If you swap two rows, the determinant is multiplied by a negative sign.\n\nAs such, we want to leave one row or column to have only one nonzero entry using the above two operations. You can pick any row or column you want; pick the 86th column or the 20th row for all I care, but honestly, the only row you'll be headed towards is death row if you commit such an irredeemably felony. We will pick the first column and nothing else; our goal is to leave the very first entry $A_{11}$ nonzero and all other entries $A_{j1}$ zero. To make $A_{21}$ zero, subtract $\\frac{A_{21}}{A_{11}}$ times the first row from the second; to make $A_{31}$ zero, do the same for $A_{31}$; and so on, until all other first entries of each row are zero. The determinant is simply the minor of $A_{11}$; repeat this entire process until a single number is left behind.\n\n","n":0.029}}},{"i":68,"$":{"0":{"v":"Defining Linear Maps","n":0.577},"1":{"v":"\n## Matrices\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **matrix** is an array of elements; if $A$ is a $n \\times m$ matrix for positive integers $n$, $m$, it has $n$ rows and $m$ columns. The notation $A_{ij}$ denotes the element of the matrix in the $i$th row and $j$th column; occasionally we write $A = {A_{ij}}$.\n\n\n$$\n\nA=\\begin{bmatrix}\nA_{11} & A_{12} & ... & A_{1m} \\\\\nA_{21} & A_{22} & ... & A_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{n1} & A_{n2} & ... & A_{nm}\n\\end{bmatrix}\n$$\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define **scalar multiplication** of matrices as follows, where $\\lambda \\in R$ and $A$ is a matrix: $\\lambda A$ is simply $\\{\\lambda A_{ij}\\}$, or\n\n$$\n\n\\lambda A=\\begin{bmatrix}\n\\lambda A_{11} & \\lambda A_{12} & ... & \\lambda A_{1m} \\\\\n\\lambda A_{21} & \\lambda A_{22} & ... & \\lambda A_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\lambda A_{n1} & \\lambda A_{n2} & ... & \\lambda A_{nm}\n\\end{bmatrix}\n$$\n\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define **matrix multiplication** as follows:\n1. Matrix multiplication is only valid between a $n \\times m$ matrix and a $m \\times p$ matrix for any positive integers $n$, $m$, $p$: the number of columns of the first matrix must match the number of rows of the second.\n2. Let $A$ be a $n\\times m$ matrix and $B$ be $m \\times p$; write $C$ as the matrix resulting from the multiplication $AB$. Then $C$ has dimensions $n \\times p$, and $C_{ij} = \\sum_{k=1}^{m} A_{ik}B_{kj}$. Alternatively, it is sum of the product of the elements of the $i$th row of $A$ and the $j$th column of $B$ (which have the same length, as we insisted).\n\nNote that matrix multiplication often does not satisfy commutativity: $AB \\neq BA$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define **matrix addition** between two matrices $A$ and $B$ **of the same size** as the matrix formed out of the sums of their elements:\n$$\n(A+B)_{ij} = A_{ij}+B_{ij}\n$$\n\n## Linear maps\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Let $A$, $B$ be sets. A **map** $T$ (not necessarily linear) from $A$ to $B$ assigns each element $x \\in A$ a unique $x' \\in B$. We write\n$$\nT: A \\to B \\text{ and/or } x\\to x' = T(x)\n$$\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. $A$ is the **domain** of $T$, $B$ the **range** (codomain), $T(x)=x'$ the **image** of $x$ under $T$, and $A \\to T(A)$ is the **image of $A$** under $T$ (i.e. the entire set $A$ transformed to $T(A)$). \n\nWhile it is true that $T(A)$ is a subset of $B$, $B$ may have elements that are not in $T(A)$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. (**Linear maps**). Let $V$ and $W$ be real vector spaces ($\\mathbb{R^n}$ and $\\mathbb{R^m}$ respectively). $T$ is a **linear map** from $V$ to $W$ if it satisfies the following properties:\n1. $T(\\mathbf{a+b}) = T(\\mathbf{a}) + T(\\mathbf{b})$, for vectors $\\mathbf{a, b}\\in \\mathbb{R^n}$. (Linearity)\n2. $T(\\lambda \\mathbf{a})=\\lambda T(\\mathbf{a})$ for a scalar $\\lambda \\in \\mathbb{R}$. (Scalar multiplicity)\n\nCombined together, we have $T(\\lambda \\mathbf{a} + \\mu \\mathbf{b})=\\lambda T(\\mathbf{a})+ \\mu T(\\mathbf{b})$: $T$ can be distributed across a linear combination.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. $T(V)$ is a subspace of $W$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. By the above properties, $T(V)$ is a subset of $W$ and is closed under any linear combinations. Thus it is a subspace by definition.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. For **any** linear map $T$, $T(\\mathbf{0}) = 0$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. We do a little functional manipulation: (that's what my therapist used to say to me)\n$$\n\\begin{aligned}\nT(\\mathbf{a + 0}) &= T(\\mathbf{a}) + T(\\mathbf{0}) \\text{ by the above properties} \\\\\nT(\\mathbf{a}) &= T(\\mathbf{a}) + T(\\mathbf{0}) \\\\\nT(\\mathbf{0}) &= 0\n\\end{aligned}\n$$\nHowever, it is not necessarily true that if $T(\\mathbf{b}) = 0$, then $\\mathbf{b} = 0$.\n\n> Insert random linear map joke here \n","n":0.041}}},{"i":69,"$":{"0":{"v":"Eigenvalues and Eigenvectors","n":0.577},"1":{"v":"who tf is victor eigen and why is he obsessed with lambs","n":0.289}}},{"i":70,"$":{"0":{"v":"Singular Value Decomposition","n":0.577},"1":{"v":"## Defining SVD\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **singular value decomposition** is a factorization of a real or complex $m\\times n$ matrix $A$, of the form\n$$\nA = U \\Sigma V^{\\dagger}\n$$\n> where $U$ is a $m\\times m$ unitary matrix, $\\Sigma$ is a $m\\times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal, and $V$ is a $n\\times n$ unitary matrix.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. In the above factorization, we typically denote the diagonal terms of $\\Sigma$, $\\sigma_1, \\sigma_2, ..., \\sigma_n$, as the **singular values** of $A$ (usually reordered so that $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_n$).\n\nThis can be thought of as a generalization of the diagonalization\n$$\nA = P\\Lambda P^{-1}\n$$\napplicable only for square matrices that have $n$ linearly independent eigenvectors.\n\n## Constructing a possible SVD\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Every real or complex $m\\times n$ matrix $A$ has a singular value decomposition.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. We prove the above statement by literally willing the factorization into existence through the power of ~~God and anime~~ Hermitian matrices on our side. This is considered to be a \"sigma move\", and - this is true - is the reason why $\\Sigma$ is the letter chosen for SVD.\n\nBegin by defining\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A square matrix $H$ is **positive semi-definite** if it has non-negative eigenvalues; a matrix $D$ is **positive definite** if it has strictly positive eigenvalues.\n\n> <span style=\"background-color: #1eff12; color: black;\">Sketch of proof</span>.  \n1. Construct a Hermitian matrix $H = A^{\\dagger}A$ and prove it is positive semi-definite using the Hermitian form. \n2. Denote the eigenvalues of $H$, all non-negative by virtue of semi-definiteness, as $\\lambda_1, ..., \\lambda_n$, with corresponding eigenvectors $\\mathbf{v_1, v_2, ..., v_n}$.\n2. Construct another Hermitian matrix $H' = AA^{\\dagger}$. Show that $A\\mathbf{v_i}$ are eigenvectors of $H'$, and that $H'$ has the same nonzero eigenvalues as $H$.\n3. Using the fact that transformation matrices from the standard orthonormal basis to the orthonormal eigenvectors of a Hermitian matrix are unitary, construct the SVD.\n\n**Part 1: positive semi-definiteness of $H$**.\nLet $A$ be a real or complex $m\\times n$ matrix, and let $H$ be the $n\\times n$ Hermitian matrix\n$$\nH = A^{\\dagger}A\\ (=(A^{\\dagger}A)^{\\dagger})\n$$\nwhich, as previously shown, is diagonalizable via the unitary matrix $V$ as\n$$\nV^{\\dagger}H V = \\Lambda\n$$\nwhere $\\Lambda$ is the diagonal matrix of eigenvalues of $H$. Now consider the Hermitian form\n$$\n\\mathbf{x}^\\dagger H \\mathbf{x} \n$$\nfor some $n\\times 1$ vector $\\mathbf{x}$. As we have\n$$\n\\mathbf{x}^\\dagger H \\mathbf{x} = \\mathbf{x}^\\dagger A^{\\dagger}A \\mathbf{x} = (A\\mathbf{x})^\\dagger (A\\mathbf{x}) = |A\\mathbf{x}|\n$$\nwith $A\\mathbf{x}$ being a vector, this form is always non-negative by virtue of norms being non-negative. We know that Hermitian forms can be simplified onto the principal axes of the eigenvectors via\n$$\n\\mathbf{x}^\\dagger H \\mathbf{x} = \\mathbf{x}^\\dagger V \\Lambda V^{\\dagger}\\mathbf{x} = (V^{\\dagger}\\mathbf{x})^{\\dagger}\\Lambda(V^{\\dagger}\\mathbf{x}) = (\\mathbf{x}')^{\\dagger}\\Lambda\\mathbf{x'}\n$$\nwhich involves only the sum of diagonal terms\n$$\n\\sum_{i=1}^n \\lambda_i |\\mathbf{x}'_i|^2.\n$$\nAs shown above, this is non-negative; $\\mathbf{x}$ can be any vector, so we conclude that the $\\lambda_i$ are non-negative. Denote the corresponding values to $\\lambda_i$, reordered with $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n$, as $\\mathbf{v_i}$; supposes also that, after reordering, only the first $r$ eigenvalues are strictly positive ($\\lambda_1, ..., \\lambda_r$, with the other $n-r$ eigenvalues being zero).\n\nNote here that the eigenvectors $\\mathbf{v_i}$ form an orthonormal set because they are the columns of the unitary matrix $V$; thus, each $\\mathbf{v_i}$ is a** unit vector.**\n\n**Part 2: constructing $H'$**. Let $\\mathbf{v_i}$ be the eigenvectors of $H$ as above, with \n$$\nH\\mathbf{v_i} = A^{\\dagger}A\\mathbf{v_i} = \\lambda_i \\mathbf{v_i}.\n$$\nDefine\n$$\n\\mathbf{u_i} = A\\mathbf{v_i},\n$$\nwhich is an eigenvector of $H' = AA^{\\dagger}$, also Hermitian, as\n$$\nAA^{\\dagger}(A\\mathbf{v_i}) = A(A^{\\dagger}A\\mathbf{v_i})=A(H\\mathbf{v_i}) = A(\\lambda_i \\mathbf{v_i}) =\\lambda_i (A\\mathbf{v_i}) = \\lambda_i \\mathbf{u_i},\n$$\nleading to the conclusion that if $\\lambda_i$ is an eigenvalue of $H$, then it is an eigenvalue of $H'$. \n\n**Part 3: constructing the SVD**. In order to make $\\mathbf{u_i}$ orthonormal to construct the unitary matrix $U$ containing the $\\mathbf{u_i}$ as columns (as any matrix with orthonormal columns is unitary), consider\n$$\n\\begin{aligned}\n\\mathbf{u_i}\\cdot \\mathbf{u_i}&= \\mathbf{u_i}^{\\dagger}\\mathbf{u_i} \\\\\n&= \\mathbf{v_i}^{\\dagger}A^{\\dagger}A\\mathbf{v_i} \\\\\n&=\\mathbf{v_i}^{\\dagger}H\\mathbf{v_i} \\\\\n&=\\lambda_i\\mathbf{v_i}^{\\dagger}\\mathbf{v_i} \\\\\n&= \\lambda_i\n\\end{aligned}\n$$\nTherefore, we need to divide each $\\mathbf{u}_i$ by $\\sqrt{\\lambda_i}$. Call these values $\\sigma_i = \\sqrt{\\lambda_i}$. Thus,\n$$\n\\mathbf{u_i} = \\frac{A\\mathbf{v_i}}{\\sigma_i}\n$$\nform a unitary matrix $U$. The above equation in matrix form can be expressed as\n$$\nU = AV\\Sigma^{-1}\n$$\nwhere $\\Sigma$ is the diagonal matrix of $\\sigma_i$s, which leads to\n$$\nA = U \\Sigma V^{-1}.\n$$\n\n> What if we have some eigenvalues equal to zero?\n\nIn that case, we will no longer be able to find\n$$\n\\mathbf{u_i} = \\frac{A\\mathbf{v_i}}{\\sigma_i}\n$$\nfor $\\sigma_i = \\sqrt{\\lambda_i} = 0$. However, we can implement a correction: first sort the $\\sigma_i$ in descending order as $\\lambda_i \\geq 0$, owing to $H$ being positive semi-definite. Suppose that $\\sigma_1, \\sigma_2, ..., \\sigma_r$ are strictly positive after sorting, and that $\\sigma_{r+1}, ..., \\sigma_n$ are all zero. Obtain $\\mathbf{u_1, u_2, ..., u_r}$ as above. By the Gram-Schmidt process, it is always possible to find $\\mathbf{w_{1}, ..., w_{n-r}}$ such that\n$$\n\\{\\mathbf{u_1, u_2, ..., u_r, w_1, ..., w_{n-r}}\\}\n$$\nform an orthonormal set, and thus that the matrix $U$ with columns equal to these vectors is unitary. Accordingly, we can no longer write\n$$\nU = AV\\Sigma^{-1}\n$$\nwhere $\\Sigma$ is a square matrix with the singular values $\\sigma_i$ on its diagonal, as $\\sigma_{r+1}, ..., \\sigma_n$ are zero and make $\\Sigma$ non-invertible; instead, we write directly\n$$\nU\\Sigma = Av\n$$\nwhere $\\Sigma$ has rows equal to zero corresponding to the columns $\\mathbf{w_1}, ..., \\mathbf{w_{n-r}}$ of $U$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The $m$ columns of $U$ are referred to as the **left-singular vectors** of $A$; the $n$ columns of $V$ are referred to as the **right-singular vectors**.\n\n## Interpretations of the SVD\n\n> <span style=\"background-color: #03cafc; color: black;\">Geometric interpretation</span>.\n\nThe SVD decomposes any $m\\times n$ matrix $A$, itself representing a linear map from $\\mathbb{R^n \\to R^m}$, into\n$$\nA = U\\Sigma V^{-1}\n$$\nor three linear maps $V^{-1}, \\Sigma$, and $U$ applied in that order, where $\\Sigma$ is diagonal and $U$ and $V$ are unitary. Unitary matrices (and their inverses) map one orthonormal basis to another, and thus they can be thought of as **rotations** or **reflections**: they \"rotate\" or \"reflect\" the original basis, sometimes in combination, to form the new one. Thus, the SVD tells us that any linear map $A: \\mathbb{R^n \\to R^m}$ is equivalent to three maps applied in sequence:\n1. The rotation/reflection matrix $V^{-1}$.\n2. The enlargement matrix $\\Sigma$, which enlarges the vector in some directions by a scalar multiple and may also make the other directions disappear (as $\\Sigma$ maps from $\\mathbb{R^n \\to R^m}$.)\n3. The rotation/reflection matrix $U$.\n\nNote that the notion of a \"rotation/reflection\" only really makes sense if $U$ and $V$ are real, i.e. they are orthogonal instead of unitary.\n\n> <span style=\"background-color: #03cafc; color: black;\">Series interpretation.</span>\n\nThe SVD can be thought of as a matrix analogue of functional series like the Taylor or Fourier series. To observe this, note that\n$$\nA = \\begin{bmatrix}\n\\mathbf{u_1} & \\mathbf{u_2} & ... & \\mathbf{u_m} \\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1 & 0 & 0 & ... \\\\\n0 & \\sigma_2 & 0 & ... \\\\\n0 & 0 & \\sigma_3 & ... \\\\\n\\vdots & \\vdots & \\vdots & \\ddots \\\\\n0 & 0 & 0 & \\dots\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{v_1^\\dagger} \\\\\n\\mathbf{v_2^\\dagger} \\\\\n\\vdots \\\\\n\\mathbf{v_n^\\dagger}\n\\end{bmatrix}\n$$\nWhich leads to\n$$\nA = \\begin{bmatrix}\n\\mathbf{u_1} & \\mathbf{u_2} & ... & \\mathbf{u_m} \\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_1\\mathbf{v_1^\\dagger} \\\\\n\\sigma_2\\mathbf{v_2^\\dagger} \\\\\n\\vdots \\\\\n\\end{bmatrix}\n$$\nand\n$$\nA = \\sum_{i=1}^m \\sigma_i\\mathbf{u_i}\\mathbf{v_i}^{\\dagger}\n$$\neach of which are simpler $m\\times n$ matrices, with $\\mathbf{u_i}$ being $m \\times 1$ and $\\mathbf{v_i}^\\dagger$ being $1 \\times n$. This is useful in contexts such as image decomposition, where a large image stored as a matrix of pixels can be decomposed into simpler incomplete images.\n\n## Applications of SVD\n\n> <span style=\"background-color: #bc42f5; color: black;\">Application</span>. **Linear least-squares**.\n\nSuppose we are trying to fit a multivariate linear regression model to data: for some input in $n$ variables\n$$\n\\mathbf{x} = \\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix},\n$$\nand some output we want in $m$ variables (most commonly just a single variable)\n$$\n\\mathbf{y} = \\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m\n\\end{bmatrix},\n$$\ncan we find a $m \\times n$ matrix $W$ such that the systems of linear equations\n$$\nW\\mathbf{x} = \\begin{bmatrix}\nW_{11}x_1 + W_{12}x_2 + ... + W_{1n}x_n \\\\\nW_{21}x_1 + W_{22}x_2 + ... + W_{2n}x_n \\\\\n\\vdots \\\\\nW_{n1}x_1 + W_{n2}x_2 + ... + W_{nn}x_n\n\\end{bmatrix}\n$$\nis as close as possible to $\\mathbf{y}$, e.g. it minimizes the Euclidean distance $||W\\mathbf{x-y}||$ (just one possible metric)?\n\nThis is the problem of linear least-squares; if solved correctly, it gives us the **line of best fit** for a set of data, perfect for all your linear regression needs such as predicting the Earth's climate in 2050, mapping your level of respect in me as a human being over time, and forecasting how many chicks you'll pull per year in the next decade (perfectly modeled by a flat line). It is slightly less perfect for modelling needs such as trying to \"conquer the stock market\" (sorry, Chad, I don't think you're going to become the next Warren Buffett using Excel), tracking my erotic fascination towards Danny DeVito over time (exponential growth), or calculating the gravitational pull between me and your** mom.**\n\nIt turns out that the linear least-squares problem is solved using SVD. (It's also solved using multivariable calculus, but the symbol $\\nabla$ is an affront to all that is good and holy.) If we write\n$$\nW\\mathbf{x} - \\mathbf{y} = U\\Sigma V^{-1}\\mathbf{x - y}\n$$\nusing SVD on $W$, we have\n$$\nU\\Sigma V^{-1}\\mathbf{x - y} = U\\Sigma (V^{-1}\\mathbf{x}) - UU^{\\dagger}\\mathbf{y} = U(\\Sigma\\mathbf{z} - \\mathbf{d}),\n$$\nwhere $UU^{\\dagger} = I$ by definition of a unitary matrix and $\\mathbf{z} = V^{-1} \\mathbf{x}, \\mathbf{d} = U^{-1}\\mathbf{y}$. Taking the Euclidean norm gives\n$$\n||W\\mathbf{x-y}|| = (U(\\Sigma\\mathbf{z} - \\mathbf{d}))^{\\dagger}U(\\Sigma\\mathbf{z} - \\mathbf{d}) = (\\mathbf{z}^{\\dagger}\\Sigma^{\\dagger}-\\mathbf{d}^{\\dagger})U^{\\dagger}U(\\Sigma\\mathbf{{z-d}})\n$$\nwhere $U^{\\dagger}U = I$ and disappears, so the above simply equals\n$$\n\\begin{aligned}\n(\\mathbf{z}^{\\dagger}\\Sigma^{\\dagger}-\\mathbf{d}^{\\dagger})(\\Sigma\\mathbf{{z-d}}) \\\\\n= \\sum_{i=1}^{m} (\\Sigma \\mathbf{z} - \\mathbf{d})^2_i\n\n\\end{aligned}\n$$\nSupposing there are $r$ nonzero singular values $\\sigma_1, \\sigma_2, ..., \\sigma_r$, for these $r$ values we have \n$$\n\\sum_{i=1}^r (\\sigma_{i}z_{i}-d_i)^2\n$$\nand for the remaining $m-r$ values, we have\n$$\n\\sum_{i=r+1}^{m}d_i^2.\n$$\nBoth terms are sums of squares and are always non-negative, and so\n$$\n||W\\mathbf{x-y}|| \\geq \\sum_{i=r+1}^{m}d_i^2\n$$\nwhere $\\mathbf{d}$ is a constant, with equality holding if and only if\n$$\nz_i = \\frac{d_i}{\\sigma_i}\n$$\nfor $i= 1,...,r$.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Application</span>. **Pseudoinverse**.\n","n":0.025}}},{"i":71,"$":{"0":{"v":"Similar Matrices and Diagonalization","n":0.5},"1":{"v":"## Definitions and Basic Properties\nRecall the transformation laws from the previous section. We define\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. If $n\\times n$ matrices $A$ and $B$ are identical under a change of basis, as detailed previously by\n$$\nA = P^{-1} B P\n$$\n> for some invertible $n\\times n$ matrix $P$, then $A$ and $B$ are **similar** or **conjugate** matrices.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. As $P^{-1} I P = I$, the only matrix the identity matrix is similar to is itself.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. Similar matrices have the same determinant and trace.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \nLet $A' = P^{-1} A P$ for similar square matrices $A, A'$ and invertible square matrix $P$. Our goal is to prove that $\\det A' = \\det A$ and $Tr(A') = Tr(A)$. <br/><br/>\nFirst: \n$$\n\\begin{aligned}\n\\det A' &= \\det (P^{-1}A P) \\\\\n &= \\det P^{-1} \\det P \\det A \\\\\n &= \\det(P^{-1}P) \\det A \\\\\n &= \\det I \\det A \\\\\n &= \\det A\n\\end{aligned}\n$$\n> And also:\n$$\n\\begin{aligned}\nTr(A') &= Tr(P^{-1}AP) \\\\\n&= P^{-1}_{ik}A_{kj}P_{ji} \\\\\n&= P_{ji}P^{-1}_{ik}A_{kj} \\\\\n&= \\delta_{jk}A_{kj} \\\\\n&= A_{jj} \\\\\n&= Tr(A).\n\\end{aligned}\n$$\n> Geometrically speaking, the determinant of similar matrices being the same can be intuited by the fact that they both represent the same linear map under different bases (and thus have the same volume scaling effect).\n\nAs a linear map has the same trace and determinant no matter what basis its matrix is expressed in, we can speak of a **unique trace and determinant** for that map.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. If $u$ is an eigenvector of $A$, then $v = P^{-1} u$ is an eigenvector of $A'$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Suppose that $Au = \\lambda u$ for some scalar $\\lambda$. Then if\n$$\n\\begin{aligned}\nA'(P^{-1} u) = \\lambda(P^{-1} u)\n\\end{aligned}\n$$\n> we have\n$$\n\\begin{aligned}\nA'(P^{-1} u) &= \\lambda(P^{-1} u) \\\\\nP^{-1}A P^ (P^{-1} u) &= \\lambda (P^{-1} u) \\\\\nP^{-1} A u &= \\lambda P^{-1} u \\\\\nAu &= \\lambda u\n\\end{aligned}\n$$\n> which proves the statement as every step is reversible.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. Similar matrices have the same characteristic equation.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nSuppose that $A$ and $B$ are similar: $B = P^{-1} A P$ for some transformation matrix $P$. We thus have\n$$\n\\begin{aligned}\n\\det(B - \\lambda I) &= \\det(P^{-1} A P - \\lambda I) \\\\\n&= \\det{P^{-1}}\\det(AP - \\lambda P) \\\\\n&= \\det{P^{-1}}\\det(A-\\lambda I) \\det{P} \\\\\n&= \\det(PP^{-1}) \\det(A-\\lambda I) \\\\\n&= \\det(A-\\lambda I)\n\\end{aligned}\n$$\nwhich is the characteristic equation of $A$.\n\n## Diagonalization\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A matrix representing a linear map $A: \\mathbb{F^n} \\to \\mathbb{F^n}$ is **diagonalizable** if its eigenvectors span $\\mathbb{F^n}$.\n\nThis means that:\n1. The matrix has $n$ linearly independent eigenvectors;\n2. The matrix can be written in the form of a diagonal matrix under a change of basis (as the matrix representation of $A$ is diagonal if and only if the basis chosen are its eigenvectors)\n3. The matrix is similar to such a diagonal matrix.\n\nAccording to the above definition of similar matrices, we also have\n$$\nD = P^{-1}AP\n$$\nwhere $D$ is diagonal.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. If the $n\\times n$ matrix $A$ has $n$ distinct eigenvalues, then it must be diagonalizable, as their eigenvectors are distinct and thus linearly independent; however, this is not a necessary condition (i.e. some matrices have repeated eigenvalues but are still diagonalizable).\n\nThe above scenario can occur if one eigenvalue corresponds to more than one linearly independent eigenvector: i.e. the system of equations\n$$\n(A-\\lambda I)\\mathbf{x} = 0\n$$\nhas a solution space of $\\dim 2$. In other words, there are no defective eigenvalues. This will be clarified with the lemma below:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. Suppose that a linear map $A: \\mathbb{F^n} \\to \\mathbb{F^n}$ has eigenvalues $\\lambda_1, \\lambda_2, ..., \\lambda_r$, and let $B_1, B_2, ..., B_r$ be the bases of the eigenspaces $E_{\\lambda_1}, E_{\\lambda_2}, ..., E_{\\lambda_n}$ (in that order). Then the union of $B_1, ..., B_n$ form a linearly independent set.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nAttempt a proof by contradiction. Denote the $j$th basis vector of $B_i$ as $v_{ij}$; if the set of all such $v_{ij}$s are linearly dependent, then there exist scalars $c_{11}, ..., c_{ij}$ such that\n$$\n\\sum_{i=1}^{r} \\sum_{j=1}^{m_{\\lambda_i}} c_{ij}v_{ij} = 0.\n$$\nwhere $m_{\\lambda_i}$ denotes the geometric multiplicity of $\\lambda_i$. Utilizing the property that $Av_{ij} = \\lambda_{i} v_{ij}$, we proceed similarly to the proof for linear independence of eigenvectors by applying the operator\n$$\n(A-\\lambda_1 I) (A - \\lambda_2 I) ... (A-\\lambda_{K-1} I) (A - \\lambda_{K+1} I) ... (A - \\lambda_r I) \n$$\nequalling\n$$\n\\prod_{i=1,2,...,\\bar{K},...,r} (A-\\lambda_i I)\n$$\nto the above equation, which leads to\n$$\n\\begin{aligned}\n&\\prod_{i=1,2,...,\\bar{K},...,r}^{r} (A - \\lambda_i I)\\sum_{i=1}^{r} \\sum_{j=1}^{m_{\\lambda_i}} c_{ij}v_{ij} \\\\\n&=\\sum_{i=1}^{r} \\sum_{j=1}^{m_{\\lambda_i}} (\\prod_{i=1,2,...,\\bar{K},...,r}(\\lambda_i - \\lambda_k)) c_{ij}v_{ij} \\\\\n\n\\end{aligned}\n$$\nwhere the product is zero for every term except $i = K$, leading to the above sum equalling\n$$\n\\sum_{j=1}^{m_{\\lambda_K}} \\prod_{i=1,2,...,\\bar{K},...,r}(\\lambda_K - \\lambda_k) c_{Kj}v_{Kj}\n$$\nwhich is zero. This reaches a contradiction, as the vectors $v_{K1}, v_{K2}, ... v_{Km_{\\lambda_{k}}}$ are a basis for $E_{\\lambda_{K}}$ and are linearly independent by definition. $\\square$\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. A consequence of the above lemma concerns defective and non-defective eigenvalues. The lemma essentially states that the eigenspaces of any linear map are all linearly independent; as such, if no eigenvalue is defective - i.e. if the geometric multiplicities sum to $n$ - then $n$ linearly independent eigenvectors can be found, and the map is diagonalizable. If, however, defective eigenvalues do exist, the map is not diagonalizable.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Diagonalization matrix for linear maps**. The transformation matrix $P$ that maps the matrix $A$, in the standard orthonormal basis of $\\mathbb{F}^n$, to the diagonalized basis of $n$ distinct eigenvectors of $A$, $\\mathbf{x_1}, \\mathbf{x_2}, ..., \\mathbf{x_n}$ is \n$$\nP = \\begin{bmatrix}\n\\mathbf{x}_1 & \\mathbf{x}_2 & ... & \\mathbf{x}_n\n\\end{bmatrix}\n$$\n> where $D = P^{-1} A P$ results in the diagonalized matrix\n$$\nD = \\begin{bmatrix}\n\\lambda_1 & 0 & ... & 0 \\\\\n0 & \\lambda_2 & ... & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots \\\\\n0 & 0 & ... & \\lambda_n\n\\end{bmatrix}\n$$\n> where $\\lambda_1, ..., \\lambda_n$ correspond to the eigenvectors $\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_n$.\n\nThis gives us a general process for determining whether a matrix is diagonalizable, as well as determining what that diagonalization is:\n\n1. Find the characteristic equation of the matrix and determine its roots,\n2. For each root $\\lambda_i$, check the system of homogeneous linear equations\n$$\n(A-\\lambda_i I)\\mathbf{x} = \\mathbf{0}\n$$\nand the dimension of its solution space (which is the dimension of the eigenspace).\n3. Check if the dimension of the solution space (the geometric multiplicity) is equal to the algebraic multiplicity for every $\\lambda_i$ - i.e. check if there are defective eigenvalues.\n4. If there are no defective eigenvalues, find $n$ linearly independent eigenvectors from the above systems of equations and form $P$ as shown above.\n\n","n":0.031}}},{"i":72,"$":{"0":{"v":"Forms","n":1},"1":{"v":"## What are forms?\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **form** $F(\\mathbf{x})$, with associated **coefficient matrix** $A$ (or $\\mathcal{F}(\\mathbf{x})$, which has a funny squiggly little tail and is ten times more of a pain in my ass when trying to type this up in LaTeX), is defined as the linear map\n$$\nF(\\mathbf{x}) = \\mathbf{x}^{\\dagger} A \\mathbf{x} = \\sum_{i=1}^{n}\\sum_{j=1}^n x^{*}_i A_{ij}x_j\n$$\n> which is the complex scalar product between $\\mathbf{x}$ and the image of $\\mathbf{x}$ under $A$, $A\\mathbf{x}$. The map $F$ maps the $n\\times 1$ vector $\\mathbf{x}$ to a complex scalar in $\\mathbb{C}$: $\\mathbb{C^n \\to C}$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. If the coefficient matrix of a form $F$ is Hermitian, $F$ is called a **Hermitian form**.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. Hermitian forms have real outputs; they map to $\\mathbb{R}$ instead of $\\mathbb{C}$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nConsider the form $F(\\mathbf{x})$ with Hermitian coefficient matrix $H$. We have\n$$\nF(\\mathbf{x}) = \\mathbf{x}^{\\dagger} H \\mathbf{x}\n$$\nwhich, in order to prove is real, requires\n$$\nF(\\mathbf{x})^{\\dagger} = F(\\mathbf{x})^{*} = F(\\mathbf{x})\n$$\nas the complex conjugate of a scalar is itself if and only if the scalar is real. This implies that\n$$\n(\\mathbf{x}^{\\dagger} H \\mathbf{x})^\\dagger = \\mathbf{x}^{\\dagger} H^\\dagger \\mathbf{x} = \\mathbf{x}^{\\dagger} H \\mathbf{x}\n$$\nwhich is true as $H^{\\dagger} = H$ by property of a Hermitian matrix.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. If the coefficient matrix of a form $F$ is a real symmetric matrix, $F$ is called a **quadratic form**.\n\n## Principle axes and simplifying forms\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Any Hermitian form \n$$\nF(\\mathbf{x}) = \\mathbf{x}^{\\dagger} H \\mathbf{x} = \\sum_{i=1}^{n}\\sum_{j=1}^n x^{*}_i H_{ij}x_j\n$$ \n> can be simplified into a form whose coefficient matrix is diagonal:\n$$\nF'(\\mathbf{x'})=(\\mathbf{x'})^{\\dagger}\\Lambda \\mathbf{x'} = \\sum_{i=1}^n |x_i|^2 \\Lambda_{ii}.\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThe above simplification follows from the fact that all Hermitian matrices are diagonalizable by unitary matrices. Let $H$ be the Hermitian matrix which is the coefficient matrix of the form $F(\\mathbf{x})$, with\n$$\nH = U\\Lambda U^{-1} = U\\Lambda U^{\\dagger}.\n$$\nwhere $\\Lambda$ is the matrix of eigenvalues of $H$. Define $\\mathbf{x}' = U^{\\dagger} \\mathbf{x}$. Thus we have\n$$\n\\begin{aligned}\nF(\\mathbf{x}) &= \\mathbf{x}^{\\dagger} H \\mathbf{x} \\\\\n&= \\mathbf{x}^{\\dagger} (U\\Lambda U^{\\dagger}) \\mathbf{x} \\\\\n&= ( \\mathbf{x}^{\\dagger} U)\\Lambda (U^{\\dagger} \\mathbf{x}) \\\\\n&=(U^{\\dagger} \\mathbf{x})^{\\dagger} \\Lambda (U^{\\dagger} \\mathbf{x}) \\\\\n&= \\mathbf{(x')}^{\\dagger} \\Lambda\\mathbf{x'}\n\\end{aligned}\n$$\nwhich corresponds to the norm $F'(\\mathbf{x}')$ with corresponding coefficient matrix $\\Lambda$, which is a diagonal matrix with diagonal entries equal to the eigenvalues $\\lambda_1, \\lambda_2, ..., \\lambda_n$ of $H$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. For a form $F$ with Hermitian coefficient matrix $H$, define the orthonormal basis eigenvectors corresponding to eigenvalues $\\lambda_1, ..., \\lambda_n$ as the **principal axes** of $F$.\n\n## Applications of forms\n\n> <span style=\"background-color: #bc42f5; color: black;\">Application</span>. **Hessian matrix.** (Time to bust out the purple.)\n\nRecall from **Differential Equations** that the **Hessian matrix** for a multivariate function $f(x_1,x_2,...,x_n),\\ \\mathbb{R^n \\to R},$ of $n$ variables was defined\n$$\nH_{ij} = \\frac{\\partial f}{\\partial x_i x_j}\n$$\nwith the matrix-valued function $H(\\mathbf{a})$ for some $\\mathbf{a}\\in\\mathbb{R}^n$ defined as \n$$\nH_{ij}(\\mathbf{a}) = \\frac{\\partial f}{\\partial x_i x_j}(\\mathbf{a}).\n$$\nIf $f$ has continuous second-order derivatives, recall that the mixed second-order derivatives commute, i.e.\n$$\nH_{ij} = \\frac{\\partial f}{\\partial x_i x_j} = \\frac{\\partial f}{\\partial x_j x_i} = H_{ji}\n$$\nmeaning that $H$ is symmetric. Suppose that $f(\\mathbf{a})$ is a critical point of $f$, $\\mathbf{a}\\in\\mathbb{R}^n$: $\\frac{\\partial f}{\\partial x_i} (\\mathbf{a}) = 0$ for all $i= 1, 2, ..., n$. To analyze whether $\\mathbf{a}$ is a maximum, a minimum, or a saddle point, we consider a small perturbation $\\mathbf{a + x}$ near $\\mathbf{a}$ with $|\\mathbf{x}|<<1$. By Taylor's theorem, we have\n$$\nf(\\mathbf{a}+ \\mathbf{x}) = f(\\mathbf{a}) + \\sum_{i=1}^n \\mathbf{x}_i \\frac{\\partial f}{\\partial x_i} (\\mathbf{a}) +\\frac{1}{2} \\sum_{i=1}^n\\sum_{j=1}^n \\mathbf{x}_i \\frac{\\partial^2 f}{\\partial x_i x_j} (\\mathbf{a})\\mathbf{x}_j\n$$\nplus terms with powers of $\\mathbf{x}$ above $2$. The term containing the first derivatives are zero as the first derivatives themselves are zero. The second-derivative term can be rewritten in matrix form as\n$$\n\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\mathbf{x}_i H_{ij}(\\mathbf{a})\\mathbf{x}_j = \\frac{1}{2}\\mathbf{x}^{T} H(\\mathbf{a})\\mathbf{x}\n$$\nThis is a form with a real symmetric associated coefficient matrix, and as shown above, it can be simplified to the form\n$$\n(\\mathbf{x}')^T \\Lambda \\mathbf{x}\n$$\nwhich allows us to write\n$$\nf(\\mathbf{a+x})-f(\\mathbf{a})\\approx \\frac{1}{2}(\\mathbf{x}')^T \\Lambda \\mathbf{x} = \\sum_{i=1}^n \\lambda_i(x_i)^2,\n$$\nwhich is positive if all the $\\lambda_i$ are positive, negative if all the $\\lambda_i$ are negative, and indeterminate if there is at least one positive and one negative eigenvalue. These cases correspond to a minimum point (surrounding point $f(\\mathbf{a+x})$ larger than $f(\\mathbf{a})$), a maximum point (surrounding point $f(\\mathbf{a+x})$ smaller than $f(\\mathbf{a})$), and a saddle point respectively.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Application</span>.  **Quadric surfaces.**\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **quadric surface** in $n$-dimensions is a defined by the zeroes of a real quadratic polynomial in $n$ variables:\n\n$$\n\\sum_{i,j=1}^n A_{ij}x_i x_j + \\sum_{i=1}^n \\mathbf{b}_i x_i + c = 0\n$$\n> where $\\mathbf{b}$ is a $n\\times 1$ real vector and $A$ is a $n\\times n$ real matrix. This represents a general quadratic polynomial as it encompasses the second-order terms $(x_1x_j)$, the first-order terms $(x_i)$, and the constant term $c$. In matrix form, we can write the above as\n$$\n\\mathbf{x}^T A \\mathbf{x + b^T x} + c = 0.\n$$\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Any quadric surface can be expressed in the form\n$$\n(\\mathbf{x}')^T \\Lambda \\mathbf{x'} = k\n$$\n> for some $n\\times 1$ column vector $\\mathbf{x}$, diagonal matrix of eigenvalues $\\Lambda$, and real constant $k$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nUsing $A$ as above, define the symmetric matrix\n$$\nS = \\frac{1}{2}(A + A^T).\n$$\nTaking the transpose of both sides of the quadric surface equation gives\n$$\n\\mathbf{x}^T A^T \\mathbf{x + b^T x} + c = 0.\n\n$$\nwhere the latter two terms are not affected by the transpose because the second term is a dot product and the third is a constant. Adding the new equation to the original equation gives\n$$\n\\mathbf{x}^T (A+A^T) \\mathbf{x + 2b^T x} + 2c = 0,\n$$\nor\n$$\n\\frac{1}{2}\\mathbf{x}^T (A+A^T) \\mathbf{x + b^T x} + c = 0,\n$$\nleading to \n$$\n\\mathbf{x}^T S \\mathbf{x + b^T x} + c = 0.\n$$\nAs proven previously, $S$ is always diagonalizable and has real eigenvalues by virtue of being symmetric; let $S = Q\\Lambda Q^{-1}$, where $Q$ is a transformation matrix of orthonormal eigenvectors and $\\Lambda$ is the diagonal matrix of eigenvalues of $S$. Thus we have\n$$\n(\\mathbf{x}')^T \\Lambda \\mathbf{x' + (b')^T x} + c = 0\n$$\nfor $\\mathbf{x'} = Q^{-1} \\mathbf{x}$ and $\\mathbf{b'} = Q^{-1}\\mathbf{b}$. The transformation \n$$\n\\mathbf{x}' = \\mathbf{y} - \\frac{1}{2}\\Lambda^{-1} \\mathbf{b}'\n$$\nwhich exists as long as $\\Lambda^{-1}$ exists, i.e. there is no zero eigenvalue, results in\n$$\n\\begin{aligned}\n\\mathbf{(x')}^T \\Lambda \\mathbf{x}' \\\\\n= ( \\mathbf{y}^T - \\frac{1}{2}(\\mathbf{b}')^T \\Lambda^{-1} )\\Lambda(\\mathbf{y} - \\frac{1}{2}\\Lambda^{-1} \\mathbf{b}') \\\\\n=( \\mathbf{y}^T - \\frac{1}{2}(\\mathbf{b}')^T \\Lambda^{-1} )(\\Lambda\\mathbf{y} - \\frac{1}{2} \\mathbf{b}') \\\\\n= \\mathbf{y}^T \\Lambda\\mathbf{y}  - \\frac{1}{2} \\mathbf{y}^T \\mathbf{b}' - \\frac{1}{2}(\\mathbf{b}')^T\\mathbf{y}+\\frac{1}{4}(\\mathbf{b}')^T\\Lambda^{-1}(\\mathbf{b}') \\\\\n= \\mathbf{y}^T \\Lambda\\mathbf{y}  -  (\\mathbf{b}')^T\\mathbf{y}+\\frac{1}{2}(\\mathbf{b}')^T\\Lambda^{-1}(\\mathbf{b}') - k \\\\\n= \\mathbf{y}^T \\Lambda\\mathbf{y}  -  (\\mathbf{b}')^T(\\mathbf{y}-\\frac{1}{2}\\Lambda^{-1}(\\mathbf{b}')) - k \\\\\n= \\mathbf{y}^T \\Lambda\\mathbf{y}  -  (\\mathbf{b}')^T\\mathbf{x} - k\n\n\\end{aligned}\n$$\nwhere $k$ is a constant (this can be done because every term in the above is a constant). Adding this back to the original equation yields\n$$\n\\mathbf{y}^T\\Lambda\\mathbf{y}=k\n\n$$\nfor some constant $k$, and $\\mathbf{y} = \\begin{bmatrix} x_1' \\\\ x_2 \\\\ ... \\\\ x_n' \\end{bmatrix}$ (i.e. because of the transformation $\\mathbf{x}' = \\mathbf{y} - \\frac{1}{2}\\Lambda^{-1} \\mathbf{b}'\n$, we are now operating along the $x_1', x_2', ..., x_n'$ axes instead of the original $x_1, x_2, ..., x_n$ axes.)\n\n> <span style=\"background-color: #bc42f5; color: black;\">Application</span>. **Conic sections**.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Conic sections are a special case of quadric surfaces: specifically, those in 2 variables (e.g. $x$ and $y$). \n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. Conic sections have general form\n$$\n\\lambda_1 (x')^2 + \\lambda_2 (y')^2 = k,\n$$\n> or, equivalently,\n$$\n\\frac{(x')^2}{\\lambda_2} + \\frac{(y')^2}{\\lambda_1} = k\n$$\n\n> for any constant $k$. Note that $x'$ and $y'$ are intentionally used to indicate that they can be different orthogonal axes than the default $x$- and $y$-axes. This derives from the general form of quadric sections $\\mathbf{y}^T\\Lambda\\mathbf{y}=k$ with eigenvectors $\\lambda_1, \\lambda_2$, which holds true as long as $\\lambda_1 \\neq 0$ and $\\lambda_2 \\neq 0$.\n\nConic sections (so-called because can be classified into the following forms according to the values of $\\lambda_1$ and $\\lambda_2$:\n1. **Ellipses**. If $\\lambda_1\\lambda_2 > 0$, the two eigenvalues are of the same sign. As such, the equation can be manipulated to yield\n$$\n\\frac{(x')^2}{a^2}+\\frac{(y')^2}{b^2}=1\n$$\nfor some $a$, $b$, which determines an ellipse with width $2a$ and height $2b$ about the origin. Its **principal axes** are given by the $x'$ and $y'$ axes (which are the eigenvectors of the coefficient matrix of the original form), and its scale is determined by $k$.\nIf $a = b$ or $\\lambda_1 = \\lambda_2$, the ellipse becomes a circle; any pair of orthogonal principal axes can be chosen.\n\n2. **Hyperbolas.** If $\\lambda_1 \\lambda_2 < 0$, the conic section is a hyperbola with principal axes coinciding with the $x'$- and $y'$-axes.\n\n3. **Parabolas**. If $\\lambda_1 = 0$ or $\\lambda_2 = 0$ (but not both at the same time), $\\Lambda$ is not invertible; instead, an alternate translation gives a conic section of the form $\\lambda_1 (x')^2 + b_2' y_2 = k$, which describes a (quadratic) parabola with principal axes coinciding with the $x'$- and $y'$-axes.    \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. (**Principal axis theorem**). From the fact that symmetric matrices have orthogonal eigenvectors, we observe that the **principal axes** of any conic section are orthogonal.\n\n","n":0.026}}},{"i":73,"$":{"0":{"v":"Eigenvalues and Eigenvectors of Hermitian Matrices","n":0.408},"1":{"v":"Recall the following definitions and properties:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definitions</span>. A Hermitian matrix $A$ is such that $A^{\\dagger} = A$, where $A^{\\dagger}$ denotes the complex conjugate of the transpose of $A$; the complex dot product is defined $\\mathbf{u \\cdot v}$, for two vectors $\\mathbf{u, v}$, as being $\\bar{u_i}v_i$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The eigenvalues of a Hermitian matrix are real.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLet $A$ be a Hermitian matrix. For eigenvalues $\\mathbf{x}$ and corresponding eigenvalues $\\lambda$ of $A$, we have\n$$\nA\\mathbf{x} = \\lambda\\mathbf{x}\n$$\nAnd thus\n$$\n\\mathbf{x^{\\dagger}}A\\mathbf{x} = \\lambda\\mathbf{x^{\\dagger}x}\n$$\nTaking the Hermitian conjugate of both sides gives\n$$\n\\mathbf{x^{\\dagger}}A^{\\dagger}\\mathbf{x} = \\mathbf{x^{\\dagger}}A\\mathbf{x} =\\lambda^{\\dagger}\\mathbf{x^{\\dagger}x} = \\lambda^* \\mathbf{x^{\\dagger}x}\n$$\nas $A^{\\dagger} = A$ by definition, and the Hermitian conjugate of a scalar $\\lambda$ is just its conjugate. It thus follows that\n$$\n\\lambda^* \\mathbf{x^{\\dagger}x}= \\lambda \\mathbf{x^{\\dagger}x},\n$$\nand that\n$$\n(\\lambda^* - \\lambda)\\mathbf{x^{\\dagger}x} = 0,\n$$\nand as $\\mathbf{x^\\dagger x} \\neq 0$ unless $\\mathbf{x}$ is the zero vector (impossible if $\\det(A-\\lambda I) = 0$, by definition), we have\n$$\n\\lambda^* = \\lambda\n$$\nand thus $\\lambda$ is real.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. As real symmetric matrices are also Hermitian, the eigenvalues of real symmetric matrices are real.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> (Adjoint maps).\n\nProceed with the following definitions:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. For an inner product $\\langle\\mathbf{u,v}\\rangle$, define the **adjoint** $A^*$ to a linear map $A$ as such that $\\langle A\\mathbf{u,v}\\rangle = \\langle \\mathbf{u}, A^*\\mathbf{v} \\rangle$. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Self-adjoint** maps $A$ are such that $A$ is its own adjoint over some inner product (e.g. the dot product).\n\n\nHermitian matrices are **self-adjoint maps** over the complex dot product; that is, they satisfy \n$$\n\\mathbf{u} \\cdot (A\\mathbf{v}) = (A\\mathbf{u}) \\cdot \\mathbf{v}\n$$\nwhere $\\cdot$ denotes the **complex dot product** (in contrast, real symmetric matrices are self-adjoint over the real dot product).\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> that Hermitian matrices are self-adjoint. The complex dot product between $A\\mathbf{u}$ and $\\mathbf{v}$ can also be expressed in matrix form as\n\n$$\n(A\\mathbf{u})^{\\dagger} \\mathbf{v}.\n$$\n> Taking the Hermitian transpose of this gives\n$$\n \\mathbf{v}^{\\dagger} (A\\mathbf{u})\n$$\nwhich, as $A^{\\dagger} = A$, equals\n$$\n\\mathbf{v}^{\\dagger} A^{\\dagger} \\mathbf{u} = (A\\mathbf{v})^{\\dagger} \\mathbf{u}\n$$\nwhich is $(A\\mathbf{v}) \\cdot \\mathbf{u}$. The proof that symmetric matrices are self-adjoint follows similarly.\n\nWe can utilize the properties of self-adjoint matrices particularly well here. Let $\\mathbf{x}$ be an eigenvector of $A$ and consider the norm\n$$\n\\lambda(\\mathbf{x}\\cdot \\mathbf{x}).\n$$\nBy $A\\mathbf{x} = \\lambda \\mathbf{x}$, we have\n$$\n\\begin{aligned}\n&\\lambda(\\mathbf{x}\\cdot \\mathbf{x}) \\\\\n&= (\\lambda \\mathbf{x}\\cdot \\mathbf{x}) \\\\\n&= (A\\mathbf{x} \\cdot \\mathbf{x}) \\\\\n&= (\\mathbf{x} \\cdot A\\mathbf{x}) \\\\\n&= (\\mathbf{x} \\cdot \\lambda \\mathbf{x}) \\\\\n&= \\lambda^* (\\mathbf{x}\\cdot \\mathbf{x}) \n\\end{aligned}\n$$\nby the properties of the complex dot product (non-commutative because of a conjugate). As $(\\mathbf{x}\\cdot \\mathbf{x}) > 0$ ($\\mathbf{x} \\neq 0$), we have $\\lambda = \\lambda^*$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. A $n\\times n$ Hermitian matrix $A$ has $n$ orthogonal eigenvectors.\n\nTo prove this, we need to introduce a powerful tool:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Gram-Schmidt Process**.\n\nThe Gram-Schmidt process yields an orthogonal set of $r$ vectors $\\tilde{B} = \\{\\mathbf{v_1,v_2,...,v_r}\\}$ from $r$ linearly independent vectors $B = \\{\\mathbf{w_1,w_2,...,w_r}\\}$.\n\nFirst define the **projection operator** $P_{\\mathbf{v}}(\\mathbf{w})$ as an orthogonal projection of the vector $\\mathbf{w}$ onto the direction of $\\mathbf{v}$:\n$$\nP_{\\mathbf{v}}(\\mathbf{w}) = \\frac{\\mathbf{v\\cdot w}}{\\mathbf{v \\cdot v}}\\mathbf{v}.\n$$\nThis projection works by first constructing a vector that is a scalar multiple of $\\mathbf{v}$: $P_{\\mathbf{v}}(\\mathbf{w}) = \\lambda\\mathbf{v}$ for some $\\mathbf{v}$. As $P_{\\mathbf{v}}(\\mathbf{w})$ is a projection of $\\mathbf{w}$ onto $\\mathbf{v}$, it has magnitude equal to $|\\mathbf{w}|\\cos \\theta$ where $\\theta$ is the angle between the two vectors. Thus we have\n$$\n\\begin{aligned}\n|\\lambda\\mathbf{v}| &= |\\mathbf{w}|\\cos \\theta \\\\\n\\lambda^2 (\\mathbf{v}\\cdot \\mathbf{v}) &= (\\mathbf{w}\\cdot\\mathbf{w})\\cos^2 \\theta \\\\\n\\lambda^2 &= \\frac{(\\mathbf{w}\\cdot\\mathbf{w})}{(\\mathbf{v}\\cdot \\mathbf{v})}\\cos^2 \\theta \\\\\n\n\\end{aligned}\n$$\nas $\\cos^2 \\theta = \\frac{(\\mathbf{w \\cdot v})^2}{(\\mathbf{w\\cdot w})(\\mathbf{v\\cdot v})}$ by the properties of the dot product, we have\n$$\n\\lambda = \\frac{\\mathbf{v\\cdot w}}{\\mathbf{v \\cdot v}}.\n$$\nAlso observe that $\\mathbf{w} - P_\\mathbf{v}(\\mathbf{w})$ is the normal vector from $\\mathbf{w}$ to $\\mathbf{v}$, and is thus orthogonal to $\\mathbf{v}$. This gives us a clue on how to begin our Gram-Schmidt orthogonalization process for $B = \\{\\mathbf{w_1,w_2,...,w_n}\\}$ to orthogonal vectors $\\tilde{B} = \\{\\mathbf{v_1,v_2,...,v_n\\}}$:\n1. Let $\\mathbf{v}_1 = \\mathbf{w_1}$.\n2. We want $\\mathbf{v_2}$ to be orthogonal to $\\mathbf{v}_1$. Therefore, let $\\mathbf{v}_2$ be the perpendicular from $\\mathbf{w}_2$ onto $\\mathbf{v_1}$: $\\mathbf{v_2} = \\mathbf{w_2} - P_{\\mathbf{v_1}}(\\mathbf{w_2})$.\n3. We want $\\mathbf{v_3}$ to be orthogonal to both $\\mathbf{v}_1$ and $\\mathbf{v}_2$. Thus let $\\mathbf{v}_3$ be the perpendicular from $\\mathbf{w}_3$ to both $\\mathbf{v_1}$ and $\\mathbf{v}_2$: $\\mathbf{v}_3 = \\mathbf{w}_3 - P_{\\mathbf{v_1}}(\\mathbf{w}_3) - P_{\\mathbf{v_2}}(\\mathbf{w}_3)$.\n4. $\\vdots$\n5. We want $\\mathbf{v_n}$ to be orthogonal to $\\mathbf{v_1}, \\mathbf{v_2}, ..., \\mathbf{v_{n-1}}$, so let $\\mathbf{v_n}$ to be the perpendicular of $\\mathbf{w}_n$ to all of the above vectors: $\\mathbf{v}_n = \\mathbf{w}_n - \\sum_{i=1}^{n-1}P_{\\mathbf{v}_{i}}(\\mathbf{w}_n)$. \n\nIn each step, we construct $\\mathbf{v}_i$ from the difference between $\\mathbf{w}_i$ and its orthogonal projection to the vector space formed by $\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_{i-1}$, thus yielding an orthogonal vector to all the previous $\\mathbf{v}$s. \n \n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> that the Gram-Schmidt process yields a list of orthogonal vectors.\n\nProceed by induction. First, the base case: $\\mathbf{v}_1$ and $\\mathbf{v}_2$ are orthogonal by definition of the projection operator ($\\mathbf{v}_2$ is perpendicular to a projection onto $\\mathbf{v}_1$.) If we can prove that any consecutive sequence of vectors in $\\tilde{B}$ are orthogona, then we can prove that they are mutually orthogonal.\n\nSuppose that $\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_{k}$ are an orthogonal list of vectors; that is, suppose that \n$$\n\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0\n$$\nfor all $i, j = 1, 2, ..., k$ and $i \\neq j$. We want to prove that $\\mathbf{v}_{k+1}$ is orthogonal to this list. Consider, for any $j = 1, 2, ..., k$,\n$$\n\\begin{aligned}\n \\mathbf{v}_j \\cdot \\mathbf{v}_{k+1} \\\\\n= \\mathbf{v}_{j} \\cdot (\\mathbf{w}_{k+1} - \\sum_{i=1}^{k}P_{\\mathbf{v}_i} (\\mathbf{w}_{k+1})) \\\\\n= \\mathbf{v}_{j} \\cdot (\\mathbf{w}_{k+1} - \\sum_{i=1}^{k}\\frac{\\mathbf{v_i \\cdot w_{k+1}}}{\\mathbf{v_i \\cdot v_i}} \\mathbf{v}_i) \\\\\n= \\mathbf{v}_{j} \\cdot \\mathbf{w}_{k+1} - \\sum_{i=1}^{k}\\frac{\\mathbf{v_i \\cdot w_{k+1}}}{\\mathbf{v_i \\cdot v_i}} \\mathbf{v}_{j} \\cdot\\mathbf{v}_i\n\\end{aligned}\n$$\nwhere, as $\\mathbf{v}_i \\cdot \\mathbf{v}_j = 0$ for all $i \\neq j$, the sum has its only nonzero term as $i = j$:\n$$\n= \\mathbf{v}_{j} \\cdot \\mathbf{w}_{k+1} - \\frac{\\mathbf{v_j \\cdot w_{k+1}}}{\\mathbf{v_j \\cdot v_j}} \\mathbf{v}_{j} \\cdot\\mathbf{v}_j \n$$\nwhich is zero. Thus, by the principle of mathematical induction, $\\mathbf{v}_{k+1}$ is orthogonal to the list $\\mathbf{v_1, v_2, ..., v_k}$. $\\square$\n\nThe extra wao sugoi thing about the Gram-Schmidt process is that it doesn't just give us a list of orthogonal vectors; it gives us a list of basis vectors of $\\mathbf{F}^n$, from any list of $n$ vectors in $\\mathbf{F}^n$. This is owing to\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Any set of nonzero orthogonal vectors is also linearly independent (and thus a basis). \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nSuppose for the sake of contradiction that $\\{\\mathbf{v_1, v_2, ..., v_n}\\}$ is a list of orthogonal vectors (all of which are nonzero), and that there exist scalars $a_1, ..., a_n$, not all zero, such that\n$$\n\\sum_{i=1}^n a_i \\mathbf{v_i} = 0\n$$\nviolating linear independence. Taking the dot product with $\\mathbf{v_K}$ for some $K = 1, 2, ..., n$ yields\n$$\n\\sum_{i=1}^n a_i (\\mathbf{v_i} \\cdot \\mathbf{v_K}) = 0\n$$\nbut as the dot product of any $\\mathbf{v_i, v_K}$, $i \\neq K$, is zero by definition of an orthogonal set of vectors, the above sum is only nonzero when $i = K$. Thus, it is equivalent to the single term\n$$\na_K (\\mathbf{v_K \\cdot v_K}) = 0\n$$\n$\\mathbf{v}_K$ is nonzero and thus has nonzero norm; thus, $a_K = 0$. However, the above process can be conducted with any $K = 1, 2, ..., n$, leading to the conclusion that \n$$\na_i = 0\n$$\nfor all $i = 1, 2, ..., n$. This forms a contradiction.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>.\n1. A set of $n$ mutually orthogonal vectors form a basis in $\\mathbb{F}^n$.\n2. Each $\\mathbf{v}_n$ is a linear combination of the $\\mathbf{w}_n$.\n3. If $\\mathbf{w_1, w_2, ..., w_n}$ is a list of linearly dependent vectors, the Gram-Schmidt process will not be able to generate a full list of $n$ orthogonal vectors; if $\\mathbf{w}_k$ is the first linearly dependent vector in $\\mathbf{w}$, then $\\mathbf{v}_k = 0$.  This is because $\\mathbf{v}_k$ is a linear combination of $\\mathbf{w}_1, ..., \\mathbf{w}_k$.\n4. If a set of linearly independent eigenvectors $B = \\{\\mathbf{w_1,...,w_r}\\}$ yields an orthogonal set of vectors $\\tilde{B} = \\{\\mathbf{v_1, ..., v_r}\\}$ by Gram-Schmidt, then $\\tilde{B}$ also constitutes a set of eigenvectors because $\\tilde{B}$ is a linear combination of $B$. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. An **orthonormal** set, as previously defined is an orthogonal set of vectors $\\{\\mathbf{u_1, u_2, ..., u_n}\\}$ such that\n$$\n\\mathbf{u_i} \\cdot \\mathbf{u_j} = \\delta_{ij},\n$$\n> i.e. the vectors are mutually orthogonal (have dot product zero) and each of norm 1. An orthogonal set can be converted to an orthonormal set simply by normalizing each vector to norm 1.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. If $B$ is an orthogonal set that forms a set of eigenvectors, then the orthonormal set $\\tilde{B}$ constructed from $B$ is also a set of eigenvectors (owing to eigenspaces being subspaces). \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Transformation matrices between two orthonormal bases are unitary.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nSuppose that the two bases $u = \\{\\mathbf{u_1, ..., u_n}\\}$ and $v = \\{\\mathbf{v_1, ..., v_n\\}}$ are orthonormal; we have $\\mathbf{u_i \\cdot u_j = v_i \\cdot v_j} = \\delta_{ij}$. The transformation matrix mapping $u$ to $v$ can be denoted\n$$\nU = \\begin{bmatrix}\nU_{11} & U_{12} & ... & U_{1n} \\\\\nU_{21} & U_{22} & ... & U_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nU_{n1} & U_{n2} & ... & U_{nn}\n\\end{bmatrix}\n$$\nwhere\n$$\n\\mathbf{v}_j = \\sum_{i=1}^n U_{ij} \\mathbf{u}_i\n$$\nwith its columns expressing the basis $v$ in terms of the vectors of $u$. We want to prove that this matrix is unitary, i.e. that \n$$\nU^{\\dagger} U = I\n$$\nor, in scalar form, that\n$$\nU^{\\dagger}_{ik} U_{kj} = U^*_{ki} U_{kj} = \\delta_{ij}\n$$\nAs $\\mathbf{v}_j \\cdot \\mathbf{v}_k = 0$ for $i \\neq j$, we have\n$$\n\\begin{aligned}\n(\\sum_{i=1}^n U_{ij} \\mathbf{u}_i) \\cdot (\\sum_{r=1}^n U_{rk} \\mathbf{u}_i) = 0\n\\end{aligned}\n$$\nwhere the only term in the sum that is nonzero is when $i=r$, i.e.\n$$\n\\sum_{i=1}^n U_{ij}^* U_{ik}(\\mathbf{u_i} \\cdot \\mathbf{u_i}) = 0.\n$$\n(The complex conjugate arises due to the dot product being the complex scalar product). As $\\mathbf{u}_i \\cdot \\mathbf{u}_i = 1$ by orthonormality, we have\n$$\n\\sum_{i=1}^n U_{ij}^* U_{ik} = 0\n$$\nfor $j \\neq k$.\n\nAs $\\mathbf{v}_j \\cdot \\mathbf{v}_j = 1$, we have\n$$\n\\begin{aligned}\n(\\sum_{i=1}^n U_{ij} \\mathbf{u}_i) \\cdot (\\sum_{r=1}^n U_{rj} \\mathbf{u}_i) = 1\n\\end{aligned}\n$$\nor that\n$$\n\\sum_{i=1}^n U_{ij}^* U_{ij} = 1\n$$\nAs such we conclude from the above that\n$$\nU_{ij}^* U_{ik} = \\delta_{jk}\n$$\nusing suffix notation, or $U^*_{ki} U_{kj} = \\delta_{ij}$. Thus $U$ is unitary. $\\square$\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. A $n\\times n$ Hermitian matrix $H$ has $n$ orthogonal, and thus linearly independent, eigenvectors.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n> We present the following alternatives:\n\n> 1. **Proof by condescension.** The statement is trivial and thus does not require a proof.\n 2. **Proof a la Pierre de Fermat**. I have devised a truly marvelous proof of this theorem, but it is too large to fit inside my computer's hard drive.\n3. **Proof by appeal to authority**. My textbook says so, so it must be true.\n4. **Proof by slippery slope**. If Hermitian matrices don't have $n$ orthogonal eigenvectors, then soon enough symmetric matrices aren't going to have orthogonal eigenvectors too and the universe will die a slow, painful, tortured heat death.\n5. **Proof by emotional appeal**. \n\n(I'm going to get out of here and never come back)\n\nSuppose that $\\lambda_i$ and $\\lambda_j$ are two eigenvalues of $H$, not necessarily distinct, corresponding to eigenvectors $v_i$ and $v_j$ respectively. Our goal is to prove that $v_i$ and $v_j$ are orthogonal, i.e. $v_i \\cdot v_j = 0$; in matrix form, this can be expressed as $v_i^{\\dagger} v_j = 0$ (using the complex scalar product).\n\n**Case 1**: $\\lambda_i  \\neq \\lambda_j$. In any case, we have $Hv_i = \\lambda v_i$ and $H v_j = \\lambda v_j$. \n\nNow consider the (complex) dot product between $v_i$ and $v_j$, $v_i^{\\dagger} v_j$. If $Hv_j = \\lambda_j v_j$, then we have\n$$\nv_i^{\\dagger}Hv_j = v_i^{\\dagger} \\lambda_j v_j\n$$\nwhere taking the Hermitian transpose of both sides gives\n$$\nv_j^{\\dagger} H^{\\dagger} v_i = v_j^{\\dagger}\\lambda_j^* v_i\n$$\nwhere $H^{\\dagger} = H$, so\n$$\nv_j^{\\dagger} H v_i = v_j^{\\dagger} \\lambda_i v_i = v_j^{\\dagger}\\lambda_j^* v_i\n$$\nas $Hv_i = \\lambda_i v_i$. Thus $(\\lambda_i^* - \\lambda_j) (v_j^{\\dagger} v_i) = 0$. However, as proven above, the eigenvalues of $H$ are real, so $\\lambda_i^* = \\lambda_i \\neq \\lambda_j$, and so $v_j^\\dagger v_i = 0$. (This can also be proven using self-adjoint maps).\n\n**Case 2:** $\\lambda_i = \\lambda_j$ (repeated eigenvalue). Suppose that $H$ has distinct eigenvalues $\\lambda_1, \\lambda_2, ..., \\lambda_r$, with the other $n-r$ eigenvalues being repeated. By Case 1 (distinct eigenvalues correspond to orthogonal/orthonormal eigenvectors), the corresponding eigenvectors $\\{\\mathbf{v_1,v_2,...,v_r}\\}$ form an orthonormal set. Let\n\n$$\nv = \\{\\mathbf{v_1,v_2,...,v_r,u_{1},u_{2},...,u_{n-r}\\}}\n$$\nbe an orthonormal basis for $\\mathbb{F}^n$; by Gram-Schmidt, such an orthonormal basis can always be found containing $\\mathbf{v_1,...,v_r}$ (which are orthonormal, and thus not affected by Gram-Schmidt) by selecting any basis containg $\\mathbf{v_1,v_2,...,v_r}$ and applying the process. Thus let $P$ represent the transformation matrix from the original basis of $H$ to the orthonormal basis $v$, i.e.\n$$\nP = \\begin{bmatrix}\n\\mathbf{v_1} & \\mathbf{v_2} & ... & \\mathbf{v_r} & \\mathbf{u_1} & \\mathbf{u_2} & ... & \\mathbf{u_{n-r}}\n\\end{bmatrix}\n$$\nBy the above theorem, $P$ is unitary with $P^{\\dagger} = P^{-1}$. As such, $H$ with basis changed to $v$ is\n$$\nP^{-1} H P = P^{\\dagger} H P = (P^{\\dagger} H P )^{\\dagger}\n$$\nas $H^{\\dagger} = H$, and so $H$ is again symmetric under basis transformation to $v$:\n$$\nP^{-1}HP = \\tilde{H} = \\begin{bmatrix}\n\\Lambda & \\mathbf{0} \\\\\n\\mathbf{0} & C\n\\end{bmatrix}\n$$\nwhere $\\mathbf{0}$ represents the zero matrix (of appropriate size), $\\Lambda$ is the diagonal matrix of distinct eigenvalues $\\lambda_1, \\lambda_2, ..., \\lambda_n$ (as the first $r$ basis vectors $\\mathbf{v_1,v_2,...,v_r}$ are eigenvectors)\n$$\n\\Lambda = \\begin{bmatrix}\n\\lambda_1 & 0 & ... & 0 \\\\\n0 & \\lambda_2 & ... & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & ... & \\lambda_r\n\\end{bmatrix}\n$$\nAnd $C$ is a $(n-r)\\times (n-r)$ symmetric matrix\n$$\nC = \\begin{bmatrix}\nc_{11} & ... & c_{1(n-r)} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nc_{(n-r)1} & ... & c_{(n-r)(n-r)}\n\\end{bmatrix}\n$$\nwhich is symmetric because $P^{-1}HP$ is symmetric as shown above. The eigenvalues of $C$ are also eigenvalues of $H$: $\\tilde{H}$ and $H$ share a characteristic equation because they are similar, so\n$$\n\\det(H-\\lambda I) = \\det(\\tilde{H} - \\lambda I) = (\\lambda_1 - \\lambda)(\\lambda_2 - \\lambda)...(\\lambda_r - \\lambda)\\det (C-\\lambda I)\n$$\nwhere $\\det(C-\\lambda I)$ represents the eigenvalues of $C$. As $\\lambda_1, \\lambda_2, ..., \\lambda_r$ are the distinct eigenvalues of $H$, it follows that $\\det(C-\\lambda I)$ solves for the repeated eigenvalues. \n\nFirst, assume that $C$ does not have repeated eigenvalues. $C$ contains the repeat eigenvalues of $H$, but we assume for the sake of simplicity that these eigenvalues do not repeat more than once (i.e. algebraic multiplicity $\\leq 2$); if they do, repeat the above steps until we obtain a new $C$ in the bottom-right corner with no repeated eigenvalues.\n\nAs $H$ is Hermitian, $C$ is also Hermitian, and so by Case 1 we know that $C$ has $(n-r)$ orthogonal eigenvectors, all distinct; let them be labeled $\\mathbf{V_1, V_2, ..., V_{n-r}}$. Now carry out a further change of basis for $H$ through the transformation matrix\n$$\nQ = \n\\begin{bmatrix}\nI_{r\\times r} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{V}\n\\end{bmatrix}\n$$\nwhere $I_{r\\times r}$ is the $r\\times r$ identity matrix and $\\mathbf{V}$ is the matrix of eigenvectors of $C$, in order. The transformation\n$$\n\\hat{H} = Q^{-1}\\tilde{H}Q = Q^{-1}(P^{-1}HP)Q\n$$\nthus leaves the first $r$ columns the same, but diagonalizes the last $(n-r)$ columns:\n$$\n\\hat{H} = \\begin{bmatrix}\n\\lambda_1 & 0 & ... & 0 \\\\\n0 & \\lambda_2 & ... & 0 \\\\\n\\vdots & \\vdots & \\ddots & 0 \\\\\n0 & 0 & ... & \\lambda_n\n\\end{bmatrix}\n$$\nWe conclude that $H$ is diagonalizable through a series of transformations $P$ and $Q$, and therefore that $H$ has $n$ linearly independent eigenvectors, represented by the matrix\n$$\nV = PQ.\n$$\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>.\n1. The eigenvectors of Hermitian matrices can always form an orthonormal basis for $\\mathbb{C}^n$.\n2. Hermitian matrices are always diagonalizable; in particular, they are diagonalizable by unitary matrices, as the transformation matrix from the standard orthonormal basis to the basis of eigenvectors (orthonormal) is unitary by an above theorem.\n3. All of the above also applies to (real) symmetric matrices: they have orthogonal eigenvectors, have real eigenvalues, and are diagonalizable by orthogonal matrices instead of unitary matrices.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. $A$ is a **normal matrix** if $AA^{\\dagger} = A^{\\dagger}A$. It is possible to show that normal matrices are always diagonalizable.","n":0.02}}},{"i":74,"$":{"0":{"v":"Eigenspaces","n":1},"1":{"v":"> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the set of all vectors corresponding to an eigenvalue $\\lambda$ as the **eigenspace** of $\\lambda$, denoted $E_{\\lambda}$. As this set is the nullspace of $(A-\\lambda I)$, it is a subspace.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **algebraic multiplicity** $M_{\\lambda}$ of an eigenvalue $\\lambda$ of a matrix $A$ as the multiplicity of the root $\\lambda$ in the characteristic equation of $A$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. For any matrix $A$ with characteristic polynomial of degree $n$, summing across all its eigenvalues yields\n$$\n\\sum_{\\lambda} M_{\\lambda} = n.\n$$\nWe refer to any eigenvalue $\\lambda$ with $M_{\\lambda} > 1$ as *degenerate*. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **geometric multiplicity** $m_{\\lambda}$ of $\\lambda$ as the dimension of its eigenspace: $m_{\\lambda} = \\dim E_{\\lambda}$. (This is equivalent to the number of linearly independent eigenvectors corresponding to $\\lambda$: the number of independent solutions to $(A-\\lambda I)\\mathbf{x=0}$.)\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. For any eigenvalue $\\lambda$, we have $m_{\\lambda} \\leq M_{\\lambda}$: the geometric multiplicity is no greater than the algebraic multiplicity.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span> will be shown later.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **defect** of $\\lambda$ as $\\Delta_{\\lambda} = M_{\\lambda} - m_{\\lambda}$. As given by the above theorem, the defect is always non-negative. \n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. If $\\sum_{\\lambda} \\Delta_{\\lambda}$ is positive, i.e. if $\\sum_{\\lambda} m_{\\lambda} < n$, then the eigenvectors of $A$ do not span its donamin $\\mathbb{F}^n$. This is because the eigenspaces corresponding to $A$ have total dimension less than $n$.\n\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define a **generalized eigenvector** $\\mathbf{x}$ of a linear map (matrix) $A: \\mathbb{F^n}\\to\\mathbb{F^n}$ as satisfying the equation\n$$\n(A-\\lambda I)^k\\mathbf{x=0},\n$$\n> where $\\lambda$ is the corresponding eigenvalue and $k$ is a positive integer. Generalized eigenvectors are necessary when degenerate eigenvalues exist; i.e. the eigenvectors do not span $\\mathbb{F}^n$. In such a case, the generalized eigenvectors do span $\\mathbb{F}^n$ and the behavior of $A$ can be analyzed through them.\n\nBut even if no eigenvalue is defective - i.e. the dimensions of all the eigenspaces sum to $n$ - can we guarantee that the eigenvectors of $A$ span its domain? The following theorem ensures it is so:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Linear independence of eigenvectors**. Suppose the linear map $A$ has distinct eigenvectors $\\mathbf{x_1, x_2, ..., x_r}$, corresponding to distinct eigenvalues $\\lambda_1, \\lambda_2, ..., \\lambda_r$. Then the eigenvectors $\\mathbf{x_1, ..., x_r}$ are linearly independent.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. We proceed by contradiction: suppose that there exist scalars $c_1, ..., c_r \\in \\mathbb{F}$, not all of which are zero, such that\n$$\nc_1 \\mathbf{x_1} + c_2 \\mathbf{x_2} + ... + c_r\\mathbf{x_r} = \\mathbf{0}\n$$\n> i.e. that $\\mathbf{x_1, ..., x_r}$ are not linearly independent. For some $K = 1, ..., r$, define\n$$\nD = (A-\\lambda_{1} I) (A - \\lambda_2 I) ... \\overline{(A-\\lambda_K I)}...(A-\\lambda_r I)\n$$\n> where the bar denotes that the term is missing from the product; i.e. $D$ is the product of all $A-\\lambda I$ except for $A-\\lambda_K I$. Hence we can also write\n$$\nD = \\prod_{i \\neq K, i = 1, ..., r} (A-\\lambda_i I)\n$$\n> Noting that $(A-\\lambda_i I)\\mathbf{x_i} = 0$ by definition. Applying $D$ to the above equation yields\n$$\n\\begin{aligned}\nD(c_1 \\mathbf{x_1} + c_2 \\mathbf{x_2} + ...  + c_K \\mathbf{x_K} + ... +  c_r\\mathbf{x_r}) &= \\mathbf{0} \\\\\n\n\\end{aligned}\n$$\n>and thus\n$$\n\\begin{aligned}\nD(c_K \\mathbf{x_K}) &= \\mathbf{0} \\\\\n\\prod_{i \\neq K, i = 1, ..., r} (A-\\lambda_i I)(c_K \\mathbf{x_K}) &= \\mathbf{0} \\\\\n\\prod_{i \\neq K, i = 1, ..., r} c_K (\\lambda_K-\\lambda_i)(\\mathbf{x_K}) &= \\mathbf{0}\n\\end{aligned}\n$$\n> as all the other terms yield zero, and $A\\mathbf{x_K} = \\lambda_K \\mathbf{x_K}$. This equation is a single product of constants $\\lambda_K - \\lambda_i$, $c_K$, and the vector $\\mathbf{x_K}$. As all the eigenvalues are distinct, $\\lambda_K - \\lambda_i \\neq 0$; hence $c_K = 0$. However, this argument is valid for any $K$ and thus we have $c_K = 0$ for all $K = 1, 2, ..., r$. This contradicts our definition of linear dependence. $\\square$\n\nAn alternative, and much shorter, proof is found below:\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. As above, let $\\mathbf{x_i}$ be the eigenvectors of $A$ and $\\lambda_i$ be their corresponding eigenvalues for $i = 1, 2, ..., r$. Let\n$$\nc_1 \\mathbf{x_1} + c_2 \\mathbf{x_2} + ...  + c_K \\mathbf{x_K} = \\mathbf{0}\n$$\n> be the **shortest possible** sequence of $\\mathbf{x_1, ..., x_K}$ that are linearly dependent. Multiplying both sides by\n$$\n(A-\\lambda_1 I)(c_1 \\mathbf{x_1} + c_2 \\mathbf{x_2} + ...  + c_K \\mathbf{x_K}) = \\mathbf{0}\n$$\n> eliminates $\\mathbf{x_1}$ from the equation and thus yields an even shorter sequence, which is a contradiction.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remarks</span>. \n1. As all eigenvectors are linearly independent, if a $n\\times n$ matrix has $n$ distinct eigenvectors, its eigenvectors span $\\mathbb{F^n}$. \n2. A $n\\times n$ matrix can have at most $n$ distinct eigenvectors, as its domain has dimension $n$.\n3. If a $n\\times n$ matrix does not have $n$ distinct eigen**values**, it is sometimes still possible to find $n$ distinct eigenvectors that span its domain.\n\n","n":0.036}}},{"i":75,"$":{"0":{"v":"Differential Equations","n":0.707},"1":{"v":"> cf. Better Call Saul, s06e11, \"Breaking Bad\"\n\nAs it turns out, we can derive close to everything we covered in the Differential Equations course with Linear Algebra - in particular, the solution to second-order ordinary differential equations. For a differential equation of the form\n$$\n\\frac{d^2 x}{dt^2} + b\\frac{dx}{dt} + cx = 0,\n$$\nor\n$$\n\\ddot{x} + b\\dot{x} + cx = 0,\n$$\nthe change of variable $y = \\dot{x}$ (a technique we used in Differential Equations!) yields the linear system of equations in $\\dot{x}$ and $\\dot{y}$\n$$\n\\begin{cases}\n\\dot{x} = y \\\\\n\\dot{y} = -cx - by\n\\end{cases}\n$$\nor, in matrix form,\n$$\n\\begin{bmatrix}\n0 & 1 \\\\\n-c & -b\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\ y\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\dot{x} \\\\ \\dot{y}\n\\end{bmatrix}\n$$\nThis is a special case of the general system of differential equations\n$$\nA\\mathbf{x} = \\dot{\\mathbf{x}},\n$$\nor\n$$\n\\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\ y\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\dot{x} \\\\ \\dot{y}\n\\end{bmatrix}\n$$\n\nAs shown, $A$ can be transformed into one of three canonical forms through a change of basis with some matrix $P$:\n$$\n\\begin{aligned}\nP^{-1}A\\mathbf{x} = P^{-1}\\mathbf{\\dot{x}} \\\\\nP^{-1}AP(P^{-1}\\mathbf{x}) = P^{-1}\\mathbf{\\dot{x}} \\\\\nP^{-1}AP\\mathbf{z} = \\dot{\\mathbf{z}}\n\\end{aligned}\n$$\nwhere $\\mathbf{z} = P^{-1}\\mathbf{x}$, and $P^{-1}AP$ is one of three canonical forms:\n1. $P^{-1}AP = \\begin{bmatrix} \\lambda_1 & 0\\\\ 0 & \\lambda_2 \\end{bmatrix}$. As such, the system of equations is separated:\n$$\n\\begin{cases}\n\\lambda_1 z_1 = \\dot{z_1} \\\\\n\\lambda_2 z_2 = \\dot{z_2}\n\\end{cases}\n$$\nleading to $z_1 = A_1e^{\\lambda_1 t}, z_2 = A_2 e^{\\lambda_2 t}$ where $A_1$ and $A_2$ are constants. \n\n2. $P^{-1}AP = \\begin{bmatrix} \\lambda & 0\\\\ 0 & \\lambda \\end{bmatrix}$. The solution is simply $z_1 = A_1 e^{\\lambda t}, z_2 = A_2 e^{\\lambda t}$.\n\n3. $P^{-1} AP = \\begin{bmatrix} \\lambda & 1\\\\ 0 & \\lambda \\end{bmatrix}$. We thus have\n$$\n\\begin{cases}\n\\lambda_1 z_1 + z_2 = \\dot{z_1} \\\\\n\\lambda_2 z_2 = \\dot{z_2}\n\\end{cases}\n$$\nyielding $z_2 = A_2 e^{\\lambda_2 t}$, and substituting into the first equation gives $z_1 = A_1e^{\\lambda_1 t} + A_2te^{\\lambda_2 t}$.\n\nTo obtain solutions for $\\mathbf{x}$ from $\\mathbf{z}$, simply multiply \n by $P$.","n":0.06}}},{"i":76,"$":{"0":{"v":"Change of Basis","n":0.577},"1":{"v":"## Why change basis?\n\nIs there an ideal type of matrix that's the easiest to work with: that's trivially easy to multiply, to take exponents of, to add, etc., and that we can convert all matrices to? The answer is yes - diagonal matrices satisfy all those properties - and the central purpose of changing basis for linear maps is to express them through the lens of such an easily-workable matrix.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **diagonal** matrix is a matrix $D$ with all elements not on its diagonal equal to zero: more precisely, we have $D = \\{D_{ij}\\}$ and $D_{ij} = 0$ whenever $i \\neq j$. As such, $D$ will look something like this:\n$$\nD = \\begin{bmatrix}\nD_{11} & 0 & 0 & \\dots & 0 \\\\\n0 & D_{22} & 0 & \\dots & 0 \\\\\n0 & 0 & D_{33} & \\dots & 0 \\\\\n0 & 0 & 0 & \\ddots & 0 \\\\\n0 & 0 & 0 & \\dots & D_{nn}\n\\end{bmatrix}\n$$\n> or, in Delta notation, we have $D = \\{D_{ii} \\delta_{ij}\\}$ (not invoking suffix/summation convention).\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. Diagonal matrices are extremely convenient to work around in part because it is easy to take their exponents. For instance, we have\n$$\n(D^2)_{ij} = \\sum_{k}D_{ii}\\delta_{ik}D_{kk}\\delta_{kj} = D_{ii}^2 \\delta_{ij} ^2 = (D)_{ij}^2\n$$\n> or, in other words, we have\n$$\nD^2 = \\begin{bmatrix}\nD_{11}^2 & 0 & 0 & \\dots & 0 \\\\\n0 & D_{22}^2 & 0 & \\dots & 0 \\\\\n0 & 0 & D_{33}^2 & \\dots & 0 \\\\\n0 & 0 & 0 & \\ddots & 0 \\\\\n0 & 0 & 0 & \\dots & D_{nn}^2\n\\end{bmatrix}\n$$\n> and similarly\n$$\nD^n = \\begin{bmatrix}\nD_{11}^n & 0 & 0 & \\dots & 0 \\\\\n0 & D_{22}^n & 0 & \\dots & 0 \\\\\n0 & 0 & D_{33}^n & \\dots & 0 \\\\\n0 & 0 & 0 & \\ddots & 0 \\\\\n0 & 0 & 0 & \\dots & D_{nn}^n\n\\end{bmatrix}\n$$\n> for any positive integer $n$. As such, raising a diagonal matrix to a power is equal to simply raising all its elements to that power.\n\n## Eigenvectors as a Change of Basis\n\nUp until this point, we've made passing references and sly allusions to the fact that the matrix representation for a linear map $A$ is \"in terms of the standard basis\". If we want to change the basis for that matrix representation, do we change the linear map itself? \n\nThe answer is no. The linear map itself does not change - if $(1,0)$ was mapped to $(4,3)$ before, so will it remain even after a change of basis. But what will change is the **matrix representation** of that linear map; and as we will see, some bases result in far simpler matrices than others.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Suppose that the linear map $A: \\mathbb{F^n} \\to \\mathbb{F^n}$ has $n$ linearly independent eigenvectors $\\mathbf{x_1, x_2, ..., x_n}$. Then using these eigenvectors as a basis for its matrix representation results in a diagonal matrix for $A$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. What we mean by a **change of basis** is as follows. Any vector $\\mathbf{x} \\in \\mathbb{F}^n$ can be written in terms of a linear combination of the standard basis $(1,0,...,0),...,(0,...,0,1) = e_1, ..., e_n$. As previously defined, the matrix representation of a linear map $A$ in terms of the standard basis is\n$$\n\\begin{bmatrix}\nA(e_1) & A(e_2) & ... & A(e_n)\n\\end{bmatrix}\n$$\n> where the columns are $e_1, ..., e_n$ under transformation by $A$. If we instead use the basis of eigenvectors $\\mathbf{x_1}, \\mathbf{x_2}, ..., \\mathbf{x_n}$, which span $\\mathbb{F^n}$, we obtain the matrix representation\n$$\n\\begin{bmatrix}\nA(\\mathbf{x}_1) & A(\\mathbf{x}_2) & ... & A(\\mathbf{x}_n)\n\\end{bmatrix}\n$$\n> which, as $A(\\mathbf{x_i}) = \\lambda_i \\mathbf{x}_i$ by definition, yields\n$$\n\\begin{bmatrix}\n\\lambda_1(\\mathbf{x}_1) & \\lambda_2(\\mathbf{x}_2) & ... & \\lambda_n(\\mathbf{x}_n)\n\\end{bmatrix}\n$$\n> However, as we are using $\\mathbf{x_1}, \\mathbf{x_2}, ..., \\mathbf{x_n}$ as our basis, each column must be expressed in terms of a linear combination of these vectors:\n$$\n\\begin{cases}\n\\lambda_1\\mathbf{x_1} = \\lambda_1\\mathbf{x_1} + 0\\mathbf{x_2} + ... + 0\\mathbf{x_n} \\\\\n\\lambda_2\\mathbf{x_2} = 0\\mathbf{x_1} + \\lambda_2\\mathbf{x_2} + ... + 0\\mathbf{x_n} \\\\\n\\vdots\n\\end{cases}\n$$\n> This yields the matrix\n$$\nA = \\begin{bmatrix}\n\\lambda_1 & 0 & 0 & ... & 0 \\\\\n0 & \\lambda_2 & 0 & ... & 0 \\\\\n0 & 0 & \\lambda_3 & ... & 0 \\\\\n0 & 0 & 0 & \\ddots & 0 \\\\\n0 & 0 & 0 & \\dots & \\lambda_n\n\\end{bmatrix}\n$$\n> which is diagonal.\n\nOn a more intuitive level: the basis for a linear map is the \"axes\" on which it is acting; if we choose the standard basis, for instance, these axes are simply, in the case of $\\mathbb{R}^3$, the x-, y-, and z-axis. Eigenvectors are vectors along which the linear map preserves direction; if you apply the linear map to its eigenvector, you get a constant multiple of that eigenvector. Thus, if you let the eigenvectors of the linear map be its axes, then all it does along each axis is scale that axis by a constant multiple - yielding a diagonal matrix, as above.\n\nSuch a change of basis - transforming an ordinarily complex matrix into a simple diagonal one - can be useful in the case of repeated applications of the linear map; as exponentiating a diagonal matrix is relatively painless, if we apply $A$ multiple times with eigenvectors as the basis, then our result should also be relatively simple.\n\n## Change of Basis: The General Formula\n\nLet's step outside the bounds of eigenvalues and eigenvectors, and consider the case where we want to change the basis of a linear map $A$ from **any** basis $\\{\\mathbf{e}_i\\}$ of $\\mathbb{F}^n$, $i=1,...,n$, to another basis $\\{\\mathbf{\\tilde{e}}_i\\}$ of $\\mathbb{F}^n$, $i=1,...,n$ (with the funny little hat).\n\nAs displayed above, a change of basis from $\\{e_i\\}$ to $\\{\\tilde{e}_i\\}$ is essentially rewriting a linear combination of $e_n$s\n$$\n\\mathbf{x} = a_1e_1 + a_2e_2 + ... + a_n e_n\n$$\nto the linear combination of $\\tilde{e}_n$s\n$$\n\\mathbf{x} = b_1 \\tilde{e}_1 + b_2 \\tilde{e}_2 + ... + b_n \\tilde{e}_n.\n$$\n(You can pinpoint the exact moment where the amount of craps I gave about bolding out all the vectors dropped irrevocably to zero.)\n\nIn the basis $\\{e_i\\}$, we have\n$$\ne_i = 0e_1 + 0e_2 + ... + 1e_i + ... + 0e_n\n$$\nfor any $i = 1, 2, ..., n$; in matrix form, this can be written as\n$$\n\\begin{bmatrix}\n1 & 0 & ... & 0 \\\\\n0 & 1 & ... & 0 \\\\\n0 & 0 & \\ddots & 0 \\\\\n0 & 0 & ... & 1\n\\end{bmatrix}\n\\begin{bmatrix}\ne_1 & e_2 & ... & e_n\n\\end{bmatrix}\n= \n\\begin{bmatrix}\ne_1 & e_2 & ... & e_n\n\\end{bmatrix}\n$$\nwhere the first matrix is just the identity matrix. In this case, call the identity matrix the **transformation matrix** from the basis $\\{e_i\\}$ to the basis $\\{e_i\\}$; it represents what linear combinations you have to do to go from $e_i$ to $e_i$. (Never has the visceral urge to scream \"no shit, Sherlock\" at myself in front of the computer screen been so unbearably strong.) \n\nBut what if we wanted to change our basis to $\\{\\tilde{e}_i\\}$? Suppose, then, that\n$$\n\\tilde{e}_j = \\sum_{i=1}^{n} P_{ij}e_i\n$$\nfor all $i = 0, 1, ..., n$, and some matrix of scalars $P = P_{ij} \\in \\mathbb{F}$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **transformation matrix** representing a change of basis from $\\{e_i\\}$ to $\\{\\tilde{e}_i\\}$ is the matrix $P = \\{P_{ij}\\}$. Note that these $P_{ij}$ can always be found, as $e_i$ and $\\tilde{e}_i$ span the same space.\n\nIf we write\n$$\nP = \\begin{bmatrix}\nP_{11} & P_{12} & ... & P_{1n} \\\\\nP_{21} & P_{22} & ... & P_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nP_{n1} & P_{n2} & ... & P_{nn}\n\\end{bmatrix}\n$$\nwe can observe that column $j$ represents the coefficients in the linear combinations for $\\tilde{e}_j$, in terms of the basis $\\{e_i\\}$.\n\nThe same procedure can be applied in reverse to yield a transformation matrix $Q$, mapping from the basis $\\{\\tilde{e}_i\\}$ back to $\\{e_i\\}$:\n\n$$\nQ = \\begin{bmatrix}\nQ_{11} & Q_{12} & ... & Q_{1n} \\\\\nQ_{21} & Q_{22} & ... & Q_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nQ_{n1} & Q_{n2} & ... & Q_{nn}\n\\end{bmatrix}\n$$\nwhere we have\n$$\ne_j = \\sum_{i=1}^n Q_{ij}\\tilde{e}_i.\n$$\n\n\n> <span style=\"background-color: #ffb812; color: black;\">Property</span>. $Q$ is the inverse of $P$: $PQ = QP = I$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nGiven the following:\n$$\n\\begin{cases}\n\\tilde{e}_j = \\sum_{i=1}^{n} P_{ij}e_i \\\\\ne_j = \\sum_{i=1}^n Q_{ij}\\tilde{e}_i.\n\\end{cases}\n$$\nSubstituting the first equation into the second yields\n$$\ne_j = \\sum_{i=1}^n Q_{ij}(\\sum_{k=1}^n P_{ki}e_k)\n$$\nor, in summation notation,\n$$\ne_j = Q_{ij}P_{ki}e_k\n$$\nHowever, we also have\n$$\ne_j = \\sum_{k=1}^n \\delta_{kj}e_k = \\delta_{kj}e_k\n$$\nand so $Q_{ij}P_{ki} = P_{ki}Q_{ij} = \\delta_{kj}$; however, $P_{ki}Q_{ij}$ equals $(PQ)_{kj}$, and so $(PQ)_{kj} = \\delta_{kj}$ and $PQ = I$. The argument is equivalent for proving $QP = I$, and thus proving that $P = Q^{-1}$.\n\n## Transformation laws\n\n### Transforming the components of a vector from one basis to another\n\nSuppose some vector $\\mathbf{u} \\in \\mathbb{F}^n$ can be written in terms of a basis $\\{e_i\\}$ of $\\mathbb{F}^n$, as follows:\n$$\n\\mathbf{u} = \\sum_{i=1}^n u_i e_i,\n$$\nor, in matrix form,\n$$\n\\mathbf{u}_{\\{e_i\\}} = \\begin{bmatrix}\nu_1 \\\\\nu_2 \\\\\n\\vdots \\\\\nu_n\n\\end{bmatrix}\n$$\nIf we instead want to express $\\mathbf{u}$ through the basis $\\{\\tilde{e}_i\\}$, i.e.\n$$\n\\mathbf{u} = \\sum_{i=1}^n \\tilde{u}_i \\tilde{e}_i,\n$$\nor, in matrix form,\n$$\n\\mathbf{u_{\\{\\tilde{e}_i\\}}} = \\begin{bmatrix}\n\\tilde{u}_1 \\\\\n\\tilde{u}_2 \\\\\n\\vdots \\\\\n\\tilde{u}_n\n\\end{bmatrix}\n$$\nthen using the entries of $P$ and $Q$ as above, we have\n$$\ne_i = Q_{ki}\\tilde{e}_k\n$$\nusing summation convention, and thus\n$$\n\\mathbf{u} = Q_{ki} a_i \\tilde{e}_k\n$$\nwhere $Q_{ki} a_i$ represents the vector $Q\\mathbf{u}$. Thus: \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Transformation law for vector components**. The components of $\\mathbf{u}$ in the basis $\\{\\tilde{e}_i\\}$ are given by\n$$\n\\mathbf{u}_{\\{\\tilde{e}_i\\}} = Q\\mathbf{u}_{\\{e_i\\}}.\n$$\nConversely, $\\tilde{\\mathbf{u}} = P\\mathbf{u}$ - i.e. the components of $\\mathbf{u}$ in terms of $\\tilde{e}_i$ is the components of $\\mathbf{u}$ in terms of $e_i$, multiplied by the transformation matrix from $\\tilde{e}_i$ to $e_i$ (on the left).\n\n### Transforming the basis of a linear map\nConsider a linear map $A: \\mathbb{F^n} \\to \\mathbb{F^n}$ in which $\\mathbf{u} \\to \\mathbf{u}' = A(\\mathbf{u})$. Suppose that in the basis $\\{e_i\\}$, $\\mathbf{u}$ and $\\mathbf{u}'$ have vector components $\\mathbf{u}_e$ and $\\mathbf{u}'_{e}$ respectively, such that \n$$\n\\mathbf{u}'_e = A\\mathbf{u}_e.\n$$\nNow suppose that we want to change the basis of $A$ to another basis of $\\mathbb{F}^n$, which we call $\\{\\tilde{e_i}\\}$. Suppose that in this basis, $\\mathbf{u}$ and $\\mathbf{u}'$ have vector components $\\mathbf{u}_{\\tilde{e}}$ and $\\mathbf{u}'_{\\tilde{e}}$ respectively, such that \n$$\n\\mathbf{u}'_{\\tilde{e}} = A\\mathbf{u}_{\\tilde{e}}.\n$$\nLet $P$ be the transformation matrix from $e$ to $\\tilde{e}$. Then, by the transformation laws for vector components above, we have\n$$\n\\mathbf{u}'_{e} = P\\mathbf{u}'_{\\tilde{e}}\n$$\nand\n$$\n\\mathbf{u}_{e} = P\\mathbf{u}_{\\tilde{e}}\n$$\nAs such\n$$\n\\mathbf{u}'_e = P\\mathbf{u}'_{\\tilde{e}} = AP\\mathbf{u}_{\\tilde{e}}\n$$\nwhich yields\n$$\n\\mathbf{u}_{\\tilde{e}}' = P^{-1}AP\\mathbf{u}_{\\tilde{e}}\n$$\nAs the columns of the matrix representation of $A$ in terms of the basis $\\tilde{e}$ is the components of $\\tilde{e}_1, \\tilde{e}_2, ..., \\tilde{e}_n$ transformed by $A$, we conclude that \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Under a change of basis from $\\{e_i\\}$ to $\\{\\tilde{e}_i\\}$ represented by a transformation matrix $P: \\tilde{e}_i \\to e_i$, the matrix representation of a linear map $A$, $A_{\\tilde{e}}$, is given by\n$$\nA_{\\tilde{e}}=P^{-1}A_eP.\n$$\nIf instead $A$ was a linear map from $\\mathbb{F}^n$ to $\\mathbb{F}^m$, with bases $\\{e_i\\}$ and $\\{f_i\\}$ respectively, then a change of basis to $\\{\\tilde{e}_i\\}$ and $\\{\\tilde{f}_i\\}$ (spanning $\\mathbb{F}^n$ and $\\mathbb{F^m}$) works in a largely similar manner. Suppose that $P$ is the transformation matrix from $e \\to \\tilde{e}$, and $S$ from $f \\to \\tilde{f}$. Then if\n$$\n\\mathbf{u}'_{\\tilde{f}} = A_{\\tilde{e},\\tilde{f}}\\mathbf{u}_{\\tilde{e}}\n$$\nunder bases $\\tilde{e}, \\tilde{f}$, we have\n$$\nS\\mathbf{u}'_{\\tilde{f}} = \\mathbf{u}'_{f} = A_{e, f} \\mathbf{u}_{e} = A_{e, f} P \\mathbf{u}_{\\tilde{e}}\n$$\nand so\n$$\nS\\mathbf{u}'_{\\tilde{f}} = A_{e, f} P \\mathbf{u}_{\\tilde{e}}\n$$\nyielding\n$$\n\\mathbf{u}'_{\\tilde{f}} = S^{-1}A_{e, f} P \\mathbf{u}_{\\tilde{e}}.\n$$\n\n","n":0.024}}},{"i":77,"$":{"0":{"v":"Cayley-Hamilton Theorem","n":0.707},"1":{"v":"> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Every complex square matrix $A$ satisfies its own characteristic equation: if $p_A(\\lambda) =0$ denotes the characteristic equation of $A$, then $p_A(A) = 0$ also (replacing constants with constant multiples of the identity matrix).\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. (Partial, only for diagonalizable and diagonal matrices)\n\n**Case 1**: diagonal matrices of the form\n$$\nA = \\begin{bmatrix}\n\\lambda_1 & 0 & ... & 0 \\\\\n0 & \\lambda_2 & ... & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & ... & \\lambda_n\n\\end{bmatrix}.\n$$\nThe characteristic equation of $A$ is given by $\\det(A - \\lambda I) = 0$, or equivalently, \n$$\n(\\lambda_1 - \\lambda)(\\lambda_2 - \\lambda) ...(\\lambda_n-\\lambda) = 0\n$$\nby Laplace expansion. Plugging in $A$ into the equation and replacing $\\lambda_i$ with $\\lambda_i I$ gives\n$$\n\\begin{aligned}\n(\\lambda_1 I - A)(\\lambda_2 I-  A) ... (\\lambda_n I - A)  = \\mathbf{0}\\\\\n\\end{aligned}\n$$\nwhich is a product of $n$ diagonal matrices with the first diagonal term missing, the second term missing, etc., in that order. As multiplying diagonal matrices is equivalent to multiplying their terms, $(\\lambda_1 I - A)(\\lambda_2 I - A)$ has its first two diagonal terms missing; multiplying that onto $(\\lambda_3 I - A)$ results in a matrix with its third diagonal term missing; and so on and so forth, until the resulting product is a matrix with all its diagonal terms missing, i.e. the zero matrix.\n\n**Case 2**: diagonalizable matrices. We have\n$$\nP^{-1}AP = D\n$$\nfor some diagonal matrix $D$ and transformation matrix $P$. As previously proven, similar matrices have the same characteristic equation; if $A$ is similar to $D$, then $D$ must satisfy the characteristic equation of $A$, $p_A$. Hence\n$$\np_A(D) = p_A(P^{-1} A P) = 0\n$$\nSuppose that $p_A$ can be written as the polynomial $\\sum_{i=0}^{n} c_i \\lambda^i$. Then we have \n$$\n\\sum_{i=0}^n c_i (P^{-1}AP)^i = 0\n$$\nor, because of the following lemma:\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. Powers of diagonalizable matrices. $(P^{-1} A P)^{i} = P^{-1}APP^{-1}AP...P^{-1}AP = P^{-1}A^iP$; this is useful in taking higher powers, as exponentiating a diagonal matrix is fairly easy.\n\nWe thus have\n$$\n\\begin{aligned}\n\\sum_{i=0}^n c_i P^{-1}A^iP &= 0 \\\\\nP^{-1}\\sum_{i=0}^n c_iA^i P &= 0 \\\\\nP^{-1}(\\sum_{i=0}^n c_iA^i)P &= 0 \\\\\n\\sum_{i=0}^n c_iA^i &= 0.\n\\end{aligned}\n$$\nThus $A$ satisfies its own characteristic equation.\n\n**Case 3**: $2\\times 2$ matrices. As previously shown, all $2\\times 2$ matrices can be written as one of three canonical forms; the first two canonical forms are diagonal and are addressed in Case 1, with the final canonical form\n$$\nB = \\begin{bmatrix}\n\\lambda & 1 \\\\\n0 & \\lambda\n\\end{bmatrix}\n$$\nhaving characteristic equation $p_B(z) = (\\lambda-z)^2$; $p_B(B) = 0$, as is computationally verifiable.\n\nSuppose that $B = P^{-1} A P$ again for some transformation matrix $P$ and $2\\times 2$ matrix $A$. Then $B$ and $A$ have the same characteristic equation due to similarity. We also have\n$$\n\\begin{aligned}\np_B(B) = 0 &= p_A(P^{-1} A P) \\\\\n &= P^{-1}p_A(A)P\n\\end{aligned}\n$$\nowing to derivations in the previous case, and thus $p_A(A) = 0$.\n","n":0.047}}},{"i":78,"$":{"0":{"v":"Canonical Forms","n":0.707},"1":{"v":"> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **canonical form** for a $2\\times 2$ matrix includes the following:\n1. an enlargement along the axes $\\begin{bmatrix}\n\\lambda_1 & 0 \\\\\n0 & \\lambda_2\n\\end{bmatrix}$,\n2. a shear $\\begin{bmatrix}\n\\lambda & 1 \\\\\n0 & \\lambda\n\\end{bmatrix}$, and\n3. scalar multiples of the identity matrix $\\begin{bmatrix}\n\\lambda & 0 \\\\\n0 & \\lambda\n\\end{bmatrix}$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. All $2\\times 2$ matrices are similar to one of the above canonical forms.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nWhich canonical form the matrix is similar to depends on its eigenvalues: whether it has two identical eigenvalues, or two distinct eigenvalues. Let $A$ be the $2\\times 2$ matrix in question, and let $\\lambda_1, \\lambda_2$ be its two eigenvalues; we thus have either $\\lambda_1 = \\lambda_2$, or $\\lambda_1 \\neq \\lambda_2$ (note that two eigenvalues, not necessarily real are guaranteed to exist by the Fundamental Theorem of Algebra).\n\n**Case 1**. $\\lambda_1 \\neq \\lambda_2$. As different eigenvalues correspond to different eigenvectors, and different eigenvectors are linearly independent, $A$ must be diagonalizable as its eigenvectors span $\\mathbb{F}^2$. As such, it is similar to $\\begin{bmatrix}\n\\lambda_1 & 0 \\\\\n0 & \\lambda_2\n\\end{bmatrix}$.\n\n**Case 2**. $\\lambda_1 = \\lambda_2 = \\lambda$. The matrix is either (i) diagonalizable if the eigenvalue is not defective, or (ii) not diagonalizable. If the matrix is diagonalizable, then from our study of diagonalization, it is similar to $\\begin{bmatrix}\n\\lambda & 0 \\\\\n0 & \\lambda\n\\end{bmatrix}$ and we are done. However, if the matrix is not diagonalizable, $\\lambda$ must be defective, indicating that\n$$\n(A-\\lambda I)\\mathbf{x} = 0\n$$\nhas a solution space of dimension 1. (It cannot have dimension 0 because $A\\mathbf{x} = \\lambda\\mathbf{x}$ by definition for another $\\mathbf{x}$ besides the zero vector; this is guaranteed by $\\det(A - \\lambda I) = 0$). Let the vector $\\mathbf{v}$ span the eigenspace $E_{\\lambda}$ and construct $(\\mathbf{v,w})$ spanning $\\mathbb{F}^2$ by selecting $\\mathbf{w}$ as linearly independent to $\\mathbf{v}$. Suppose that\n$$\nA\\mathbf{w} = \\alpha \\mathbf{v} + \\beta \\mathbf{w}\n$$\nfor some $\\alpha, \\beta \\in \\mathbb{F}$  - i.e. that $A$ maps $\\mathbf{w}$ to $(\\alpha, \\beta)$ in the basis $(\\mathbf{v,w})$. As such, as the matrix representation of $A$ has columns $A\\mathbf{v}, A\\mathbf{w}$, we know that $A$ can be transformed into\n$$\nA = \\begin{bmatrix}\nA\\mathbf{v} & A\\mathbf{w}\n\\end{bmatrix} \n= \\begin{bmatrix}\n\\lambda & \\alpha \\\\\n0 & \\beta\n\\end{bmatrix} \n$$\nby a change of basis. Let this transformed matrix be denoted $\\tilde{A}$. As $\\tilde{A}$ and $A$ represent the same linear map, they have the same eigenvalues (but not necessarily the same eigenvectors); thus $\\tilde{A}$ also has a repeat eigenvalue $\\lambda$. This means that $\\beta = \\lambda$ (as can be seen from the characteristic equation). We further observe that\n$$\n(\\tilde{A}-\\lambda I)^2 = \\begin{bmatrix}\n0 & \\alpha \\\\\n0 & 0\n\\end{bmatrix}^2 = \n\\begin{bmatrix}\n0 & 0 \\\\\n0 & 0\n\\end{bmatrix}.\n$$\nDefine $\\mathbf{u} = (\\tilde{A}-\\lambda I) \\mathbf{w}$, where $\\mathbf{u} \\neq 0$ because $\\mathbf{w}$ is not an eigenvector of $\\tilde{A}$ (it is linearly independent to the eigenvector). As such, we have $(\\tilde{A}-\\lambda I) \\mathbf{u} = \\mathbf{0}$ per the above, and thus $\\mathbf{u}$ is an eigenvector of $\\tilde{A}$. We also have\n$$\n\\tilde{A} \\mathbf{w} = \\mathbf{u} + \\lambda \\mathbf{w},\n$$\nas well as\n$$\n\\tilde{A}\\mathbf{u} = \\lambda \\mathbf{u}\n$$\nby $\\mathbf{u}$ being an eigenvector, and thus we conclude that the matrix $\\hat{A}$ where\n$$\n\\hat{A} = \\begin{bmatrix}\n\\tilde{A}\\mathbf{u} & \\tilde{A}\\mathbf{w}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\lambda & 1 \\\\\n0 & \\lambda\n\\end{bmatrix}\n$$\nis similar to $\\tilde{A}$, and thus similar to $A$. $\\square$\n\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. The matrices $\\begin{bmatrix}\n\\lambda_1 & 0 \\\\\n0 & \\lambda_2\n\\end{bmatrix}$ and $\\begin{bmatrix}\n\\lambda_2 & 0 \\\\\n0 & \\lambda_1\n\\end{bmatrix}$ are similar by means of the transformation matrix $\\begin{bmatrix}\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. It is possible to derive simiar canonical forms, or **Jordan normal forms**, for any complex $n\\times n$ matrix; specifically, all square matrices $A$ are similar to a canonical form $\\tilde{A}$ satisfying\n$$\n\\tilde{A}_{ii} = \\lambda_i,\\ \\tilde{A}_{i(i+1)} \\in \\{1, 0\\}, \\text{ 0 otherwise}\n$$\n> i.e. the elements on the diagonal are the eigenvalues, the elements on the superdiagonal (one above the diagonal) are one or zero, and all other elements are zero.\n\n","n":0.04}}},{"i":79,"$":{"0":{"v":"Basic Definitions and Results","n":0.5},"1":{"v":"\n## Polynomials and algebra\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Fundamental theorem of algebra**. Let $p(z)$ be a polynomial of degree $n \\geq 1$:\n$$\np(z) = \\sum_{i=0}^n a_i z^i\n$$\n> for $a_i \\in \\mathbb{C}$. Then the equation $p(z)=0$ has at least one solution in $\\mathbb{C}$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Trivial.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. $p(z)$ having at least one solution $\\omega_n$ implies that it can be factored into the form\n$$\np(z) = (z-\\omega_n)q(z)\n$$\n> for some polynomial $q(z)$ of degree $n-1$. Recursion on this until we are left with a polynomial of degree 0 - a constant, which necessarily equals the leading coefficient $a_n$ - leads to the following factorization:\n$$\np(z) = \\prod_{i=1}^n a_n(z-\\omega_i)\n$$\n> where $\\omega_i$ is the $i$th root of $p(z)$. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **multiplicity** of a root $\\omega_i$ of $p(z)$ is the highest power of the term $(z-\\omega_i)$ in the above factorization of $p(z)$; alternatively, we may say that if $(z-\\omega_i)^k$ is its highest power, then $\\omega_i$ is a $k$-times repeated root of $p(z)$.\n\n>  <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. According to the above, a polynomial with complex coefficients of degree $n$ has exactly $n$ complex roots, with multiplicity counted.\n\n## Eigenvalues and Eigenvectors\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (**Linear map definition for eigenvectors and eigenvalues**). For a linear map $A: \\mathbb{F}^n \\to \\mathbb{F}^n$, where $\\mathbb{F} = \\mathbb{R}$ or $\\mathbb{C}$, define an **eigenvector** of $A$ as a vector $\\mathbf{x} \\in \\mathbb{F}^n$ such that\n$$\nA(\\mathbf{x}) = \\lambda \\mathbf{x}\n$$\n> for some scalar $\\lambda \\in \\mathbb{F}$. Also define the **eigenvalue** corresponding to $\\mathbf{x}$ as $\\lambda$. \n\n> <span style=\"background-color: #ffb812; color: black;\">Remarks</span>. \n1. The geometrical interpretation of an eigenvector $\\mathbf{x}$ of a linear map $A$ is such that $\\mathbf{x}$ preserves its direction when transformed by $A$.\n2. The subspace formed by all eigenvectors of $A$, denoted $l = \\text{span} (\\mathbf{x})$, is denoted the **invariant subspace** of $A$ (as it is invariant in direction under transformation by $A$)\n3. For any polynomial $p(z) = \\sum_{i=0}^n c_i z^i$, $p(A)\\mathbf{x} = p(\\lambda) \\mathbf{x}$ by matrix algebra.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Matrix definition for eigenvectors and eigenvalues)**. For an $n\\times n$ matrix $A$, define its eigenvector $\\mathbf{x}$ and corresponding eigenvalue $\\lambda$ as satisfying\n$$\nA\\mathbf{x}=\\lambda \\mathbf{x}\n$$\n> or, equivalently,\n$$\n(A-\\lambda I)\\mathbf{x}=0\n$$\n> where $I$ denotes the $n\\times n$ identity matrix. This forms a system of linear equations that either has unique solution $\\mathbf{x} = \\mathbf{0}$ if $(A-\\lambda I)$ is invertible, or has other solutions if $(A-\\lambda I)$ is not invertible. Thus, to find values of $\\lambda$ that give us eigenvectors $\\mathbf{x}$ outside of $\\mathbf{x}=0$, we have\n$$\n\\det (A-\\lambda I) = 0\n$$\n> as the condition for $A-\\lambda I$ being singular. Thus, we conclude that if $\\mathbf{x}$ is a nonzero eigenvector of $A$, then its corresponding eigenvalue $\\lambda$ must satisfy the above equation. \n\n> <span style=\"background-color: #ffb812; color: black;\">Remark</span>. As previously considered in \"Matrix Inverses and Linear Equations\", we know that if $\\det (A-\\lambda I) = 0$, then the system of linear equations $(A-\\lambda I)\\mathbf{x} = 0$ has at least one solution.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. $\\det (A-\\lambda I) = 0$ is the **characteristic equation** of square matrix $A$. \n\n> <span style=\"background-color: #ffb812; color: black;\">Properties</span>.\n1. The characteristic equation of a $n\\times n$ matrix $A$ is a polynomial in $\\lambda$ of degree $n$; even if $A$ is a real matrix, its characteristic equation may still have complex roots\n2. By the Fundamental Theorem of Algebra, the characteristic equation of $A$ has $n$ complex roots (counting repeated roots/multiplicities); thus, $A$ has $n$ eigenvalues and associated eigenvectors (which need not be distinct).\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Suppose $p(\\lambda) = \\sum_{i=0}^n c_i \\lambda^i$ is the characteristic equation of $A$, with roots equalling the eigenvalues of $A$, $\\lambda_1, \\lambda_2, ..., \\lambda_n$, not necessarily all distinct. Then:\n1. $\\det A = c_0 = \\lambda_1 \\lambda_2 ... \\lambda_n$.\n2. $c_{n-1} = (-1)^{n-1}Tr(A) = (-1)^{n-1}(\\lambda_1 + \\lambda_2 + ... + \\lambda_n)$. This also implies that the trace of a matrix is the sum of its eigenvalues.\n3. $c_n = (-1)^n$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. $A - \\lambda I$, by definition, has determinant zero for all $\\lambda_1, ..., \\lambda_n$; using the Kronecker delta, we write\n$$\nA - \\lambda I = \\{A_{ij} - \\lambda \\delta_{ij}\\}\n$$\n> and thus\n$$\n\\det(A - \\lambda I) = \\sum \\epsilon_{j_1, j_2, ..., j_n}(A_{j_1 1} - \\lambda \\delta_{j_1 1})(A_{j_2 2}-\\lambda \\delta_{j_2 2}) ... (A_{j_n n} - \\lambda \\delta_{j_n n})\n$$\n> across all possible permutations of $j_1, j_2, ..., j_n = 1, 2, ..., n$, not necessarily in that order. The terms $A_{j_i i} - \\lambda \\delta_{j_1 i}$ only contain $\\lambda$ if $j_1 = i$, as only then will the $\\delta$ term be nonzero. Thus, the only term that contains $n$ powers of $\\lambda$, i.e. every term in the product has a $\\lambda$, is\n$$\n\\epsilon_{1,2,3,4,5,6,...,n}(A_{11}-\\lambda)(A_{22}-\\lambda)...(A_{nn}-\\lambda)\n$$\n> which is simply\n$$\n(A_{11}-\\lambda)(A_{22}-\\lambda)...(A_{nn}-\\lambda) = (-1)^{n}\\lambda^n + ...\n$$\n> as the $\\epsilon$ term is one. Thus, the leading coefficient of $p(\\lambda)$ is one; with the knowledge that it has roots $\\lambda_1, ..., \\lambda_n$, we can factor it into\n$$\np(\\lambda) = (\\lambda_1 - \\lambda)(\\lambda_2 - \\lambda)...(\\lambda_n - \\lambda)\n$$\n> Note that the $\\lambda^{n-1}$ term also originates from\n$$\n(A_{11}-\\lambda)(A_{22}-\\lambda)...(A_{nn}-\\lambda)\n$$\n> as it is not possible for a term in the sum for the determinant to have $n-1$ occurrences of $\\lambda$ (that would imply that only $1$ number from $j_1, j_2, ..., j_n$ is out of position compared to $1,2,...,n$, which is impossible). We have\n$$\n(A_{11}-\\lambda)(A_{22}-\\lambda)...(A_{nn}-\\lambda)\n= (-1)^n \\lambda^n +(-1)^{n-1}(A_{11}+A_{22}+...+A_{nn})\\lambda^{n-1}+...\n$$\n> and so the coefficient of $\\lambda^{n-1}$ in $p(\\lambda)$ is $(-1)^{n-1}(A_{11}+A_{22}+...+A_{nn})$, which is equal to $(-1)^{n-1}$ times the trace. If we compare this to\n$$\np(\\lambda) = (\\lambda_1 - \\lambda)(\\lambda_2 - \\lambda)...(\\lambda_n - \\lambda)\n$$\n> the we have $A_{11}+A_{22}+...+A_{nn} = \\lambda_1 + \\lambda_2 + ... + \\lambda_n$: the sum of eigenvalues for $A$ is equivalent to its trace. Finally, in order to express $\\det A$ using $p(\\lambda)$, note that\n$$\np(\\lambda) = \\det (A - \\lambda I)\n$$\n> by definition, and so $p(0) = \\det A = c_0 = \\lambda_1\\lambda_2...\\lambda_n$.","n":0.032}}},{"i":80,"$":{"0":{"v":"Complex Numbers","n":0.707},"1":{"v":"\n## Introduction, in brief\nLet's start with a return to form: complex numbers. In the realm of linear algebra specifically, complex numbers are important for two reasons. First, in the set of complex numbers $\\mathbb{C}$, we can guarantee that a polynomial of degree $n$ will have $n$ roots by the Fundamental Theorem of Algebra; never again will we have to worry about equations like $\\lambda^2 + 1 =0$ making us more confused than tasting a burger from Pizza Hut and finding it delicious. This becomes particularly relevant when we have to deal with these polynomials, which arise when we find the eigenvalues of a particular matrix - more on that later. \n\n\n> Definition (Complex number). \n\nWe define the imaginary unit $i$ as satisfying $i^2 = -1$; as such, we also define the set of complex numbers $\\mathbb{C}$ to encompass all numbers of the form $$z=a+bi$$    where $a$ and $b$ are real. We write $a = \\text{Re}(z)$, $b=\\text{Im}(z)$, and the complex conjugate $\\bar{z}=a-bi$ (a theorem in algebra will demonstrate that if $z$ is a root of a polynomial, then $\\bar{z}$ is too).\n\nBut second of all - and much more thematically - while real numbers are *one-dimensional*, all lying upon the same infinitely long number line, complex numbers are two-dimensional; it is useful to think of $z=a+bi$ as a vector in the *complex plane*, $\\begin{bmatrix}\n    a\\\\b\n\\end{bmatrix}$. \n\nThe representation of complex numbers as vectors is done on an *Argand plane*, analogous to the Cartesian plane but with the x-axis representing the real part of $z$ and the y-axis representing the imaginary part.\n\nWhat follows is a carousel of important results for complex numbers which are truly astounding in their mind-numbingness, not because of what they are but because we've seen them all before. More on this below.\n","n":0.059}}},{"i":81,"$":{"0":{"v":"Roots of Unity","n":0.577},"1":{"v":"> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Roots of unity). We refer to the complex roots of the equation $\\omega^n=1$ as the *nth roots of unity*; as this is a polynomial of deg $n$, we have $n$ roots of unity, which can be completely described by \n$$\n\\omega=e^{\\frac{2\\pi ki}{n}},\\ k=0,1,2,3,...,(n-1).\n$$\n> As a consequence of the above, we also have \n$$\n        \\sum \\omega = 1+e^{\\frac{2\\pi i}{n}} + e^{\\frac{4\\pi i}{n}} + ... + e^{\\frac{2(n-1)\\pi i}{n}} = 0\n$$\n>  (You may have noticed that we've reached a critical mass of handwaving away statements without proof in this section. De Moivre is surely spinning in his grave. The reason why is because I can't be bothered to prove any of this stuff, so the proofs are left as an exercise to the reader.)\n\n\n","n":0.089}}},{"i":82,"$":{"0":{"v":"Geometric Equations","n":0.707},"1":{"v":"## Complex equation of a line\nThis equation is not called \"complex equation of a line\" because it involves complex numbers, but because it is unnecessarily complex. Observe. \n\nWhat can we do if we want to find a line that goes through the point $x_0$ on the Argand diagram and is parallel to some complex number $\\omega$? By what we know of vector equations for lines, we can write the line as $x = x_0 + \\lambda \\omega$ for some real scalar $\\lambda$. If we rewrite this as $\\frac{x-x_0}{\\omega}=\\lambda$ and take the conjugate of both sides, we obtain\n$$\n    \\frac{\\bar{x}-\\bar{x_0}}{\\bar{\\omega}}=\\bar{\\lambda}=\\lambda\n$$\nas $\\lambda$ is real. Thus the conjugate of this expression is equal to itself:\n$$\n    \\frac{\\bar{x}-\\bar{x_0}}{\\bar{\\omega}}=\\frac{x-x_0}{\\omega}\n$$\nwhich gives us the equation of a line parallel to $\\omega$ and passing through $x_0$. \n\n## Complex equation of a circle\nA circle with center $x_0$ and radius $r$, in abstract terms, is simply a locus of points a distance $r$ away from the center $x_0$. (In less abstract terms, it can be referred to as \"an inferior donut\" or \"a superior oval\".) This can be formulated as \n$$\n    |x-x_0|=r\n$$\nor, squaring both sides,\n$$\n \\begin{aligned}\n        |x-x_0|^2&=r^2 \\\\\n        (x-x_0)(\\bar{x}-\\bar{x_0})&=r^2\n    \\end{aligned}\n$$\n","n":0.073}}},{"i":83,"$":{"0":{"v":"Formulas and Theorems","n":0.577},"1":{"v":"><span style=\"background-color: #03cafc; color: black;\"> Definition</span> (Modulus and argument). Define the modulus of $z=a+bi$ as $|z|=\\sqrt{a^2+b^2}$; this is analogous to the length of its vector representation on the Argand plane. <br/><br/>\nDefine its argument as the angle its vector makes with the real axis: $\\arg z = \\tan^{-1}(\\frac{b}{a})$. The modulus-argument pair $(r,\\theta)$ can uniquely describe a complex number $z$, but each $z$ has infinitely many arguments $\\theta+2k\\pi$ (a full revolution, but not the French kind). We often take only the principal argument - $-\\pi<\\theta<\\pi$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition.</span> We have\n$$z\\bar{z}=a^2+b^2 = |z|^2$$ and $$z^{-1}=\\frac{\\bar{z}}{|z|^2}$$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem.</span> (Triangle inequality). For any two complex numbers $z_1$ and $z_2$, we have $|z_1+z_2|\\leq |z_1|+|z_2|$.\n","n":0.094}}},{"i":84,"$":{"0":{"v":"Complex Exponentiation","n":0.707},"1":{"v":"To extend exponentiation to complex numbers, we use the Taylor series definition of exponentiation:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Exponential function). Define $e^z=\\sum_{n=0}^{\\infty}\\frac{x^z}{z!}$, which can be verified to satisfy the known properties of exponentiation, such as $e^{a}e^{b}=e^{a+b}$. We assume that this sum converges for all complex numbers $z \\in \\mathbb{C}$.\n\nSimilarly, we would like to extend the trigonometric functions to the complex realm, where a geometric definition fails due to the budding, unhinged insanity that underlines the words \"an angle of $39+46\\pi i$ degrees\":\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Complex sine and cosine). Define \n$$\n\\sin z = \\sum_{n=0}^{\\infty} (-1)^{n}\\frac{x^{2n+1}}{(2n+1)!} = x-\\frac{x^3}{3!}+\\frac{x^5}{5!}+...\n$$\n> and\n$$\n\\cos z = \\sum_{n=0}^{\\infty} (-1)^{n}\\frac{x^{2n}}{(2n)!} = 1-\\frac{x^2}{2!}+\\frac{x^4}{4!}-...\n$$\n\nFrom the two above results, we obtain a very important formula throughout all of math.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span> (Euler's formula).         \n$$\ne^{iz}=\\cos z + i\\sin z\n$$\n> It almost feels like I should be wearing a suit and tie before doing this. We also note that any complex number can thus be written in terms of a complex exponential, as its modulus-argument form $(r,\\theta)$ suggests it can be written as \n$$\n        z=r(\\cos \\theta + i\\sin \\theta)=re^{i\\theta}\n$$\n> which allows us to state that multiplication between two complex numbers $z_1=r_1e^{i\\theta_1}$ and $z_2=r_2e^{i\\theta_2}$ requires the multiplication of their moduli and addition of their arguments.\n\n---\n\n# Complex Logarithms\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Complex logarithm). Define the complex logarithm $\\omega = \\ln z$ as the number which satisfies $e^\\omega = z$.<br/><br/> If $z$ is a complex number $z=re^{i\\theta}$, then we have $e^{\\omega} = re^{i\\theta}$ and thus $\\ln \\frac{1}{r} = i\\theta - \\omega$ and $\\omega = \\ln re^{i\\theta} = i\\theta +\\ln r$. (also, $\\ln ab = \\ln a + \\ln b$ gets us here.)\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (Complex powers). Define $z^\\alpha$ for complex $z$ as $e^{\\alpha\\ln z}$, where we insist that the argument used for $z$ is $-\\pi < \\theta < \\pi$, the principal argument.\n\nThis gives rise to\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span> (De Moivre's Theorem). \n$$\n        \\cos n\\theta + i\\sin n\\theta =(\\cos \\theta + i\\sin n\\theta)^n.\n$$\nThis can be proven by induction; it is functionally identical to stating that $e^{ni\\theta}=(e^{i\\theta})^n$, which is obvious over the reals but not so obvious over complex numbers.\n","n":0.053}}},{"i":85,"$":{"0":{"v":"Group Theory","n":0.707},"1":{"v":"> Ever since Galois ended the quintic equation's whole career and then swiftly kicked the bucket two hundred years ago, mathematicians have been probing the mysteries of the great and noble field of Group Theory. Now there's just one last group they need to find: a group of friends.\n\n","n":0.143}}},{"i":86,"$":{"0":{"v":"Quotient Groups","n":0.707},"1":{"v":"> The normal vector to a plane is a vector which has exactly nothing in common with any vector on the plane. The normal distribution is a distribution that somehow manages to pull $e$ and $\\pi$ out of its ass even without being named after Euler. The normal subgroup is a subgroup that satisfies the extremely intuitive and not-at-all whack-ass condition of $gHg^{-1} = H$. I'm starting to feel like mathematicians actually have no idea what the word \"normal\" means.\n","n":0.112}}},{"i":87,"$":{"0":{"v":"Quotient Group Theorems","n":0.577},"1":{"v":"The existence of this section is regrettable, but unfortunately necessary.\n\n## Theorems of normal subgroups\n\nThe last section tried its best to plagiarize every reasonably relevant diagram it could off the internet just to build up a visual understanding of what normal subgroups really were. This section will completely destroy any and all traces of that visual understanding and restore our belief that maybe Galois lived a little too long after all.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. $H$ is a normal subgroup of $G$ if and only if $gH = Hg$ for all $g\\in G$.\n\nBut wait, you say, weren't we using this as the *definition* of normal subgroups all this time? Yes we were. This is because I'm a rebel at heart who refuses to abide by the demands of everything right and true about mathematics.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nAs it turns out, $gH = Hg$ does *not* define a normal subgroup; $ghg^{-1} \\in H$ does. From that, we have\n$$\nghg^{-1} = h_1 \\iff gh = h_1 g \\iff gH = Hg\n$$\nas every $gh$ can be mapped via a bijection to some $h_1 g\\in Hg$. (We know already that right and left cosets have the same size, and if we didn't know that, we do now. It is impossible for two different left-coset elements to be equal the same right-coset element. As the sizes of the two sets are the same, and every element in $gH$ has a unique match in $Hg$, the two sets are the same.)\n\nConversely, if $gH = Hg$, then $gh = h_1g$ for some $h, h_1 \\in H$, not necessarily equal. This yields $gh g^{-1} = h_1 \\in H$, satisfying normality.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. The following subgroups of $G$ are normal: $\\{e\\}$, $G$, and every subgroup of $G$ (if $G$ is abelian). \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nFor $\\{e\\}$, we have $geg^{-1} = (ge)g^{-1} = gg^{-1} = e \\in \\{e\\}$.\n\nFor $G$ itself, we have $gg_1 g^{-1} \\in G$ by closure.\n\nFor abelian groups $G$, any subgroups $H$ must also be abelian. Thus for $h \\in H$, we have $ghg^{-1} = gg^{-1} h = eh = h \\in H$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. If $H \\leq G$ has index $2$ (is half the size of $G$), then $H \\triangleleft G$. (Translator's note: $\\triangleleft$ means \"is a normal subgroup of\")\n\nThis is my first time using the fancy symbol. It feels like some sort of milestone.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nIf $H$ is index $2$, there are only two left cosets of $H$: one equal to $H$ itself, one equal to all elements in $G$ not in $H$. Of the right cosets of $H$, one is also $H$ itself. As the right cosets also partition $G$, the other right coset must be all the elements in $G$ but not in $H$ - i.e. equal to the remaining left coset. Thus, the right cosets and left cosets of $H$ are equal; and by the above lemma, this implies $H \\triangleleft G$.\n\n## The big one\n\nThis title exudes an uncomfortably intimidating aura that I just can't handle properly right now.\n\nBefore we get to *the big one* (note: it's the big one), we batter our hearts with an uncomfortable truth:\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. Let $\\phi: G \\to H$ be a group homomorphism between $G$ and its subgroup $H$. Then the kernel of $\\phi$ is a normal subgroup of $G$: $\\text{Ker}(\\phi) \\triangleleft G$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nBy definition, we know that $\\text{Ker}(\\phi)$ is the set of every $g\\in G$ that maps to $e_G$, the identity of $G$ (which is also the identity of $H$). We also know from previous theorems that $\\text{Ker}(\\phi)$ forms a group. In order for it to be a normal subgroup, we must have\n$$\ng k g^{-1} \\in \\text{Ker}(\\phi)\n$$\nfor all $g \\in G$ and $k \\in \\text{Ker}(\\phi)$. This is true by\n$$\n\\phi(gkg^{-1}) = \\phi(g)\\phi(k)\\phi(g^{-1}) = \\phi(g)e_G \\phi(g)^{-1} = \\phi(g)\\phi(g)^{-1} = e_G\n$$\nas $\\phi(k) = e_G$ by $k \\in \\text{Ker}(\\phi)$, and $H$ and $G$ share a binary operation. Thus $gkg^{-1} \\in \\text{Ker}(\\phi)$, satisfying the condition for $\\text{Ker}(\\phi) \\triangleleft G$.\n\nEvery trombone from the deepest pits of hell and every choir of angels from the highest gardens of heaven all sing the Song of Revelation in unison as they strip away the last veil concealing the ultimate truth:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **THE <sup>(first)</sup> ISOMORPHISM THEOREM.** Let $\\phi$ be a homomorphism from a group $G$ to its subgroup $H$. Then the function\n$$\n\\begin{aligned}\n\\bar\\phi: G/\\text{Ker}(\\phi) \\to \\text{Im}(\\phi) \\\\\n\\end{aligned}\n$$\n> defined as\n$$\n\\bar\\phi(g\\text{Ker}(\\phi)) = \\phi(g)\n$$\n> is well-defined and is a group isomorphism.\n\n### What the hell does this mean??\n\n$\\phi$ is a homomorphism from $G$ to $H$; $\\text{Ker}(\\phi)$ is all the elements in $G$ that get mapped to the identity $e_G$. Things in $G$ are either in $\\text{Ker}(\\phi)$ or they're not. (Well spotted.) If they are, then that means\n$$\n\\phi(g_1) = e_G,\\ g_1 \\in \\text{Ker}(\\phi)\n$$\nIf they're not, then that means\n$$\n\\phi(g_2) \\neq e_G,\\ g_2 \\notin \\text{Ker}(\\phi)\n$$\nBut at the same time, we also have\n$$\n\\phi(g_1 g_2) = \\phi(g_1) \\phi(g_2) = \\phi(g_2) \\phi(g_1) = e_G \\phi(g_2) = \\phi(g_2)\n$$\nas $\\phi(g_1) = e_G$ and any operation with the identity is commutative; the above also arises from $\\text{Ker}(\\phi)$ being a normal subgroup. If we really take an uncomfortably close look at the expression\n$$\ng_1g_2\n$$\nwith $g_2$ encompassing all elements in $\\text{Ker}(\\phi)$, we'll see that it's just a way of saying\n$$\ng_1 \\text{Ker}(\\phi)\n$$\nthe coset of $\\text{Ker}(\\phi)$ under $g_1$; all elements $k$ in this coset satisfy\n$$\n\\phi(k) = \\phi(g_1).\n$$\nTherefore, the cosets of $\\text{Ker}(\\phi)$ split the group $G$ into different cosets based on where $\\phi$ takes each one of these cosets. In other words, the cosets of the normal subgroup $\\text{Ker}(\\phi)$ splits $G$ into different sets based on the *image* of $\\phi$:\n\n![alt text](./assets/images/image-82.png)\n\n(Wonderful illustration over at https://www.math3ma.com/blog/the-first-isomorphism-theorem-intuitively)!\n\nAll the **<sup>FIRST</sup> ISOMORPHISM THEOREM!!!!** says, then, is this: not only do the cosets of $\\text{Ker}(\\phi)$ split $G$ according to the image of $\\phi$, each unique coset has a one-to-one bijection to an element in the image, establishing the isomorphism\n$$\nG/\\text{Ker}(\\phi) \\cong \\text{Im}(\\phi)\n$$\nvia the function\n$$\n\\bar\\phi(g\\text{Ker}(\\phi)) = \\phi(g)\n$$\nor, mapping the coset to its image under $\\phi$.\n\nHaving said that, we're now ready to give a proper\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nFirst of all, the quotient group $G/\\text{Ker}(\\phi)$ 1) exists, and 2) is just the group of cosets $g\\text{Ker}(\\phi)$ for all $g \\in G$. If the function $\\bar \\phi$ defined above is well-defined, then for the same coset $g\\text{Ker}(\\phi)$ written using an alternate representative as $g'\\text{Ker}(\\phi),\\ g' \\in g\\text{Ker}(\\phi)$, we should have\n$$\n\\bar \\phi(g'\\text{Ker}(\\phi)) = \\phi(g') \\text{ by definition}\n$$\nand also\n$$\n\\bar \\phi(g'\\text{Ker}(\\phi)) = \\bar \\phi(g\\text{Ker}(\\phi)) = \\phi(g)\n$$\nas a function shouldn't give something different when you plug in two things that are the same. Thus we need to prove that \n$$\n\\phi(g') = \\phi(g)\n$$\nwhich is true by virtue of $g' \\in g\\text{Ker}(\\phi)$, implying that $g' = gg_1$ for some $g_1 \\in \\text{Ker}(\\phi)$ and \n$$\n\\phi(g') = \\phi(gg_1) = \\phi(g)\\phi(g_1) = \\phi(g) e_G = \\phi(g).\n$$\n(This is essentially the same discussion as the intuitive explanation above: every coset of the kernel has the same image under $\\phi$.)\n\nAs such, $\\bar \\phi$ is well-defined. As previously mentioned, $\\bar \\phi$ is an isomorphism if and only if we have $\\text{Ker}(\\bar \\phi) = \\{e_{G/\\text{Ker}(\\phi)}\\} = \\{e\\text{Ker}(\\phi\\}$, $\\text{Im}(\\bar \\phi) = \\text{Im}(\\phi)$, and $\\bar\\phi$ is a homomorphism.\n\nFor the kernel property, suppose that $\\bar\\phi(a\\text{Ker}(\\phi)) = e_G$. Then $\\phi(a) = e_G$, but then $a \\in \\text{Ker}(\\phi)$, making $a\\text{Ker}(\\phi)$ identical to the identity coset $\\text{Ker}(\\phi)$. Thus, the kernel of $\\bar \\phi$ is the identity coset only.\n\nFor the image property, note that $\\bar\\phi(g \\text{Ker}(\\phi)) = \\phi(g)$ by definition for all $g \\in G$; $\\text{Im}(g)$ is the set of all $\\phi(g)$, so $\\text{Im}(\\bar\\phi)$ and $\\text{Im}(\\phi)$ are indentical.\n\nFinally, $\\bar\\phi$ satisfies the definition of a homomorphism as we have\n$$\n\\bar\\phi(a \\text{Ker}(\\phi) \\cdot b\\text{Ker}(\\phi)) = \\bar{\\phi}(ab\\text{Ker}(\\phi)) = \\phi(ab) = \\phi(a) \\phi(b) = \\bar\\phi(a \\text{Ker}(\\phi)) \\bar\\phi(b \\text{Ker}(\\phi))\n$$\nwhere $\\cdot$ denotes the binary operation under a set of cosets. Therefore, $\\bar\\phi$ is an isomorphism from $G/\\text{Ker}(\\phi)$ to $\\text{Im}(\\phi)$. $\\square$\n\n### Simple groups\n\nLet's end things with a simple definition:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **simple group** is a group with no normal subgroups aside from the trivial subgroup $\\{e\\}$ and itself.\n\nSimple subgroups are like the prime numbers of group theory, and I mean it in every way: they're the building blocks of all non-simple groups, some of them are so complex we still have a gazillion unresolved open problems about them, and mathematicians get divorced over obsessing over them to this day.\n","n":0.027}}},{"i":88,"$":{"0":{"v":"Normal Subgroups","n":0.707},"1":{"v":"\n## What are normal subgroups?\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A subgroup $H$ of some group $G$ is said to be a **normal subgroup** of $G$ if every left coset of $H$ is also a right coset of $H$ (and vice versa). In other words, for every $g_1 \\in G$, an equality exists between the left and right cosets $g_1 H = H g_1$.\n\nThis is equivalent to saying that for every $g_1 h_1 \\in g_1 H$, we have $g_1 h_1 = h_2 g_1$ leading to $g_1 h_1 g_1^{-1} = h_2 \\in H$; as a result, an equivalent condition for a normal subgroup $H$ is\n$$\nghg^{-1} \\in H,\\text{ $\\forall h \\in H,\\ \\forall g \\in G$.}\n$$\nDenote the normal subgroup $H$ by $H \\triangleleft G$, which looks like a really fancypants symbol until you realize it's literally just $<$ with a line added to it. (By the way, $\\triangleleft$ takes 13 more key-presses to type than $<$. If every scientist since the dawn of time wasted their time by writing $\\triangleleft$ every time they wanted to use $<$, we would probably still be using kerosene lamps and tottering out of the Dark Ages on horseback.)\n\nWhy this seemingly-arbitrary condition for determining normalcy? As always with group theory, we have roughly seven thousand words of exposition plus thirty-three different lemmas to get through before we can get to anything resembling an understanding.\n\n## What are group products?\n\nEver wondered how you could fuse two groups into a single unearthly abomination horrifying enough to make the heavens quake and God himself shudder? (For the record, the answer is no. You're supposed to wonder when the McRib is cominng back to McDonalds or whether you can UberEats a lava lamp, like a normal person.) Well, here's how:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **direct product** between two groups $G$ and $H$, denoted $G \\times H$ (and pronounced \"$G$ cross $H$\") is defined as the following group of ordered pairs: \n$$\nG \\times H = \\{(g,h)\\ |\\ g \\in G,\\ h \\in H\\}.\n$$\n> Now you could ask  a kindergartener, a high-school student, and a math undergrad what $A \\times B$ means and none of them would give you the same answer. \n\n> Suppose also that $G$ has binary operation $\\cdot_G$ and $H$ has binary operation $\\cdot_H$. Then the binary operation over the group $G\\times H$ is defined \n$$\n(g_1, h_1)\\cdot_{G\\times H}(g_2, h_2) = (g_1\\cdot_G g_2, h_1\\cdot_H g_2).\n$$\n> (Proving that this obeys all the axioms of a group is left as an exercise to the reader. ~~Do all the dirty work for me, my minions~~)\n\nThe direct product is a relatively intuitive way of combining two groups: it takes all elements in one group, then all the elements of the other group, and squishes them together into an ordered pair like some bizarre failed fusion experiment to produce the ugliest Pokemon. (Are there other types of products? There are, but like a North Korean import customs officer, we're not going to look at any of those.) \n\nThankfully, it also has a very direct meaning (ba-dum tss) when it comes to group structures. For instance, consider the direct product\n\n$$\nC_2 \\times C_4 = \\{(a, b)\\ |\\ a = e^{k\\pi i}, k = 0, 1,\\ b = e^{\\frac{k\\pi i}{2}}, k = 0, 1, 2, 3\\}\n$$\nOf all groups, $C_2$ is the second-simplest (just behind the trivial group): it's just two things bonded to one another for all eternity. (I'd make a joke about how that's something you'll never experience in your life, but unfortunately that's hitting too close to home right now.)\n\n![alt text](./assets/images/image-70.png)\n\nSo what happens when we cross this with $C_4$? Let's consider the ordered pair $(a,1)$, where $a$ can be any element of $C_2$ - say $-1$ and $1$. The elements $(1,1)$ and $(-1,1)$ are still elements of $C_2$; as such, they still obey the above structure. So do $(1, -1)$ and $(-1, -1)$; so do $(1, i)$ and $(-1, i)$; and last and certainly least, so do $(1, -i)$ and $(-1, -i)$. \n\nWe've exhaustively listed all the elements in $C_2\\times C_4$; as there are $2$ elements in $C_2$ and $4$ in $C_4$, the direct product has eight elements total. What our listing has shown is that the direct product has given us four pairs - one for each element in $C_4$ - each of which forms their own little $C_2$. \n\n![alt text](./assets/images/image-71.png)\n\nAlternatively, though, we could also consider our elements to come in two groups of four; $(1,i), (1,-i), (1,1), (1,-1)$ is one, with the $C_2$ element fixed. \n\n![alt text](./assets/images/image-72.png)\n\nPutting everything together, we have something that looks like this:\n\n![alt text](./assets/images/image-73.png)\n\nWhen keeping the $C_4$ element fixed, what we have is every element in $C_4$ producing a $C_2$-structure; when keeping the $C_2$ element fixed, what we have is every element in $C_2$ producing a $C_4$-structure. As a result, every element $(a,b) \\in C_2 \\times C_4$ is part of two different structures at the same time: one $C_2$ structure, which it forms with elements of the form $(...,b)$, and one $C_4$ structure, which it forms with elements of the form $(a,...)$. \n\nA direct product can thus be understood as joining the structures of two groups together: each element in one group now finds itself as part of the structure formed by the other group.\n\n![alt text](./assets/images/image-74.png)\n\n## What are group quotients?\n\nIf direct products give us a way to join the structures of two groups together, is there a way to break the structure of a group apart? (*...and other questions my date asked me over dinner at Burger King.*)\n\nIt turns out there are, and it depends on us understanding what the \"internal structure\" of a group actually means. Recall from earlier that a subgroup represents some sort of repeating sub-structure within the structure of group; that sub-structure repeats again and again to make up the entire group, and the places where it repeats are determined by the cosets of a subgroup, which partition the group. \n\nAs such, a group can be understood as the collection of cosets of a subgroup, each of which have the same mini-structure hidden within them - this is what Lagrange's Theorem told us. For instance, this is $C_6$ (or something isomorphic to it, the 5 positive integers modulo 6) partitioned into the two cosets of $C_3$, $C_3$ and $1C_3$:\n\n\n![alt text](./assets/images/image-75.png)\n\nBut clearly $C_3$ and $1C_3$ themselves are related to each other, by means of the blue action (addition by $1$); as such we simply have\n\n![alt text](./assets/images/image-76.png)\n\nwith $H$ being $C_3$ and $1H$ being $1C_3$. \n\nIn some sense, this process is a reversal of what a direct product does to two groups; whereas a direct product copies the structure of a group and links the copies together through the structure of another group, what we just did took the structure of a group - copies of the structure of some subgroup - and abstracted the connections between these copies of the subgroup from it. \n\nCall this - the deconstruction of $C_6$ into the connections between the cosets of its subgroup $C_3$ - as the *quotient* between $C_6$ and $C_3$, denoted $C_6 / C_3$; its result is $C_2$ - and indeed, just as $C_6 / C_3 = C_2$, we also have $C_6 \\cong C_3 \\times C_2$. This is enough to give us our first tentative definition of a group quotient.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Let $G$ be a group and $H$ its subgroup. The **quotient** $G/H$ is the set of all left cosets of $H$; this may or may not actually form a group, depending on certain mysterious conditions.\n\nHere's another question worth considering: what does it mean for two cosets (or more) to form a group? Let's formalize this by confirming that the set formed by the cosets can have a binary operation defined on them that satisfy all the group axioms. If we had a set of cosets that related to one another as straightforwardly as $C_3$ in $C_6$\n\n![alt text](./assets/images/image-75.png)\n\nthen we could define a binary operation as follows:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Let $G/H$, the quotient of $G$ by subgroup $H$, be the set of left cosets of $H$ as before. Define the binary operation $\\cdot_{G/H}$ between any two such cosets, $g_1 H$ and $g_2 H$ with $g_1, g_2 \\in G$, as \n$$\n(g_1 H) \\cdot_{G/H} (g_2 H) = (g_1 \\cdot_G g_2) H.\n$$\n\nWhen does this operation go wrong? Consider the following example of a subgroup of $A_4$:\n\n![alt text](./assets/images/image-78.png)\n\nThe subgroup itself is a triangular cycle of three elements, and is thus isomorphic to $C_3 = \\{0, 1, 2\\}\\ \\text{mod 3}$; as such, all four cosets are in the form $aC_3$. Let's consider one of these cosets. The representative element of the coset $aC_3$ can be any of the three elements in the coset: either $a = a +0$ itself, or $a+1$, or $a+2$. \n\nSuppose that the coset $aC_3$ is the coset on the very top, and $a, a+1, a+2$ be the elements of that coset in clockwise order:\n\n![alt text](./assets/images/image-80.png)\n\nLet the coset at the very back of the image (the one involving a polyamorous relationship) be $(a\\cdot k)C_3$, where $k \\in A_4$ is the action given by the blue lines. If the cosets form a group, then $aC_3 \\cdot kC_3 = (a\\cdot k)C_3$ should be the same as $(a+1)C_3 \\cdot kC_3$, as $(a+1)C_3$ is the same coset as $kC_3$, just under a different representative. However, the diagram itself shows that this is untrue; $(a+1)\\cdot k$ results in the small-mustached man with the horrible haircut, which is not part of the coset $(a\\cdot k)C_3$. \n\nThus, \n$$\naC_3 \\cdot kC_3\n$$\ncontains the element $a\\cdot k$, which is in a different coset as $(a+1) \\cdot k$. This tells us one very important fact: \n\n> If the elements of a coset are taken to different cosets under some action, then the set of cosets does not have a well-defined binary operation: $a$ and $a+1$ may be two representatives for the same coset $aH$, but if under multiplication by $b$ they go to different cosets, then $aH \\cdot bH \\neq (a+1)H \\cdot bH$ because $(a+1)\\cdot b \\notin (ab)H$.\n\nThis can directly be visualized through the would-be group structure of the quotient group\n\n![alt text](./assets/images/image-81.png)\n\ncollapsed from the above, which is not a group at all, as taking the blue action (e.g. multiplication by some $g$) can take you to three different places - essentially, $a \\cdot b$ will give you three results. \n\nTherefore, our rule is that\n\n> For any coset $g_1H$ and some action defined by multiplication by $g_2$, in order for the quotient $G/H$ to be a group, the elements mapped to by that action on $g_1$ must all be part of the same coset.\n\nIn other words, the set of elements $g_1 H g_2$ (in coset notation, equalling the set $\\{g_1 h g_2\\}$ with $h \\in H$) must be the same coset. As $e \\in H$, there is only a single coset this can possibly be: the coset $g_1 g_2 H$. This gives us\n$$\ng_1 H g_2 = g_1 g_2 H\n$$\nor, at long last,\n$$\ng_2 H = H g_2\n$$\nfor all $g_2 \\in G$. This is exactly the definition of a normal subgroup! \n\nTherefore, we finally arrive at what normal subgroups truly are:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The set of left cosets $G/H$ for a subgroup $H$ of a group $G$ is only a group when $H$ is a normal subgroup: for all $g \\in G$, $ghg^{-1} \\in H$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. The above offers a justification for why quotients do not make sense unless the left cosets of a subgroup equal the right cosets. However, we can prove this even more formally. The operation\n$$\ng_1H \\cdot_{G/H} g_2H = (g_1g_2)H\n$$\n> is well-defined only if, under an alternative representative $g_1'$ where $g_1 H = g_1' H$, we still have\n$$\ng_1' H \\cdot_{G/H} g_2 H = (g_1 g_2)H. \n$$\n> Let $g_1' = g_1 h_1$ for some $h_1 \\in H$, true under the condition $g_1' \\in g_1 H$. If $g_1' g_2$ is part of the coset $(g_1g_2)H$, then $(g_1' g_2)H = (g_1 g_2)H$, as cosets with common elements are the exact same coset. As such, our condition is\n$$\ng_1' g_2 = g_1 h_1 g_2 = g_1 g_2 h\n$$\n> for some $h \\in H$; rewriting the above as\n$$\ng_1 h_1 g_2 = g_1 g_2 g_2^{-1} h_1 g_2 = g_1 g_2 (g_2^{-1} h_1 g_2)\n$$\n> we see that the only way the equation holds is if $h = g_2^{-1} h_1 g_2$, or, in other words, if $g_2^{-1} h_1 g_2 \\in H$ for all $h_1 \\in H$ and $g_2 \\in G$. This is the definition of a normal subgroup. $\\square$","n":0.022}}},{"i":89,"$":{"0":{"v":"Permutation Groups","n":0.707},"1":{"v":"> Henceforth, combinations and permutations shall be referred to as \"combs\" and \"perms\". Any objections will be directed to my hairstylist.\n","n":0.218}}},{"i":90,"$":{"0":{"v":"Sign of a Permutation","n":0.5},"1":{"v":"### Transpositions\n\n> A mathematician and a musician walk into an instrument shop and see an upright clarinet on display. \"Why, that's a clarinet transposed to E-Flat!\" the musician exclaims. But the mathematician remains unconvinced; after all, an upright clarinet transposed always becomes A Flat clarinet.\n\n~~(dear god how is this joke even worse than the last one i wrote)~~\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Recall that a **transposition** is the same thing as a $2$-cycle: it is denoted $(a_1 a_2)$ for some $a_1$, $a_2$, and it swaps the positions of these two elements and these two elements alone.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Every permutation can be written as a (not necessarily unique, and not necessarily disjoint) product of transpositions.\n\nTo paraphrase almost verbatim from my textbook, this is \"one of these theorems which seem deep and mysterious until you realize all it's telling you is that you can get things in a list into some order you want by swapping these things around two at a time.\" Excluding elbow nudges, headbutts and dropkicks, swapping two at a time is also the only way the human body permits us to swap things around.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nWe know already from disjoint cycle decomposition that any permutation $\\sigma$ is expressible as a product of cycles; it thus suffices to show that any general $k$-cycle\n$$\n(a_1 a_2 ... a_k)\n$$\ncan be written as a product of permutations. This is shown to be true via the construction\n$$\n(a_1 a_2 ... a_k) = (a_1 a_2) (a_3 a_4) ... (a_{k-1} a_k).\n$$\nTo prove this equality, we propose the following: \n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. For the distinct numbers $a_1, ..., a_k$ pictured above, we have\n$$\n(a_1 a_2)(a_2a_3...a_{k}) = (a_1a_2...a_k).\n$$\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n$$\n(a_2 a_3 ... a_k)(j) = \\begin{cases}\na_2, j = a_k \\\\\na_{i+1}, j = a_2, ..., a_{k-1} \\\\\nj, j\\neq a_2,..., a_k\n\\end{cases}\n$$\n> and thus\n$$\n(a_1 a_2)(a_2 a_3 ... a_k)(j) = \\begin{cases}\n(a_1 a_2)(a_2) = a_1, j = a_k \\\\\na_{i+1}, j = a_2, ..., a_{k-1} \\\\\na_2, j = a_1 \\\\\nj, j \\neq a_1, ..., a_k\n\\end{cases}\n$$\nwhich matches the definition of the $k$-cycle $(a_1 a_2 ... a_k)$. Given this lemma, we have\n$$\n(a_{k-2} a_{k-1})(a_{k-1} a_k) = (a_{k-2} a_{k-1} a_k)\n$$\nand\n$$\n(a_{k-3} a_{k-2}) (a_{k-2} a_{k-1} a_k) = (a_{k-3} a_{k-2} a_{k-1} a_k)\n$$\nand so on, resulting in the desired equality. This decomposition is intuitively understood as \"moving all things to the left by one position equals swapping $1$ with $2$ (so that $2$ is now at $1$), then swapping $2$ with $3$ (so that $3$ is now at $2$), ...\".\n\n### The sign of a transposition\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Writing $\\#(\\sigma)$ as the number of transpositions making up the transposition decomposition of $\\sigma$, define the **sign** of $\\sigma$ as $(-1)^{\\#(\\sigma)}$ - $1$ when $\\sigma$ can be decomposed into an even number of transpositions, $-1$ if instead that number is odd.\n\nThe well-definedness of $\\#(\\sigma)$ is predicated on the following theorem:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The decomposition of a permutation into transpositions is, in general, not unique; however, its **sign** is. The number of transpositions present in the transposition decomposition of a permutation $\\sigma \\in S_n$ will either be always odd or always even, yielding either $1$ (in the even case) or $-1$ (the odd case) as the sign of $\\sigma$ across all possible decompositions.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThe proof relies on two distinct parts: 1) that a transposition $\\sigma \\in S_n$ is equal to the identity permutation $e$ multiplied by a number of transpositions; and 2) multiplying a permutation by a transposition causes its sign to remain unchanged.\n\nThe first statement is true by the existence of transposition decompositions for every $\\sigma$. For the second statement, let us consider the composition $\\sigma(ab)$ for a transposition $(ab)$ where, without loss of generality, we assume $a<b$; denoting the number of disjoint cycles $\\sigma$ can be decomposed into (a number which is unique, by the Disjoint Cycle Decomposition Theorem) by $l(\\sigma)$, we have either\n\n1) That $a$ and $b$ lie in different cycles in the cycle decomposition of $\\sigma$:\n$$\n\\sigma = (...)(aa_1 a_2 ... a_k)(bb_1 b_2 ... b_l)...(...)\n$$\ncycling $a$ and $b$ to position $1$ if necessary. By the lemma of multiplying a cycle and a transposition above, we have\n$$\n\\begin{aligned}\n(aa_1 ... a_k)(b b_1 ... b_l)(ab) &= (aa_1)... (a_{k-1} a_k) (b b_1) ... (b_{l-1}b_l)(ab) \\\\\n&= (aa_1)... (a_{k-1} a_k) (b b_1) (ab) ... (b_{l-1}b_l) \\\\\n\\end{aligned}\n$$\nas disjoint cycles are commutative; this results in\n$$\n\\begin{aligned}\n(aa_1)... (a_{k-1} a_k) (b b_1) (ab) ... (b_{l-1}b_l) &= (aa_1)... (a_{k-1} a_k) (b_1 b) (b a) ... (b_{l-1}b_l) \\\\\n&= (aa_1)... (a_{k-1} a_k) (b_1 b a) ... (b_{l-1}b_l) \\\\\n&= (aa_1)... (a_{k-1} a_k) (bab_1) (b_1 b_2)... (b_{l-1}b_l) \\\\\n&= (aa_1)... (a_{k-1} a_k) (bab_1...b_{l-1}b_l) \\\\\n&= (a a_1) (bab_1...b_{l-1}b_l) ...(a_{k-1} a_k) \\\\\n&= (a_1 a)(a b_1 ... b_l b)... (a_{k-1} a_k) \\\\\n&= (a_1 a b_1 ... b_l b) ... (a_{k-1} a_k) \\\\\n&= (a b_1 ... b a_1) (a_1 a_2) ... (a_{k-1} a_k) \\\\\n&= (ab_1 ... b a_1 ... a_{k})\n\\end{aligned}\n$$\nand thus $l(\\sigma(ab))$, the number of disjoint cycles in the decomposition of $\\sigma(ab)$, is $1$ less than $l(\\sigma)$ (as the two distinct cycles $(aa_1... a_k)$ and $(bb_1 ... b_l)$ are now a single cycle.) Thus we have $l(\\sigma(ab)) = l(\\sigma) - 1$.\n\n2) If $a$ and $b$ instead lie in the same cycle $(a a_1 ... a_k b b_1 ... b_l)$, we have\n$$\n\\begin{aligned}\n(a a_1 ... a_k b b_1 ... b_l)(ab) &= (a_1 ... a_k b b_1 ... b_l a)(ab) \\\\\n&= (a_1 ... a_k b b_1) (b_1 ... b_l a b) \\\\\n&= (a_1 ... a_k b)(bb_1) (b_1 ... b_l a b) \\\\\n&= (a_1 ... a_k b)(bb_1) (b b_1 ... b_l a) \\\\\n&= (a_1 ... a_k b)(bb_1) (b b_1) (b_1 ... b_l a) \\\\\n\\end{aligned}\n$$\nAs permutation products are associative and a transposition is its own inverse, the $(bb_1)$s cancel out, leaving \n$$\n(a_1 ... a_k b) (ab_1 ... b_l)\n$$\nwhich means that multiplying by $(ab)$ increases the number of cycles composing $\\sigma$ by 1 - $l(\\sigma(ab)) = l(\\sigma) + 1$. In either of the two above cases, $l(\\sigma(ab)) \\equiv l(\\sigma) + 1\\ (\\text{mod}\\ 2)$.\nAs any $\\sigma$ can be written as a product of transpositions with the identity permutation $e$, we thus have\n$$\nl(\\sigma) \\equiv l(e) + \\#(\\sigma)\\ (\\text{mod}\\ 2)\n$$\nwhere $\\#(\\sigma)$ is the number of transpositions composing $\\sigma$; as $l(\\sigma)$ and $l(e)$ is unique with respect to $\\sigma$ and $e$, so is $\\#(\\sigma)\\ (\\text{mod}\\ 2)$. $\\square$\n\n### Alternating groups\n\nThe well-definedness of the **sign of a permutation** $\\sigma$, written $\\text{sign}(\\sigma)$, allows us to classify permutations into two types based on their sign.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Call $\\sigma$ an **even permutation** if $\\text{sign}(\\sigma) = 1$; call $\\sigma$ an **odd permutation** if instead $\\text{sign}(\\sigma) = -1$. (Note the similarity to the alternating Levi-Civita symbol $\\epsilon_{...}$!)\n\nMore interesting still is \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. $\\text{sign}(\\sigma)$ for $\\sigma \\in S_n$ is a group homomorphism from $S_n$ to the cyclic group $C_2 = \\{-1, 1\\}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nAs multiplication is the binary operation common to both sets, our condition for a group homomorphism is \n$$\n\\text{sign}(\\sigma \\tau) = \\text{sign}(\\sigma) \\text{sign}(\\tau)\n$$\nfor any two permutations $\\sigma, \\tau \\in S_n$. Suppose that $\\sigma$ is composed of $k$ transpositions, and $\\tau$ of $l$; by the well-definedness of the sign function, we know that as $k$ and $l$ are constant $\\text{mod }2$ no matter the decomposition used, $k + l$ will also be constant $\\text{mod }2$ - exactly the number of transpositions making up $\\sigma \\tau$ (of course, this is not the only decomposition possible for $\\sigma \\tau$). This leads to\n$$\n\\text{sign}(\\sigma \\tau) = (-1)^{k+l}=(-1)^k (-1)^l = \\text{sign}(\\sigma) \\text{sign}(\\tau)\n$$\nsatisfying the condition for a homomorphism. $\\square$\n\nThis leads directly to\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **alternating group** $A_n$ consists of all permutations in $S_n$ that have sign $1$ (or, equivalently, are even).\n\nWhy define such a group - which, by the way, we haven't even shown to be a group yet - at all? There are probably many, many reasons, the vast majority of which I have no hopes of comprehending or communicating, but two stand out as most immediate:\n\n1. > <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. $A_n$ is a subgroup of $S_n$; the set of odd permutations in $S_n$ is not.\n\n    This follows from the fact that 1) every permutation has an inverse, including even ones, and inverted even permutations are still even by virtue of the identity permutation $e$ being even; and 2) two even permutations composed together are still even (as $\\text{sign})$ is a multiplicative function). The same is not true for odd permutations; the identity permutation is not odd, and the product of two odd permutations is even ($-1 \\cdot -1 = 1$.)\n\n2. Even permutations preserve *orientation*: if $x$, $y$ and $z$ were the coordinate axes with right-handed orientation, and $\\sigma$ was an even permutation on these axes, then the resulting set of axes would remain right-handed. Importantly, this finds an application both in the formula for the cross product\n    $$\n    \\mathbf{a \\times b} = \\epsilon_{ijk}a_jb_k\n    $$\n    where $\\epsilon$ is just the sign function in a trench coat after several cosmetic surgeries in Korea, as well as for the determinant\n    $$\n    \\det A = \\sum_{\\sigma}\\text{sign}(\\sigma)a_{\\sigma(i) i}.\n    $$\n\nOne last result to cap things off! The sign of a permutation is the product of the signs of the cycles in its disjoint cycle decomposition. This much we know already, but we further propose that\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The sign of any $k$-cycle $(a_1 a_2 ... a_k)$ is $(-1)^{k-1}$. This directly follows from $(a_1a_2... a_k) =(a_1 a_2)(a_2 a_3) ... (a_{k-1} a_k)$ totaling $k-1$ transpositions.\n\nAs such, if $k$ is even, then the $k$-cycle is odd; if $k$ is odd, then the $k$-cycle is even. Fantastic.\n\nThis results in\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. A permutation is even if there is an even number of odd cycles in its disjoint cycle decomposition, per the above.\n\nwhich amounts to a fairly convenient way to check the sign of any permutation, all things considered.\n","n":0.025}}},{"i":91,"$":{"0":{"v":"Permutations and Cycles","n":0.577},"1":{"v":"> What happens when a mathematician tries to comb their perm? Nothing good; everything goes out of order.\n\n### Permutations\n\nA *permutation* is a way to arrange things in a line in order: students in a lunch line, pens in a pencilcase, people in your group of friends - no, wait, it turns out there's only one permutation for that, I'm sorry to say. Sometimes there are very few ways to do so; sometimes there are a lot, like there being more ways to organize my Magic: The Gathering collection then there are atoms in the universe. If lining things up and ordering them really is a sign of autism, then mathematicians truly embody the phrase, \"I don't struggle with autism - I'm actually very good at it.\"\n\nPermutations are very natural candidates for becoming group members because they swap things around but don't actually change anything substantial, they can and will backtrack on everything they do, and they can do ten different steps before realizing they could've done it in one step and saved all the effort. Groups involving permutations have a very simple definition:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define a **permutation** of a set $X = \\{x_1, x_2, ..., x_n\\}$ as a function $f(x)$ which re-orders $X$: for $i = 1, 2, ..., n$, $f$ maps $x_i$ to another element in the list $x_j$, with the condition that the set $\\{f(x_1), f(x_2), ..., f(x_n)\\}$ is identical to $X$ - i.e. $f$ changes the order of $X$ but does not change the elements themselves. We stipulate that $f$ is invertible; its inverse $g$ reverses the permutation, such that $(f\\circ g) = (g \\circ f) = Id$, the identity function that maps every element of the set to itself.<br/><br/>\n> Call the list of all permutations of the set $X = \\{1,2,...,n\\}$ the **$n$th permutation group**, denoted $S_n$; for a general set $X$, instead denote this group by $\\text{Sym}(X)$, called the **symmetric group** of $X$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. For any set $X$, the triple $(\\text{Sym}(X), \\circ, Id)$ indeed forms a group; if all the elements of $X$ are distinct, this group will have order $n!$ This arises directly from the definition of a permutation: a combination of two permutations is still a permutation, as is the inverse a permutation. \n\n(If at any time there is ambiguity between me being excited about something and me writing a factorial, it's always a factorial.)\n\n### Cycles\n\nA very specific type of permutation is\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **$k$-cycle**. Let $\\{a_1, a_2, ..., a_k\\}$ be a subset of $\\{1, 2, ..., n\\}$, all unique and in any order. The $k$-**cycle** of these elements, denoted $(a_1a_2...a_k)$, is a permutation that changes the order of these elements in the following ways:\n1. Initially, the element at position $i$ in the set is $a_i$. (Assumed with loss of generality; we simply label the element in the $i$th position $a_i$, not caring about sorting them in ascending order.) (~~I had to physically restrain myself from typing position $i-1$ instead of position $i$. Words cannot describe how much I loathe Python for this.~~)\n2. If $i$ is not the final position $k$, the element that will occupy position $i$ is the next element $a_{i+1}$ instead: $(a_1a_2...a_k)(i) = a_{i+1}$. In essence, every element is shifted leftward by one position: $a_{i+1}$ is shifted to position $i$.\n3. If $i$ is the final position $k$, the element that will occupy it under the $k$-cycle is $a_1$, the first element; it's gone so far left it has nowhere else to go and finds itself on the extreme right instead, which is a sentence usually reserved for describing the lifecycles of most political Reddit forums.\n4. The $k$-cycle does not change the positions of any elements outside of $\\{a_1, a_2, ..., a_k\\}$; if $m=1, 2, ..., n$ is not part of this aforementioned set, $(a_1a_2...a_k)(m) = m$. \n\nMore formally, we write \n$$\n(a_1a_2 ... a_k)(i) = \\begin{cases}\na_{i+1},\\ i \\neq k,\\ i \\in \\{a_1, a_2, ..., a_k\\} \\\\\na_1, i = k,\\ i \\in \\{a_1, a_2, ..., a_k\\}  \\\\\ni, i \\notin \\{a_1, a_2, ..., a_k\\}\n\\end{cases}\n$$\ntreating the $k$-cycle as a function of position $i$ (which it certainly is). \n\nWhat a revolution does to the entrenched social hierarchies of a thousand-year monarchy, a $k$-cycle does to $\\{a_1, a_2, ..., a_k\\}$. Every layer of the social stratosphere, from the lowly servant $a_k$ to the well-born $a_3$, join forces to topple the vile despot $a_1$ into poverty and obscurity; freedom and democratic governance are promised until the scheming $a_2$ snabs the crown.\n\n> Note that a $k$-cycle repeated $k$ times returns the list to its original order!\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Call a $2$-cycle $(a_1 a_2)$, whose only purpose is to swap the two elements back and forth, a **transposition**. (My professor tells me I'm not allowed to call them bicycles anymore.)\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Let $(a_1 a_2 ... a_k)$ and $(b_1 b_2 ... b_l)$ be two cycles (of possibly different lengths). If $\\{a_1, a_2, ..., a_k\\}$ and $\\{b_1, b_2, ..., b_l\\}$ do not share any common elements - i.e. their intersection is the null set - call the two cycles **disjoint:** they act on entirely different elements, and thus do not interfere with one another.\n\n### Compositions of cycles\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Composition of cycles.** Cycles are functions; as such, if $\\sigma$ and $\\tau$ denote two cycles, the notation $\\sigma \\tau(i)$ is understood to be $\\sigma(\\tau(i))$ - first the cycle $\\tau$ applied to $i$, then $\\sigma$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. Disjoint cycles are commutative.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLet $\\sigma = (a_1 a_2 ... a_k)$ and $\\tau = (b_1 b_2 ... b_l)$ be two disjoint cycles, with none of the $a_i$s and $b_j$s equalling one another (by definition of disjoint cycles). By the definition of a cycle given above, we have $\\sigma(i)$ if $i \\neq a_1, ..., a_k$ and $\\tau(i)$ if $i \\neq b_1, .., b_l$; any $i$ cannot be part of both $\\{a_1, ..., a_k\\}$ and $\\{b_1, b_2, ..., b_l\\}$ at once as they have intersection $\\emptyset$. Thus, either $\\sigma(i) = i$ or $\\tau(i) = i$; in addition, if $i$ is not in $\\{a_1, ..., a_k\\}$, then neither is $\\tau(i)$ (as $\\tau$ maps from $b_i$ to $b_i$, none of which are in $\\{a_1, ..., a_k\\}$.)\n\nIn the first case where $\\sigma(i) = i$, we have\n$$\n\\begin{aligned}\n\\sigma(\\tau(i)) &= \\tau(i)\\text{ as $\\tau(i) \\notin a_i$} \\\\\n&= \\tau(\\sigma(i)) \\text{ as $\\sigma(i) =i$}.\n\\end{aligned}\n$$\nSimilarly, for $\\tau(i) = i$ we have\n$$\n\\begin{aligned}\n\\tau(\\sigma(i)) &= \\sigma(i)\\text{ as $\\sigma(i) \\notin b_i$} \\\\\n&= \\sigma(\\tau(i)) \\text{ as $\\tau(i) =i$}.\n\\end{aligned}\n$$\nThis completes the proof; the statement can be intuitively understood as saying that if two actions act on different parts of something, then the order in which they are performed does not affect the outcome. This is true for groups and less true for open-heart surgery.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. Cycles can be cycled: $(a_1a_2a_3...a_k)(i) = (a_2a_3 ... a_k a_1)(i)$, and so on.\n\n>  <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThis follows directly from the definition of a cycle:\n$$\n(a_2 a_3 ... a_k a_1)(i) = \\begin{cases}\na_{i+1},\\ i \\neq 1, k \\\\\na_2, i = 1 \\\\\na_1,  i =k \\\\\ni,\\ i \\neq a_i\n\\end{cases}\n$$\nwhich corresponds exactly with the original definition of $(a_1 a_2 ... a_k)$.\n\n***\n\nAll of the above leads to the following result:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Disjoint cycle decomposition**. Every permutation $\\sigma$ in $S_n$ can be written as a composition of disjoint cycles in which $1, ..., n$ all appear, including $1$-cycles $(a_i)(i)$ which do not modify the position of any element. The expression\n$$\n\\sigma = (a^1_1 a^1_2 ... a_{k_1}^1) (a^2_1 a^2_2 ... a^2_{k_2}) ... (a^r_1 a^r_2 ... a^r_{k_r})\n$$\n> is unique barring a reordering of the elements within a cycle, or a reordering of the cycles themselves.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nProceed by induction on $S_n$. \n\nThe base case, $S_1$, consists of permutations on a set with a single element - this is already a $1$-cycle.\n\nAssume that all permutations in $S_{k}$, $k \\in \\mathbb{Z^+}$ and $k < n$, can be written as a composition of disjoint cycles. In order to prove that all permutations $\\sigma \\in S_n$ can be thus decomposed, our goal is to write\n$$\n\\sigma = \\tau \\circ \\rho\n$$\nwhere $\\tau$ is a disjoint cycle and $\\rho$ is a permutation on $S_{n-1}$ (or fewer elements), which by our strong induction assumption can be decomposed.\n\nTo proceed, write $\\tau = \\sigma \\circ \\rho^{-1}$. Consider the sequence\n$1, \\sigma(1), \\sigma(\\sigma(1)) = \\sigma^2(1), \\sigma^3(1), ..., \\sigma^n(1)$; what is the minimum value of $k$ such that $\\sigma^k(1) = 1$, and the sequence repeats?\n\nAs $1, 2, ..., n$ is a finite sequence of numbers, it is impossible that $\\sigma(1), ..., \\sigma^k(1), ...$ are distinct for all $k$; the sequence must repeat at some point. Suppose that $\\sigma^a(1) = \\sigma^b(1)$ for some $a,b$, assuming without loss of generality that $a > b$; then $\\sigma^a(1) = \\sigma^{a-b}\\sigma^b(1) = \\sigma^{a-b}\\sigma^a(1) = \\sigma^{a} \\sigma^{a-b}(1)$, yielding $\\sigma^{a-b}(1) = 1$ (as a permutation $\\sigma$ is one-to-one, and $\\sigma(a) = \\sigma(b)$ implies $a = b$.)\n\nIn addition to this sequence being finite, by the Pigeonhole Principle there are $n$ numbers from $1$ to $n$ and $k$ members of the sequence $\\sigma(1), ..., \\sigma^k(1)$; if $k > n$ there are $n+1$ numbers to fit into $n$ holes, and at least one value must be repeated. Thus $a, b < n+1$, and $a - b \\leq n$. $a-b$ is the first positive number satisfying $\\sigma^{a-b}(1) = 1$; call this number $k$, with $\\sigma^k(1) = 1$.\n\nThe sequence obtained from the above - $1, \\sigma(1), ..., \\sigma^{k-1}(1)$ - is a distinct sequence of numbers, of maximum length $n$ (as above): if we had $\\sigma^a(1) = \\sigma^b(1)$ with $k > a > b$, then as before $\\sigma^{a-b}(1) = 1$ with $a-b<k$, violating the assumption that $k$ is minimal.\n\nReturning to $\\tau = \\sigma \\circ \\rho^{-1}$, let $\\rho$ be the $k$-cycle given by the sequence\n$$\n\\rho = (1, \\sigma(1), ..., \\sigma^{k-1}(1)). \n$$\nAs all permutations have an inverse, $\\tau$ is well-defined for all $\\sigma$. In addition, for $m = 0, 1, ..., k-1$ we have\n$$\n\\tau(\\sigma^m(1)) = \\sigma(\\rho^{-1}(\\sigma^m(1))) = \\sigma(\\sigma^{m-1}(1)) = \\sigma^m(1)\n$$\nMeaning that $\\tau$ is a permutation upon the subset of $\\{1, ..., n\\}$ that excludes $\\{1, \\sigma(1), ..., \\sigma^{k-1}(1)\\}$ - a subset of size strictly less than $n$. By our strong induction assumption, $\\tau$ is thus a composition of disjoint cycles whose expression contains all of $\\{1, ..., n\\}$ barring $\\{1, \\sigma(1), ..., \\sigma^{k-1}(1)\\}$ - and so is $\\sigma = \\tau \\circ \\rho$, $\\rho$ itself being a cycle whose expression contains all of $\\{1, \\sigma(1), ..., \\sigma^{k-1}(1)\\}$. The two combined give a composition of disjoint cycles where $\\{1, ..., n\\}$ appears in its entirety.\n***\n\nTo demonstrate that this decomposition is indeed unique, consider the following equality:\n$$\n(a^1_1 a^1_2 ... a^1_{k_1})...(a^r_1 a^r_2 ... a^r_{k_r}) = (b^1_1 b^1_2 ... b^1_{l_1}) ... (b^s_1 b^s_2 ... b^s_{l_s})\n$$\nwhere the two sides are each a decomposition of the permutation $\\sigma \\in S_n$. By the above, such a decomposition must contain all of $1, ..., n$; as such, $a^1_1 = b^{i_1}_{j_1}$ for some $i_1$, $j_1$. As permutations can be cycled, we have\n$$\n\\begin{aligned} \na^1_2 &= b^{i_1}_{j_1 + 1} \\\\\na^1_3 &= b^{i_1}_{j_1 + 2} \\\\\n\\vdots \\\\\na^1_{k_1} &= b^{i_1}_{j_1 + k_1 - 1} \\\\\na^1_{k_1 + 1} = a^1_1 &= b^{i_1}_{j_1 + k_1}\n\\end{aligned}\n$$\ncycling the indices if necessary; as such, $b^{i_1}_{j_1 + k_1} = b^{i_1}_{j_1}$ and all the terms in-between the two are unique, meaning that the cycle $b^{i_1}$ is also a $k_1$-cycle - in fact, by the above equalities, the cycle $b^{i_1}$ is the exact same cycle as the cycle $a^{1}$. As disjoint cycles are commutative, the two cycles can be cancelled off from the equality; the same logic applied to all the above cycles yields the uniqueness of a single decomposition. $\\square$\n\n***\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. If the permutation $\\sigma$ has disjoint cycle decomposition $(a^1_1 a^1_2 ... a^1_{k_1})...(a^r_1 a^r_2 ... a^r_{k_r})$, then the order of $\\sigma$ - i.e. the minimum value of $k$ such that $\\sigma^k$ is the identity permutation - is the least common multiple of $k_1, ..., k_r$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLet $\\text{ord}(\\sigma) = k$; consider $\\sigma^k = (a^1_1 a^1_2 ... a^1_{k_1})^k...(a^r_1 a^r_2 ... a^r_{k_r})^k$ (as disjoint cycles are commutative). A $k_i$-cycle has order $k_i$, and as such, for $\\sigma^k$ to be the identity we must have\n$$\n(a^{i}_1 a^i_2 ... a^i_{k_i})^k = \\text{identity}\n$$\nfor all $i$, meaning that all of $k_1, ..., k_r$ divide $k$. \n\n(The above is true as if one of $(a^{i}_1 a^i_2 ... a^i_{k_i})^k$ is** not** the identity, $\\sigma$ would necessarily modify the numbers $a^i_1, a^i_2, ..., a^i_{k_i}$ to have different order, leaving $\\sigma$ not equal to the identity.)\n\nAll that's left is to prove that $k$ is the lowest such common multiple. Let $l = \\text{lcm}(k_1, ..., k_r)$; then we have\n$$\n\\sigma^l = (a^1_1 a^1_2 ... a^1_{k_1})^l...(a^r_1 a^r_2 ... a^r_{k_r})^l\n$$\nwith\n$$\n(a^{i}_1 a^i_2 ... a^i_{k_i})^l = ((a^{i}_1 a^i_2 ... a^i_{k_i})^{k_i})^a = \\text{identity}\n$$\nfor some integer $a$, by definition of the least common multiple. Thus $l$ satisfies $\\sigma^l = \\text{identity}$; as the order of $\\sigma$ is defined as the **minimum** value of $k$ such that $\\sigma^k$ is the identity, and any multiple of the LCM yields the identity, the lowest possible value of $k$ is the LCM itself. $\\square$\n\n","n":0.022}}},{"i":92,"$":{"0":{"v":"Matrix Groups","n":0.707},"1":{"v":"> A lright, who let the linear algebra nerds out of the insane asylum again??\n\n## The general linear group\n\n> <span style=\"background-color: #03cafc; color: black;\">Remark</span>. In the following section, $\\mathbb{F}$ will denote either $\\mathbb{R}$ or $\\mathbb{C}$ in statements that are applicable to both real and complex matrices. This fits in nicely: $\\mathbb{R}$ stands for \"real numbers\", $\\mathbb{C}$ stands for \"complex numbers\", and $\\mathbb{F}$ stands for \"f*ck group theory\".\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **general linear group** of $n\\times n$ matrices, denoted $GL_{n}(\\mathbb{F})$, as a group with binary operation equal to matrix multiplication, identity equal to the $n\\times n$ identity matrix $I_n$, and elements encompassing the set of all invertible $n\\times n$ matrices\n$$\n\\{A \\in M_{n\\times n}(\\mathbb{F}): \\det A \\neq 0\\}\n$$\n> where $M_{n\\times n}(\\mathbb{F})$ denotes the set of all $n\\times n$ matrices with entries in $\\mathbb{F}$, and non-zero determinants imply invertibility.\n\nAs always, we'll churn out the group axioms at breakneck speed like we're Cardi B churning out flops for her new album: identity holds for $I_n$, invertibility holds because $\\det A \\neq 0$, closure holds from multiplicativity of the determinant, and associativity is inherited from matrix multiplication. (I'm now presenting to the emergency room with acute-onset left lung collapse.)\n\nWe note that the determinant serves as a convenient map between $GL_n(\\mathbb{F})$ and $\\mathbb{F}$ excepting $0$; in fact, we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. The determinant function $\\det: GL_n(\\mathbb{F}) \\to \\mathbb{F}\\backslash\\{0\\},\\ A \\to \\det A$ is a surjective homomorphism between the groups $GL_n(\\mathbb{F})$ and $\\mathbb{F} \\backslash 0$, both defined under multiplication.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nWe have, for matrices $A, B \\in GL_n(\\mathbb{F})$ and $A\\cdot_{GL}B$ denoting their product under matrix multiplication,\n$$\n\\det (A\\cdot_{GL}B) = \\det A \\cdot \\det B\n$$\nwhere $\\cdot$ denotes scalar multiplication. This satisfies the definition of a homomorphism, as $\\cdot$ is the binary operation defined for the group $\\mathbb{F} \\backslash \\{0\\}$. \n\nSurjectiveness derives by construction from the matrix\n$$\nA = \\begin{bmatrix}\nx & & & \\\\\n& 1 & & \\\\\n& & \\ddots & \\\\\n& & & 1\n\\end{bmatrix}\n$$\ni.e. the identity matrix with its top-left element replaced by an arbitrary $x \\in \\mathbb{F}$; the determinant of this matrix is $x$ regardless of which $x \\in \\mathbb{F}$ is chosen (except zero), so its image encompasses all of $\\mathbb{F}$.\n\nIn particular, the kernel of this homomorphism defined by the determinant is a subgroup of $GL_n(\\mathbb{F})$ with special properties; in a flash of inspiration borne from the fruits of their infinite wisdom, mathematicians have deemed it fit to be called\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **special linear group**. Denote by $SL_n(\\mathbb{F})$ the kernel of the determinant homomorphism mapping between $GL_n(\\mathbb{F}) \\to \\mathbb{F} \\backslash \\{0\\}$; as the identity of $\\mathbb{F} \\backslash \\{0\\}$ is $1$ under multiplication, $SL_n(\\mathbb{F})$ encompasses all $n\\times n$ matrices that have determinant $1$.\n\nAs $SL_n(\\mathbb{F})$ is the kernel of a homomorphism, it is a normal subgroup; an application of the **<sup>FIRST</sup> ISOMORPHISM THEOREM!!!** shows that\n$$\nGL_n(\\mathbb{F}) / SL_n(\\mathbb{F}) \\cong \\text{Im}(\\det) = \\mathbb{F}\\backslash \\{0\\}.\n$$\n\n## Group actions of matrix groups\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. **Left matrix multiplication action.** $GL_n(\\mathbb{C})$ (not $\\mathbb{F}!$) acts faithfully on $\\mathbb{C^n}$, the set of complex vectors in $n$-dimensional space, via left matrix multiplication:\n$$\n\\forall A \\in GL_n(\\mathbb{C}), \\mathbf{v} \\in \\mathbb{C^n}:\\ \\phi(A, \\mathbf{v}) = A\\mathbf{v}.\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThere are three requirements for defining a group action from a group $G$ on a set $X$: 1) the identity does not change any element it acts on (identity), 2) it maps all elements in $X$ to another element in $X$ (closure), and 3) it is associative (associativity).\n\nFor identity, we have $I\\mathbf{v} = \\mathbf{v}$ by definition of $I$; for closure, any $n\\times n$ matrix multiplied by a $n \\times 1$ vector yields a $n\\times 1$ vector still in $\\mathbb{C^n}$; and for associativity, $(AB)\\mathbf{v} = A(B\\mathbf{v})$ by associativity of matrix multiplication.\n\nAs for faithfulness, we need to prove that if a matrix keeps every vector in $\\mathbb{C^n}$ the same after multiplication, then it must be the identity matrix:\n$$\nA \\mathbf{v} = \\mathbf{v},\\ \\forall \\mathbf{v} \\implies A = I_n.\n$$\nA matrix is uniquely determined by how it acts on a basis of $\\mathbb{C^n}$, e.g. on the standard normal basis $(1,0,0,..,0),...,(0,0,0,..,1)$. If $A\\mathbf{v} = \\mathbf{v}$ for any $\\mathbf{v}$, then $A$ must map all the elements of this basis to themselves; and is thus the matrix with columns  $(1,0,0,..,0),...,(0,0,0,..,1)$, which is the identity matrix. Thus the left matrix multiplication action only acts as the identity when it *is* the identity, and is faithful.\n\nThis group action has two possible orbits. The first is formed by the zero vector $\\mathbf{0}$ alone: $A\\mathbf{0} = \\mathbf{0}$ for any matrix $A \\in GL_n(\\mathbb{C})$, and if $A\\mathbf{v} = \\mathbf{0}$, then we have $\\mathbf{v} = \\mathbf{0}$ because $A$ is invertible. The second involves every other vector, i.e. $\\mathbb{C^n} \\backslash \\{\\mathbf{0}\\}$: the general equation for any two $\\mathbf{v, w} \\in \\mathbb{C^n}$ and $A \\in GL_n(\\mathbb{C})$\n$$\nA\\mathbf{v} = \\mathbf{w}\n$$\nhas an infinite number of solutions, because there are $n \\times n = n^2$ different entries in $A$ and only $n$ equations to be solved for, i.e.\n$$\n\\begin{cases}\nA_{11}v_1 + A_{12}v_2 + ... + A_{1n}v_n = w_1  \\\\\nA_{21}v_1 + A_{22}v_2 + ... + A_{2n}v_n = w_2  \\\\\n\\vdots \\\\\nA_{n1}v_1 + A_{n2}v_2 + ... + A_{nn}v_n = w_n  \\\\\n\\end{cases}\n$$\nThere are $n^2$ variables and only $n$ equations, so a possible matrix $A$ will always exist as long as $\\mathbf{v, w} \\neq \\mathbf{0}$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. **Change of basis action.** $GL_n(\\mathbb{C})$ acts on the set of all complex $n\\times n$ matrices $M_{n\\times n}(\\mathbb{C})$ by conjugation (i.e. a change of basis): for $P \\in GL_n(\\mathbb{C})$ and $A \\in M_{n\\times n}(\\mathbb{C})$, the following group action\n$$\nP \\bullet A = PAP^{-1}\n$$\n> represents a change of basis of the matrix $A$ via the change of basis matrix $P$, and is well-defined.\n\nThe well-definedness of this group action originates from conjugation; changing the basis of a matrix $A$ via a change-of-basis matrix $P$ can be thought of as conjugating $A$ with $P$. The orbits of this action are matrices who represent the same linear map under different orthonormal bases.\n\n## Orthogonal matrix groups\n### The orthogonal group\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Denote $O(n)$ by the group of all $n\\times n$ (real) orthogonal matrices under matrix multiplication: $O(n) = \\{P \\in GL_n(\\mathbb{R}): PP^{T} = I_n\\}$, where $T$ denotes matrix transposition.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. $O(n)$ is a subgroup of $GL_n(\\mathbb{R})$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLet $A$ and $B$ be two $n\\times n$ orthogonal matrices: $A, B \\in O(n)$. If we can prove that $O(n)$ is non-empty (which is isn't, because the identity matrix $I_n$ is orthogonal), and that $AB^{-1} \\in O(n)$, $O(n)$ is a subgroup of $GL_n(\\mathbb{R})$ (as it is a group of real invertible matrices, and thus already a subset of $GL_n(\\mathbb{R})$).\n\nWe have\n$$\n(AB^{-1})^{-1} = BA^{-1} = BA^T = (B^T)^T A^T = (AB^T)^T = (AB^{-1})^T\n$$\nand thus $AB^{-1} \\in O(n)$. (For any matrix $P \\in GL_n(\\mathbb{R})$, we have\n$$\n(PP^T)_{ij} = P_{ik}P_{jk} = \\delta_{ij},\n$$\nusing the dreaded summation notation, as our condition for $P \\in O(n)$.)\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. Left matrix multiplication by an orthogonal matrix on two vectors $\\mathbf{x}$ and $\\mathbf{y}$ is an **isometry** of $\\mathbf{x}$ and $\\mathbf{y}$ under the dot product:\n$$\n(A\\mathbf{x}) \\cdot (A\\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y}.\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. (What is this \"Vectors and Matrices\" nonsense? Is this what the paupers are learning in school nowadays? Jeeves, take it away!)\n\nWe have\n$$\n(A\\mathbf{x}) \\cdot (A\\mathbf{y}) = (A\\mathbf{x})^T (A\\mathbf{y}) = \\mathbf{x}^T A^TA\\mathbf{y} = \\mathbf{x}^T \\mathbf{y} = \\mathbf{x\\cdot y}\n$$\nas, by definition of orthogonal matrices, $A^T A = I$. Furthermore we have\n$$\n(A\\mathbf{x}) \\cdot (A\\mathbf{x}) = |A\\mathbf{x}|^2 = \\mathbf{x}\\cdot \\mathbf{x} = |\\mathbf{x}|^2\n$$\nand since magnitudes are non-negative, taking the square root of both sides gives $|A\\mathbf{x| = |x|}$. \n\nAs an aside, multiplication by $A$ also preserves both the distance and the angle between $\\mathbf{x}$ and $\\mathbf{y}$. This originates from\n$$\n\\begin{aligned}\n|A(\\mathbf{x - y})|^2 &= (A(\\mathbf{x-y}))^T(A(\\mathbf{x-y})) \\\\\n&= (\\mathbf{x-y})^T A^TA(\\mathbf{x-y}) \\\\\n&= (\\mathbf{x-y})^T(\\mathbf{x-y}) \\\\\n&= |\\mathbf{x-y}|\n\\end{aligned}\n$$\nand\n$$\n\\begin{aligned}\n\\cos(\\angle(A\\mathbf{x},A\\mathbf{y})) = \\frac{(A\\mathbf{x})\\cdot(A\\mathbf{y})}{|A\\mathbf{x}||A\\mathbf{y}|} = \\frac{\\mathbf{x}\\cdot \\mathbf{y}}{|\\mathbf{x}||\\mathbf{y}|} = \\cos(\\angle(\\mathbf{x},\\mathbf{y}))\n\\end{aligned}\n$$\nwith equality between the angles because $\\cos$ is a one-to-one function between $0$ and $\\pi$.\n\n### Rotations and reflections\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. We observe that, much like the general linear group, the determinant $\\det A$ is again a surjective homomorphism between $O(n) \\to \\{-1,1\\}$; define the **special orthogonal group** as the group of matrices which form the kernel of this homomorphism, i.e. all orthogonal matrices $A$ such that $\\det A = 1$. This group is denoted by $SO(n)$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. $SO(2)$ is the group of rotation matrices in $\\mathbb{R^2}$ about the origin.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nSuppose that $A \\in SO(2)$; then $AA^T = I$ and $\\det A = 1$. Brute-forcing these conditions on an arbitrary matrix $A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$ yields\n$$\n\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} a & c \\\\ b & d \\end{bmatrix} = \\begin{bmatrix} \na^2 + b^2 & ac + bd \\\\\nac + bd & c^2 + d^2\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n$$\nto achieve orthogonality, yielding\n$$\n\\begin{cases}\na^2 + b^2 = c^2 + d^2 = 1\\\\\nac+ bd = 0 \\\\\n\\det A = ad - bc = 1\n\\end{cases}\n$$\nwith the last equation originating by definition of $SO(2)$. As $a, b, c,d$ are real, their squares are non-negative; and thus all of $a, b, c, d \\in [-1, 1]$, and can be parameterized by $\\pm \\sin \\theta$ and $\\pm \\cos \\theta$. ($\\sin$ and $\\cos$ map onto $[-1, 1]$ surjectively and $\\pm \\cos \\theta$ is the only solution for $x$ to $\\sin^2 \\theta + x^2 = 1$ among the real numbers.)\n\nSuppose, without loss of generality, that $a = \\cos \\theta$ and $b = \\pm \\sin \\theta$; a quick substitution into the second equation gives\n$$\nc \\cos \\theta \\pm d \\sin \\theta = 0\n$$\nwhich leads to $d = \\pm c \\cot \\theta$, and thus $c^2 + d^2 = c^2 \\csc^2 \\theta = 1$ yielding $c = \\pm \\sin \\theta$ and $d = \\pm \\cos \\theta$. To determine their signs, consider the second equation again: $ac$ and $bd$ must multiply to opposite signs, so either $c$ is $\\sin \\theta$ and $b$ and $d$ have opposite signs or $c$ is $-\\sin \\theta$ and $b$ and $d$ have the same sign. Checking the last equation shows that $ad = \\cos^2 \\theta$, and $bc = -\\sin^2 \\theta$; $d$ must be $\\cos \\theta$ and $b$ and $c$ have opposite sign. Combining this information gives us either\n$$\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{bmatrix}\n$$\nor\n$$\n\\begin{bmatrix}\n\\cos \\theta & \\sin \\theta \\\\\n-\\sin \\theta & \\cos \\theta\n\\end{bmatrix}\n$$\nwhich describe a rotation about the origin by an angle $\\theta$ counterclockwise and clockwise, respectively.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. A matrix in $O(2)$ is either a rotation about the origin or a reflection across a line through the origin.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nFrom the above, we know that a subgroup of $O(2)$ - $SO(2)$ - is a group of all rotation matrices in $\\mathbb{R^2}$ about the origin; $SO(2)$ must be half the order of $O(2)$ as the bijection\n$$\n\\phi(\\begin{bmatrix}\na & b \\\\\nc& d\n\\end{bmatrix}) = \\begin{bmatrix}\n-a & -b \\\\\nc & d\n\\end{bmatrix}\n$$\nestablishes a one-to-one correspondence between $SO(2)\\backslash O(2)$ and $O(2)$, where the determinant of the matrix on the right-hand side is $-ad - bc = -(ad - bc)$, equalling $-1$ times the determinant of the left-hand side matrix. Thus, the left cosets of $SO(2)$ partition $O(2)$ into two sets of equal size, with the other coset being matrices of determinant $-1$; these matrices are in the general form\n$$\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n-\\sin \\theta & -\\cos \\theta\n\\end{bmatrix}.\n$$\nConsider the effects of left multiplication of this matrix on a vector in $\\mathbb{R^2}$, $\\mathbf{x} = \\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}$:\n$$\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n-\\sin \\theta & -\\cos \\theta\n\\end{bmatrix}\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix} = \\begin{bmatrix}\nx \\cos \\theta - y \\sin \\theta \\\\\n-x \\sin \\theta - y\\cos \\theta\n\\end{bmatrix}\n$$ \nWe know that a reflection over a line normal to some vector $\\mathbf{a} = \\begin{bmatrix}a \\\\ b \\end{bmatrix}$ takes the form\n$$\nR_{\\mathbf{a}}(\\mathbf{x}) = \\mathbf{x} - 2(\\mathbf{x \\cdot a})\\mathbf{a} = \\begin{bmatrix}\nx - 2(ax+by)a \\\\\ny - 2(ax+by)b\n\\end{bmatrix} = \n\\begin{bmatrix}\n(1-2a^2)x - 2aby\\\\\n(1-2b^2)y - 2abx\n\\end{bmatrix} \n$$\nwhere setting $2ab = \\sin \\theta$, $1 - 2a^2 = \\cos \\theta$, and $1 -2b^2 = -\\cos\\theta$ yields the desired result that the matrix represents a reflection.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. As every orthogonal matrix in $O(2)$ is thus either a reflection or a rotation, all rotations are products of two reflections (as if $\\det A = \\det B = -1$, then $\\det (AB) = 1$ and $AB \\in SO(2)$, making $AB$ a rotation).\n\nRotations in $\\mathbb{R^3}$ are also characterized by the special orthogonal matrix group $SO(3)$:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. Every matrix $A \\in SO(3)$, i.e. all orthogonal $3\\times 3$ matrices with determinant $1$, jis a rotation when viewed through some basis in $\\mathbb{R^3}$: in other words, it is conjugate to the general rotation matrix about the $z$-axis\n$$\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta & 0 \\\\\n\\sin \\theta & \\cos \\theta & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. TL;DR - Prove that any matrix $A$ in $SO(3)$ has $1$ as an eigenvalue; consider $A$ through its basis of eigenvectors; show that this matrix under an eigenvector change of basis is a rotation matrix.\n\nFirst, we would like to show that any matrix $A \\in SO(3)$ has the eigenvalue $1$; supposing that this eigenvector is $v$, in an orthonormal basis $(v_1, v_2, v)$ we have (without loss of generality)\n$$\nAv = v\n$$\nand so the last column of $A$ is in the form $(0,0,1)$; furthermore, we have\n$$\n(Av_1) \\cdot (v) = (Av_1) \\cdot (Av) = v_1 \\cdot v = 0\n$$\nby orthonormality and isometry, with a similar result for $v_2$, leading to $Av_1$ and $Av_2$ being independent of $v$. Thus, the expression for the first two columns of $A$ in this basis have no entry in the third row (representing $v$).\n\nShowing that $A$ has an eigenvalue $1$ is equivalent to showing that $\\det (A - I) = 0$; recalling that $A^T A = I$, this is accomplished via\n$$\n\\begin{aligned}\n\\det (A - I) &= \\det (A - AA^T) \\\\\n&= \\det (A (I - A^T)) \\\\\n&= \\det A \\det (I - A^T) \\\\\n&= \\det (I - A^T),\\ \\text{as $A \\in SO(3)$} \\\\\n&= \\det ((I-A)^T) \\\\\n&= \\det (I - A),\\ \\text{as $\\det A^T = \\det A$} \\\\\n&= -\\det(A - I)\n\\end{aligned}\n$$\nleading to $\\det (A - I) = 0$. Thus $A$ has the form\n$$\n\\begin{bmatrix}\na & b & 0 \\\\\nc & d & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\nwith $\\det A = 1$ leading to $ad - bc = 1$; and so the matrix formed by\n$$\n\\begin{bmatrix}\na & b\\\\\nc & d\n\\end{bmatrix}\n$$\nis an element of $SO(2)$, and can thus be written\n$$\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{bmatrix}\n$$\nas desired.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. Every element in $O(3)$ can be written as the product of at most three reflections.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>\n\nAs with $O(2)$, $SO(3)$ is a subgroup of index $2$ of $O(3)$ and thus partitions $O(3)$ into two left cosets: $SO(3)$ itself, which includes all matrices with determinant $-1$, and $O(3)\\backslash SO(3)$, which includes all matrices with determinant $1$.\n\nIf a matrix $A \\in SO(3)$, then we have\n$$\nA = \\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta & 0 \\\\\n\\sin \\theta & \\cos \\theta & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix} = \n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix} \n\\begin{bmatrix}\n\\cos \\theta & \\sin \\theta & 0 \\\\\n\\sin \\theta & -\\cos \\theta & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix} \n$$\nwhich is a composition of two reflections, one about the $y$-axis and the other a reflection as verified for $SO(2)$ (except holding the $z$-axis constant). If instead a matrix $B$ lies in the other left coset $O(3)\\backslash SO(3)$, then it can be written in the form\n$$\nB= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix} A\n$$\nwhere $A \\in SO(3)$ (as this matrix has determinant $-1$, and is thus in $O(3)\\backslash SO(3)$.) $A$ is a product of two reflections, and so $B$ is a product of at most three reflections. \n","n":0.02}}},{"i":93,"$":{"0":{"v":"The Mobius Group","n":0.577},"1":{"v":"> M y favorite part of the entire course was when M$\\ddot o$bius said \"IT'S M$\\ddot O$BIN' TIME\" and m$\\ddot o$bed all over the lecture notes.\n\n## M$\\ddot o$bius transformations\n\n(The $o$ with an umlaut is not to be confused with the second derivative of $o$.)\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **Möbius map** $f: \\mathbb{C \\to C}$ is defined as $f(z) = \\frac{az+b}{cz+d}$ for $a, b, c, d\\in \\mathbb{C}$, $ad - bc \\neq 0$. (If $ad-bc = 0$, then we would have\n\n$$\n\\begin{aligned}\nz,w \\in \\mathbb{C},\\ f(z)-f(w) &= \\frac{az+b}{cz+d}-\\frac{aw+b}{cw+d} \\\\ \n&= \\frac{(az+b)(cw+d)-(aw+b)(cz+d)}{(cz+d)(cw+d)} \\\\\n&= \\frac{adz + bcw - adw - bcz}{(cz+d)(cw+d)} \\\\\n&= \\frac{(ad-bc)(z-w)}{(cz+d)(cw+d)} \\\\\n&= 0\\text{ if }ad-bc=0\n\\end{aligned}\n$$\n> which is discarded for committing the cardinal sin of being too boring.)\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **Riemann sphere.** (This sounds like a pokeball used to catch mathematicians or something.) The extension of the complex plane via the introduction of an \"infinity element\" - $\\mathbb{C} \\cup \\{\\infty\\}$ - is known as the **Riemann sphere**, denoted by $\\mathbb{\\hat C}$.\n\nThe Möbius map defined above is only well-defined everywhere in the Riemann sphere and not in $\\mathbb{C}$, as $f(-\\frac{d}{c})$ would result in division by zero; we circumvent this by giving it the middle finger and saying \"dividing by zero is okay as long as we assign a fancy symbol to it, cover it in white cloth and never attempt to make eye contact with what comes out the other end\", i.e. $f(-\\frac{d}{c}) = \\infty$ where $\\infty$ is *not* to be treated as some mystical, impenetrable monument of math, but a number we can do calculations with. Similarly, we **define** $f(\\infty) = \\lim_{z\\to \\infty} f(z) = \\frac{a}{c}$. \n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. **Extension of the Möbius map to $\\mathbb{\\hat C}$.** (Guess how many times I've had to search up \"copy o with an umlaut to clipboard\" by now.) The Möbius map is well-defined everywhere in the Riemann sphere, with the following values being specially defined:\n1. $f(\\infty) = \\lim_{z\\to\\infty}\\frac{az+b}{cz+d} = \\frac{a}{c}$.\n2. $f(-\\frac{d}{c}) = \\infty$.\n3. $f(\\infty) = \\infty$ when $c = 0$ (derived from the first two definitions).<br/><br/>\n(Even though we have defined these special conventions, the notation $\\frac{az+b}{cz+d}$ will still be used interchangeably with $f$ with the understanding that the above properties hold only for the Möbius map as we have defined it.)\n\n\nOne last point to wrangle with a little bit: why is the Riemann sphere called a sphere when, as far as we know, there are no spheres visible anywhere to the naked eye? First off, Riemann must have had some really small spheres when ~~viewed naked~~ viewed through the naked eye; but second of all, consider a unit sphere centered about the origin -\n$$\nS_2 = \\{(x,y,z) \\in \\mathbb{R}: x^2 + y^2 + z^2 = 1\\}\n$$\nSuppose you're standing at the very top of this sphere - the point $(0,0,1)$, the *north pole* of the sphere - and that the complex plane (which we will consider the $xy$-plane) is sitting at the equator of this sphere. If you angle your field of vision in a particular direction - for instance, through a point $p$ on the sphere - is it possible to see any possible point on the complex plane?\n\n![alt text](./assets/images/image-83.png)\n\nThe answer is *yes*; for every point $r(p)$ on the complex plane, a line can be drawn through the north pole to a general point $p$ on the sphere, and this point $p$ is uniquely determined by $r(p)$, with there only being one possible line through\n$$\n(r(p)_x, r(p)_y, 0) \\to (0,0,1)\n$$\nwhich will only intersect the sphere at one point, and vice versa, with $\\infty$ being ampped to the point $(0,0,1)$. I suppose this explanation for why a sphere is actually a plane does motivate the study of Möbius maps a bit, but it only considers the top half of the sphere - it's a little one-sided. ~~*(Get it? Because the Möbius strip has only one side? HA HA HA HA HA HA HA - i'll see myself out bye*~~)\n\n(This technique is called **stereographic projection**, and is used to convert a globe of the Earth into flat maps. This means that the Earth is actually neither flat nor round, but is instead a secret third thing that exists as a bijection between the flat earthers and the globalists.)\n\n## Group properties of the Möbius map\n\n>  As Möbius famously said when referring to himself, \"persons who have a decided mathematical talent constitute a superior class to everyone else.\" Bold words for someone whose most lasting contribution to mankind is a strip of paper that looks like it was chewed on by a 5-year-old.\n\nA brief TL;DR - why study Möbius maps? Because Möbius maps are bijections from the Riemann sphere to itself, and because of that, Möbius maps are invertible and form a group; and because of *that*, Möbius maps are a group action on the Riemann sphere (the extended complex plane) that turn out to be the only group actions that preserve properties like angles and structures like circles.\n\n### The Möbius maps form a group\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. Möbius maps form a bijection from $\\mathbb{\\hat C}$ to $\\mathbb{\\hat C}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nAs previously defined, the Möbius map takes the entirety of $\\mathbb{\\hat C}$ as its domain; we need only prove that the map is invertible. Denote the inverse of the Möbius map as $y=f^{-1}(z)$; then we have\n$$\n\\begin{aligned}\nf(y) &= z \\\\\n\\frac{ay + b}{cy + d} &= z \\\\\nay + b &= czy + dz \\\\\n(a-cz)y &= dz - b \\\\\ny &= \\frac{dz-b}{a-cz} \n\\end{aligned}\n$$\nwhich is again a Möbius map and can be verified to be the inverse of $f$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. Let $M$ be the set of all Möbius maps (or, if you like, the set of matrices $M = \\begin{bmatrix} a&b \\\\c&d\\end{bmatrix}$ that map - not one-to-one! - onto $f(z) =\\frac{az+b}{cz+d}$, with $ad-bc = \\det M \\neq 0$.) Then $M$ forms a group under function composition.\n\nSide note: the group of all matrices with $\\det M \\neq 0$ is simply $GL_2$, so we are done! (Actually, we're not - we'll see later that these two groups aren't isomorphic.) But alas, the alluring sway of masochism compels us once again to deliver a\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n1. Closure. This one is a pain. Let $f(z) = \\frac{a_1 z + b_1}{c_1 z + d_1}$ and $g(z) = \\frac{a_2 z + b_2}{c_2 z + d_2}$ both be elements of $M$, with $a_1d_1 - b_1d_1 \\neq 0$ and $a_2d_2 - b_2c_2 \\neq 0$. Then we have\n$$\n\\begin{aligned}\nf(g(z)) &= \\frac{a_1 (\\frac{a_2 z + b_2}{c_2 z + d_2}) + b_1 }{c_1 (\\frac{a_2 z + b_2}{c_2 z + d_2}) + d_1} \\\\\n&= \\frac{a_1a_2z + a_1b_2 + b_1c_2z + b_1d_2}{c_1a_2z + c_1b_2 + d_1c_2z + d_1d_2} \\\\\n&= \\frac{(a_1a_2 + b_1c_2)z + (a_1b_2+b_1d_2)}{(c_1a_2 + d_1c_2)z + (c_1b_2+d_1d_2)}\n\\end{aligned}\n$$\nwhich is again an element of $M$, as we have\n$$\n\\begin{aligned}\n(a_1a_2 + b_1c_2)(c_1b_2 + d_1d_2) - (a_1b_2 + b_1d_2)(c_1a_2 + d_1c_2) \\\\\n=  a_1a_2d_1d_2 + b_1c_2c_1b_2 - a_1b_2d_1c_2 - b_1d_2c_1a_2 \\\\\n= b_1c_1(c_2b_2 - d_2a_2) + a_1d_1(a_2d_2 - b_2c_2) \\\\\n= (a_2 d_2 - b_2 c_2)(a_1 d_1 - b_1 c_1) \\neq 0.\n\\end{aligned}\n$$\n1. Identity. We simply have $1(z) = z$ as the identity under function composition.\n2. Inverse. The Möbius map is invertible, as shown above, and its inverse is another Möbius map.\n3. Association is inherited from function composition.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. We mentioned earlier that $GL_2$ has some relationship to $M$; and indeed, with every matrix $A = \\begin{bmatrix} a&b \\\\c&d\\end{bmatrix}$ we could define a function $\\phi$ mapping from $A$ to a Möbius map in $M$, like so - \n$$\n\\phi(A) = \\frac{az + b}{cz +d}.\n$$\n> However, such a function is **not** one-to-one: $\\phi(\\lambda A) = \\phi(\\lambda)$ for any constant $\\lambda$. Thus, $\\phi$ is a **homomorphism** rather than an isomorphism between $GL_2(\\mathbb{C}) \\to M$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nFor two matrices $A = \\begin{bmatrix} a_1&b_1 \\\\c_1&d_1\\end{bmatrix}$ and $B = \\begin{bmatrix} a_2&b_2 \\\\c_2&d_2\\end{bmatrix}$ in $GL_2(\\mathbb{C})$ (i.e. nonzero determinant), we have\n$$\n\\phi(A) \\circ_M \\phi(B) = \\frac{(a_1a_2 + b_1c_2)z + (a_1b_2+b_1d_2)}{(c_1a_2 + d_1c_2)z + (c_1b_2+d_1d_2)}\n$$\nas shown above, and\n$$\nAB = \\begin{bmatrix}\na_1 a_2 + b_1 c_2 & a_1 b_2 + b_1 d_2 \\\\\nc_1 a_2 + d_1 c_2 & c_1 b_2 + d_1 d_2\n\\end{bmatrix}\n$$\ncorresponding to the above Möbius map, as desired. This homomorphism is surjective, because for any Möbius map a matrix formed out of its coefficients can be constructed (not necessarily uniquely); its kernel is the group of multiples of the identity matrix $Z= \\{\\lambda I: \\lambda \\in \\mathbb{C}, \\lambda \\neq 0\\}$, with the First Isomorphism Theorem giving $M \\cong GL_2(\\mathbb{C})/Z$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. All  Möbius maps can be writte as a finite composition of the following simple Möbius maps, in some order and potentially with repeats:\n1. The enlargement/dilation/rotation map, $f(z) = az$ for some $a \\in \\mathbb{C}$.\n2. The translation map, $f(z) = z + b$ for $b \\in \\mathbb{C}$.\n3. The inversion map, $f(z) = \\frac{1}{z}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThe general form of a Möbius map is $f(z) = \\frac{az+b}{cz+d}$. $f(\\infty) = \\infty$ if and only if $c = 0$; when $c=0$, we have\n$$\nf(z) = \\frac{az+b}{d} = \\frac{a}{d}z + \\frac{b}{d}\n$$\nwhich is clearly a dilation by $\\frac{a}{d}$ combined with a translation by $\\frac{b}{d}$. If instead $c\\neq 0$, let $f(\\infty) = z_0$; then the following construction\n$$\nh(f(z)) = \\frac{1}{f(z)-z_0},\\ h(f(\\infty))=\\frac{1}{z_0-z_0} = \\infty\n$$\nhas $h(f(z)) \\in M$ due to closure, and $h(f(z)) = g(z)$ being a Möbius map with $c=0$ due to the first statement. We know that a Möbius map $g(z)$ with $c=0$ is composed of (at most) two simple Möbius maps, a dilation and a translation; thus we have\n$$\nf(z) = h^{-1}(g(z))\n$$\nwhere $h^{-1}(z) = \\frac{1}{z} + z_0$, and is thus composed of an inversion plus a translation. Therefore, any $f(z)$ is composed of the above three simple Möbius maps.\n\n### Fixed points of Möbius maps\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **fixed point** of a Möbius map $f$ is a complex number $z \\in \\mathbb{C}$ such that $f(z) = z$.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. If a Möbius map has three or more fixed points, it is the identity $f(z) = z$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nA fixed point of a Möbius map is equivalent to a complex root of the quadratic equation\n$$\nf(z) = \\frac{az + b}{cz + d} = z,\\ cz^2 +(d-a)z -b =0\n$$\nwhich has at most two complex roots by the Fundamental Theorem of Algebra, unless the left-hand side is identically zero, in which case $c= 0$, $d=a$ and $b =0$ yielding $f(z) = \\frac{az}{a} = z$. (This is what mathematicians call \"nuking an anthill.\") \n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. Any Möbius map is conjugate under function composition to either a dilation $f(z) = \\nu z$ or the simple translation $f(z) = z +1$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nAs shown above, the general linear group of $2\\times 2$ complex matries $GL_2(\\mathbb{C})$ is surjectively homomorphic to the group of Möbius maps $M$. For this surjective homomorphism $\\phi$, we have\n$$\nA, B, P \\in GL_2(\\mathbb{C}),\\ PAP^{-1} = B\n$$\nimplying\n$$\n\\phi(PAP^{-1}) = \\phi(P)\\circ \\phi(A)\\circ \\phi(P^{-1}) = \\phi(B)\n$$\nand as such $\\phi(A)$ and $\\phi(B)$ are conjugates in $M$. As the conjugacy classes partition $GL_2(\\mathbb{C})$ and $GL_2(\\mathbb{C})$ map surjectively onto $M$ via $\\phi$, the conjugacy classes of $GL_2(\\mathbb{C})$ completely describe the conjugacy classes of $M$.\n\nFrom [[Linear Algebra.Eigenvalues and Eigenvectors.Canonical Forms]] we know that the conjugacy classes of $GL_2(\\mathbb{C})$ are of the following three types: $\\begin{bmatrix}\\lambda & 0 \\\\ 0 &\\lambda \\end{bmatrix}$, $\\begin{bmatrix}\\lambda & 0 \\\\ 0 &\\mu \\end{bmatrix}$, and $\\begin{bmatrix}\\lambda & 1 \\\\ 0 &\\lambda \\end{bmatrix}$ for $\\lambda, \\mu \\in \\mathbb{C}$. Applying the homomorphism above gives us the Möbius maps \n$$\n\\begin{cases}\n\\frac{\\lambda z + 0}{0 + \\lambda} = z = 1z, \\\\\n\\frac{\\lambda z + 0}{0 + \\mu} = \\frac{\\lambda}{\\mu} z,\\\\\n\\frac{\\lambda z + 1}{\\lambda} = z + \\frac{1}{\\lambda}\n\\end{cases}\n$$\nwhere the final matrix $\\begin{bmatrix}\\lambda & 1 \\\\ 0 &\\lambda \\end{bmatrix}$ can be rewritten $\\begin{bmatrix}1 & \\frac{1}{\\lambda} \\\\ 0 &1 \\end{bmatrix}$, giving us $f(z) = z+1$. $\\square$\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. A Möbius map $f$ has a fixed point $z$ if and only if its conjugate $gfg^{-1}$ for some $g \\in M$ has a fixed point $g(z)$; as such, all non-identity Möbius maps have either $1$ fixed point (if conjugate to $f(z) = z+1$) or $2$ fixed points (if conjugate to $f(z)=\\nu z$).\n\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThe first statement can be demonstrated directly via\n$$\nf(z) = z \\implies gfg^{-1}(g(z)) = gf(g^{-1}(g(z))) = gf(z) = g(z)\n$$\nand as such $g(z)$ is a fixed point of $gfg^{-1}$; conversely\n$$\ngfg^{-1}(g(z)) = g(z) \\implies gf(z) = g(z) \\implies f(z) = z.\n$$\nThus, if a Möbius map $f \\in M$ is conjugate to $f(z) = \\nu z$; this fixes $0$ and $\\infty$ only, and thus $f$ has two fixed points. Alternatively, if $f \\in M$ is conjugate to $f(z) = z+1$, then this fixes $z= \\infty$ only and thus $f$ has only one fixed point.\n\n### Group actions of Möbius maps\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The group of Möbius maps acts faithfully and transitively on the Riemann sphere $\\mathbb{\\hat C}$ through $f \\in M, z\\in \\mathbb{\\hat C}: \\phi(f,z) = f(z)$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nCheck the three properties of a group action:\n1. Identity: we have $1(z) = z$ as the identity of $M$, and $\\phi(1(z), z) = z$ for all $z \\in \\mathbb{\\hat C}$.\n2. Closure: all Möbius maps map from $\\mathbb{\\hat C}\\to \\mathbb{\\hat C}$ bijectively, and thus for all $z_1, z_2 \\in \\mathbb{\\hat C}$ we have\n$$\nf(z) = z + z_2 - z_1,\\ f(z_1) = z_2 \n$$\ndemonstrating transitivity, as well as\n$$\n\\forall z,\\ f(z) = z\n$$\nimplying that $f$ is the identity by default.\n\n3. Associativity: true by definition of function composition.\n\nIn fact, it is not only true that for any *pair* of points $z_1$ and $z_2$ we can find (often an infinite number) of Möbius maps that send $z_1$ to $z_2$; we have a far stronger result as follows.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Call a group action $\\phi$ of $G$ on $X$ ***three-transitive*** if for any two triples $(x_1, x_2, x_3)$ and $(y_1, y_2, y_3)$, all in $X$, we have some $g\\in G$ such that $g(x_1)=y_1$, $g(x_2)=y_2$ and $g(x_3) = y_3$ - that is to say, an element exists that sends any triplet to another arbitrary triplet under this group action. If this $g\\in G$ is unique, call the group action **sharply three-transitive**.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. $M$ acts sharply three-transitively on $\\mathbb{\\hat C}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThe statement is true as long as we can show that there exists a Möbius map which maps any three points $(x_1, x_2, x_3)$ in $\\mathbb{\\hat C}$ to $(0, 1, \\infty)$ - the special points of the Möbius map - and that there exists a Möbius map which maps these three points to any $(y_1, y_2, y_3)$.\n\nFor the first such Möbius map, consider\n$$\nf(z) = \\frac{(z-x_1)(x_2-x_3)}{(z-x_3)(x_2-x_1)}\n$$\nwhich maps $x_1 \\to 0$, $x_2 \\to 1$, $x_3 \\to \\infty$. For the second, the Möbius map \n$$\ng(z) = \\frac{(z-y_1)(y_2-y_3)}{(z-y_3)(y_2-y_1)}\n$$\nmaps $y_1 \\to 0$, $y_2 \\to 1$ and $y_3 \\to \\infty$ instead; as such, $g^{-1}(z)$ will map $0 \\to y_1$, $1 \\to y_2$, and $\\infty \\to y_3$, and so $g^{-1}f(z)$ satisfies our condition. We know that this function is unique because\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. Any Möbius map $f \\in M$ is uniquely determined by three points; if $f(z_i)=g(z_i)$ for $i = 1, 2, 3$, then $f = g$. This can be directly shown via $fg^{-1}(z_i) = z_i$, leading to three fixed points for the Möbius map $fg^{-1}$ and thus leading to $fg^{-1}$ being the identity.\n\nThis verifies that $M$ acts sharply three-transitively on $\\mathbb{C}$. $\\square$\n\n### Möbius maps preserve geometric structures\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. Both circles and lines in the complex plane can be generally represented by the locus of complex numbers $z$ such that\n$$\nAz\\bar z + \\bar B z + B \\bar z + C = 0\n$$\n> where $A, C \\in \\mathbb{R}$, $B \\in \\mathbb{C}$, and $|B|^2 > AC$; $A=0$ gives a straight line, while $A \\neq 0$ gives a circle.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nWe know that the complex-number representation of a circle centered at a point $B$ with radius $r$ is\n$$\n|z-B| = r\n$$\nwhich can be equivalently written as\n$$\n|z-B|^2 = (z-B)\\overline{(z-B)} = r^2\n$$\nas $|z-B|$ is always non-negative. This yields\n$$\n\\begin{aligned}\n(z-B)(\\bar z - \\bar B) &= r^2 \\\\\nz\\bar z - z \\bar B - B \\bar z - B \\bar B &= r^2 \\\\\nz\\bar z - \\bar B z - B\\bar z - (B\\bar B + r^2) &= 0\n\\end{aligned}\n$$\nwhich corresponds to the given form, as $B\\bar B$ and $r^2$ are both real. \n\nOn the other hand, the locus of all complex numbers that form a line which perpendicularly bisects the line segment between $a$ and $b$ can be written\n$$\n|z-a| = |z-b|\n$$\nor \n$$\n(z-a)(\\bar z - \\bar a) = (z-b)(\\bar z - \\bar a)\n$$\nwhich reduces to the form above, with the $z\\bar z$ term cancelling out.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. A Möbius map maps any circle or line in the complex plane to another circle or line in the complex plane (not necessarily respectively, i.e. circles can be mapped to lines, and vice versa).\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nAny Möbius map is a composition of the three elementary Möbius maps: a dilation $f(z) = \\nu z$, a translation $f(z) = z + a$, and an inversion $f(z) = \\frac{1}{z}$. As such, we only need to show that all three maps preserve the equation\n$$\nAz\\bar z + \\bar B z + B \\bar z + C = 0.\n$$\nFor a dilation: $z \\to \\nu z$ gives\n$$\nA\\nu\\bar z + \\bar B \\nu z + B\\bar \\nu \\bar z + C = 0\n$$\nwhich is still in the required form, as $\\overline{(\\bar B \\nu)} = B \\bar \\nu$. \n\nFor a translation: $z \\to z + a$ gives\n$$\n\\begin{aligned}\nA(z+a)(\\bar z + \\bar a) + \\bar B (z+a) + B (\\bar z + \\bar a) + C \\\\\n= A(z\\bar z + \\bar a z + a \\bar z + a \\bar a) + \\bar Bz + \\bar Ba + B\\bar z + B \\bar a + C \\\\\n= Az \\bar z + (A\\bar a + \\bar B)z + (Aa + B)\\bar z + (Aa\\bar a + \\bar B a + B \\bar a + C)\n\\end{aligned}\n$$\nwhich corresponds to the form above because $\\overline{(A\\bar a + \\bar B)} = Aa + B$, from $A$ being real and thus $\\bar A = A$.\n\nFinally, for an inversion $z \\to \\frac{1}{z}$ we have\n$$\n\\begin{aligned}\nA\\frac{1}{z\\bar z} + \\bar B \\frac{1}{z} + B \\frac{1}{\\bar z} + C = 0 \\\\\nA + \\bar B \\bar z + B z + C \\bar z z = 0\n\\end{aligned}\n$$\nwhere the coefficients have swapped places, but the equation remains the same. $\\square$\n\n### Möbius maps preserve the cross-ratio\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Define the **cross-ratio** between four distinct complex numbers $z_1, z_2, z_3, z \\in \\mathbb{\\hat C}$ as the unique Möbius map that maps $z_1 \\to \\infty$, $z_2 \\to 0$ and $z_3 \\to 1$ as a function of $z$:\n$$\n[z_1, z_2, z_3, z] = f(z) = \\frac{z - z_2}{z - z_1} \\cdot \\frac{z_3 - z_1}{z_3 - z_2}\n$$\n> which exists uniquely as a result of $M$ acting sharply three-transitively on $\\mathbb{\\hat C}$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The cross-ratio $[z_1, z_2, z_3, z]$ is preserved by a Möbius map: $[f(z_1), f(z_2), f(z_3), f(z)] = [z_1, z_2, z_3, z]$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLet $\\lambda = g(z)$ be the unique Möbius map that maps $z_1 \\to \\infty$, $z_2 \\to 0$, and $z_3 \\to 1$, such that $z$ is mapped to $\\lambda$ under the operation $[z_1,z_2,z_3,z]$. Consider the Möbius map formed by $gf^{-1}$, which has\n$$\n\\begin{cases}\ngf^{-1}(f(z_1)) = g(z_1) = \\infty \\\\\ngf^{-1}(f(z_2)) = g(z_2) = 0 \\\\\ngf^{-1}(f(z_3)) = g(z_3) = 1 \\\\\ngf^{-1}(f(z)) = g(z) = \\lambda\n\\end{cases}\n$$\nBy definition $gf^{-1}$ is the Möbius map formed from the cross-ratio $[f(z_1), f(z_2), f(z_3), f(z)]$, which means that the cross-ratio evaluates to $gf^{-1}(f(z)) = g(z)$. \n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. There exists a Möbius map that maps from four complex numbers $(x_1, x_2, x_3, x_4)$ to $(y_1, y_2, y_3, y_4)$ if and only if their cross-ratios are equal.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nFirst suppose that some $f \\in M$ satisfies $f(x_1) = y_1, f(x_2) = y_2, f(x_3) = y_3$, and $f(x_4) = y_4$. Then we have\n$$\n[y_1, y_2, y_3, y_4] = [f(x_1),f(x_2),f(x_3),f(x_4)] =[x_1,x_2,x_3,x_4]\n$$\nas cross-ratios are preserved by Möbius maps; and conversely, if \n$$\n[y_1, y_2, y_3, y_4] =[x_1,x_2,x_3,x_4]\n$$\nthen suppose that $f \\in M$ maps $x_1 \\to \\infty$, $x_2 \\to 0$, $x_3 \\to 1$, and $x_4 \\to \\lambda = f(x_4)$; and $g \\in M$ maps $y_1 \\to \\infty$, $y_2 \\to 0$, $y_3 \\to 1$, and $y_4 \\to g(y_4) = f(x_4)$ because the cross-ratios are equal. Thus the function $g^{-1}f$ sends $x_1 \\to y_1$, $x_2 \\to y_2$, and $x_3 \\to y_3$; finally, it sends $x_4 \\to y_4$ because \n$$\ng^{-1}(f(x_4)) = g^{-1}(g(y_4)) = y_4.\n$$ \n$\\square$\n\nAnd finally, all the players take a bow and scurry off the stage as the curtains fall for our closing act!\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. The four points $z_1, z_2, z_3, z_4$ lie on the same circle or line if and only if their cross-ratio is real.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nIn $\\mathbb{\\hat C}$ - as we alluded to - lines are circles that pass through $\\infty$, which is represented by the point $(0,0,1)$. We know that any three points lie on the same circle, so without loss of generality, suppose that $z_1, z_2$ and $z_3$ are on the same circle; further suppose that $f$ is the unique Möbius map that maps $z_1 \\to \\infty$, $z_2 \\to 0$ and $z_3 \\to 1$. \n\nBy definition, $f(z_4)$ is the cross-ratio $[z_1,z_2,z_3,z_4]$. If $z_4$ lies in the same circle as $z_1$, $z_2$ and $z_3$ in $\\mathbb{\\hat C}$, then $f(z)$ will lie on the same circle as $f(z_1), f(z_2)$ and $f(z_3)$ (as Möbius maps map circles to circles on the Riemann sphere). As $f(z_1), f(z_2)$ and $f(z_3)$ are $\\infty, 0$ and $1$ by construction, and the circle uniquely determined by $\\infty, 0$ and $1$ in $\\mathbb{\\hat C}$ is the real axis, $f(z_4)$ lies an the real axis and is therefore real.\n\nConversely, if $f(z_4)$ is real, then it lies on the same circle as $f(z_1)$, $f(z_2)$ and $f(z_3)$ - the real axis; the inverse Möbius map $f^{-1}$, which always exists, also maps a circle to a circle and thus bring $z_1, z_2, z_3$ and $z_4$ to the same circle. $\\square$\n\n> FIN!\n","n":0.017}}},{"i":94,"$":{"0":{"v":"Lagrange's Theorem","n":0.707}}},{"i":95,"$":{"0":{"v":"Random Applications","n":0.707},"1":{"v":"\n> I have discovered a truly horrendous proof of the following theorem, which this margin is too large to contain.\n\n## A foray into the weeds: the Euler-Fermat theorem\n\n(_Weeds_ refers to number theory.)\n\nFermat is now considered a foundational mathematician in the development of calculus and number theory, but math was actually his side hustle; he worked as a lawyer for his day job, which makes him the benchmark every second-generation immigrant Asian kid would be compared to for the rest of history.\n\nOne of Fermat's most well-known results is *Fermat's Little Theorem*, ~~which is also what Fermat's girlfriend called his pecker~~:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Euler-Fermat Theorem** (and Fermat's Little Theorem). Let $n \\in \\mathbb{N}$ and $a \\in \\mathbb{Z}$ be coprime to $n$. Let $\\phi(n)$ be the **Euler totient function**, which computes the number of positive integers smaller than or equal to $n$ that are coprime with $n$:\n$$\n\\phi(n) = |\\{m \\in \\mathbb{N}:\\ 1 \\leq m \\leq n, \\text{gcd}(m,n) = 1\\}|\n$$\n> Given the above, the Euler-Fermat Theorem states that for $n$, $a$ and $\\phi(n)$, we have\n$$\na^{\\phi(n)}\\equiv 1\\ (\\text{mod }n)\n$$\n> Fermat's Little Theorem is a particular case of the above; for $p$ prime, $\\phi(p) = p-1$ (as every positive integer less than a prime is coprime to that prime), yielding\n$$\na^{p-1}\\equiv 1\\ (\\text{mod $p$}).\n$$\n\n(Side note: if $n$ has prime factors $p_1$, $p_2$, ..., $p_k$, $\\phi(n)$ admits the simple form\n$$\n\\phi(n) = n(1-\\frac{1}{p_1})(1-\\frac{1}{p_2}) ... (1-\\frac{1}{p_k})\n$$\nwhere $1-\\frac{1}{p_1}$ calculates the number of positive integers less than $n$ which are not multiples of $p_1$, $1-\\frac{1}{p_2}$ the number of positive integers less than $n$ which are not multiples of $p_2$, and so on; a positive integer is coprime with $n$ if it shares no prime factors with $n$, resulting in the above representation.)\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nThe Euler-Fermat theorem has about a septillion bazillion different proofs, some of which are simple enough to be understood by five-year-old children playing with their ankle bracelets (as in, the proof actually involves an ankle bracelet, though thankfully it doesn't involve any five-year-old children). But did you know that through the power of group theory, we can prove the Euler-Fermat theorem in an extremely complex and needlessly convoluted way unappreciable by anyone who hasn't studied undergraduate math? Well, we can. \n\nThe basic idea is this: if $G$ is a group, we know by a previous lemma that for all $g\\in G$, we have\n$$\ng^{|G|} = e_G\n$$\nwith $|G|$ the order of the group; as a reminder, this is because $|G|$ is a multiple of $\\text{ord}(g)$, which is just the order of the cyclic subgroup generated by $g$ alone - and by Lagrange's theorem, the order of any subgroup divides the order of the group. \n\nAs such, if we can show that the positive integers less than $n$ and coprime to $n$ form a group $U_n$ under some binary operation $\\text{mod }n$ - with  identity $1$ and $|U_n| = \\phi(n)$ by definition - we will have\n$$\ng^{\\phi(n)} \\equiv 1\\ (\\text{mod}\\ n)\n$$\nwhere $g^{\\phi(n)}$ is understood be the binary operation, whatever it is, repeated $\\phi(n)$ times. For this to match the Euler-Fermat theorem, let's just say this binary operation is multiplication $\\text{mod}\\ n$:\n$$\na, b\\in U_n,\\ a\\cdot_{U_n} b = ab\\ (\\text{mod}\\ n)\n$$\nwhere $ab$ denotes multiplication in the \"usual\" sense. (I legitimately hate the fact that these words are part of my vocabulary now.) \n\nSo, is $U_n$ a group under multiplication $\\text{mod }n$ on the set of all positive integers less than $n$ coprime with $n$? For closure, let $a, b < n$ be coprime with $n$; then \n$$\nab \\ (\\text{mod }n) = ab - kn = m\n$$\nfor some $k$ by definition, which must also be coprime to $n$; if $m$ was not coprime to $n$ (i.e. $m = cp_1$ for some prime factor of $n$, $p_1$), we would have $m + kn = ab = p_1(...)$ which shares the prime factor $p_1$ with $n$, and is thus not coprime to $n$. This is a contradiction, because $ab$ must be coprime with $n$ by virtue of neither $a$ and $b$ sharing prime factors with $n$. Thus $U_n$ is closed. Associativity is inherited from multiplication; the identity element is $1$. \n\nThe inverse is a little bit trickier. For there to be an inverse, there would need to exist a unique $a^{-1} \\in U_n$ such that $aa^{-1} \\equiv 1 \\ (\\text{mod }n)$; this implies that\n$$\naa^{-1} - kn = 1\n$$\nfor some positive integer $k$. Such an $a^{-1}$ exists via Bezout's Identity, which I will neither acknowledge, explain, nor elaborate upon.\n\nAs such, $U_n$ forms a group; and for any $a < n$ already in $U_n$, the Fermat-Euler Theorem is satisfied by the lemma we mentioned before - \n$$\na^{|U_n|} \\equiv a^{\\phi(n)} \\equiv 1\\ (\\text{mod }n).\n$$\nFor $a> n$ and $a$ coprime with $n$, however, we know that $a$ can be written as $kn + r$ for some positive integers $k$ and $r$, with $a \\equiv r\\ (\\text{mod }n)$ and $r$ also coprime with $n$ ($r \\in U_n$); if $r$ wasn't coprime with $n$, $a$ and $n$ would share a prime factor. This leads to\n$$\n\\begin{aligned}\na^{\\phi(n)}&\\equiv (kn+r)^{\\phi(n)} \\\\\n&\\equiv r^n + (\\text{powers of $(kn$)})\\ (\\text{mod }n )\\\\\n&\\equiv r^n\\ (\\text{mod }n) \\\\\n&\\equiv 1\n\\end{aligned}\n$$\nby the Binomial Theorem. This yields the desired result. $\\square$\n\n\n## Classifying small groups\n\nPossibly the most important consequence of Lagrange's Theorem is that it dramatically narrows our search range for potential subgroups of a group; this is especially true for groups $G$ of small size, whose small size guarantees a very limited number of divisors of $|G|$ and thus a very limited number of subgroups. \n\nUp until this point, we've studied a few families of groups that serve as candidates for subgroups: the cyclic group $C_n$, the dihedral gorup $D_{2n}$, and the permutation group $S_n$. One last group needs to round out our happy family of three before we can fully classify every small group (with at most 8 elements).\n\n### Quaternions\n\n> A quaternion is what Gordon Ramsay tells me to simmer in my beef stock, probably.\n\nQuaternions were first discovered by William Hamilton in 1843. As the legend goes, Hamilton was walking with his wife on a bridge at Cambridge when a flash of genius struck him, and he was moved to carve what have become the defining properties of quaternions on a rock on the bridge while his wife looked on in abject horror at a grown man suddenly dropping to the ground to play with rocks. \n\n(Unfortunately, the rock has since been lost to time and the laws carved upon it weathered and gone. ~~This makes Alexander Hamilton the second Hamilton in history to carve fundamental laws in stone only for them to have gone to shit.~~)\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Later courses will reveal that **quaternions** are an extension of the concept of complex numbers to more complex units; however, this course won't. For the time being, the **quaternions** - of which there are four, hence the name - can be defined as the complex matrices given by\n$$\n\\begin{cases}\n\\mathbf{1} = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix} \\\\\n\\mathbf{i} = \\begin{bmatrix}\ni & 0 \\\\\n0 & -i\n\\end{bmatrix} \\\\\n\\mathbf{j} = \\begin{bmatrix}\n0 & 1 \\\\\n-1 & 0\n\\end{bmatrix} \\\\\n\\mathbf{k} = \\begin{bmatrix}\n0 & i \\\\\ni & 0\n\\end{bmatrix}.\n\\end{cases}\n$$\n> These four matrices can be verified to satisfy the properties (which William Hamilton carved in stone)\n$$\n\\mathbf{i}^2 = \\mathbf{j}^2 = \\mathbf{k}^2 = \\mathbf{ijk} = -1.\n$$\n(My textbook calls these properties \"amusing\", which is a statement made by someone who is probably a little wonky in the head.)\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. Importantly, the set of quaternions along with their additive inverses $\\{\\pm \\mathbf{1, \\pm i, \\pm j, \\pm k}\\}$form a group under matrix multiplication; call this group $Q_8$, with $8$ total elements. This arises from every one of the above quaternions satisfying $x^4 = \\mathbf{1}$ and having periodic powers that exclusively equal the other quaternions.\n\nWith the quaternion group $Q_8$, we're now equipped with everything we need to describe groups of small order - except for a few last preparatory statements.\n\n### Direct product theorems\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Chinese Remainder Theorem.** ~~Unfortunately, the Chinese Remainder Theorem is not a statement about the amount of leftovers I am expected to leave every time I eat Chinese takeout.~~ If $n$, $m$ have no common factors, then $\\mathbb{Z}_{nm}$ is isomorphic to the direct product $\\mathbb{Z}_n \\times \\mathbb{Z_m}$ via the function $\\phi: \\mathbb{Z}_{nm} \\to \\mathbb{Z}_n \\times \\mathbb{Z}_m,\\ \\phi(a) = (a\\ (\\text{mod}\\ n), a\\ (\\text{mod}\\ m))$.\n\n(Unfortunately $\\phi$ already denotes the Euler totient function, but I'm too illiterate in the Greek alphabet to think of anything different.)\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nFirstly, this function is a homomorphism between the two groups (reminding ourselves that $\\mathbb{Z}_{n}$ is the integers modulo $n$, with addition as the binary operation):\n$$\n\\phi(a+b) = ((a+b)\\ (\\text{mod}\\ n), (a+b)\\ (\\text{mod}\\ m)) = \\phi(a) + \\phi(b)\n$$\nThe kernel of this homomorphism is all positive integers $a$ that are both $0$ mod $m$ and mod $n$; as $n$ and $m$ have no common factors by assumption, no such integers $< nm$ exist. As the kernel is trivial, if $\\phi(a) = \\phi(b)$, then we have only $\\phi(a) - \\phi(b) = \\phi(a-b) = 0$ implying that $a = b$. Therefore no two elements in $\\mathbb{Z}_{nm}$ map to the same element in $\\mathbb{Z_n\\times Z_m}$, and as both groups have $nm$ elements, the image of the homomorphism is the entirety of $\\mathbb{Z_n\\times Z_m}$. Thus, the two groups are isomorphic; this implies that for integers $a$ with\n$$\na \\equiv k\\ (\\text{mod }nm)\n$$\nfor coprime $n$, $m$, there is a unique way to write\n$$\n\\begin{cases}\na \\equiv x\\ (\\text{mod }n)\\\\\na \\equiv y\\ (\\text{mod}\\ m)\n\\end{cases}\n$$\nsuch that the two statements are equivalent.\n\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Direct Product Theorem.** Let $H_1$ and $H_2$ be subgroups of $G$. $G$ is isomorphic to their direct product $H_1 \\times H_2$ if (but not only if):\n1. $H_1 \\cap H_2 = \\{e\\}$\n2. If $h_1 \\in H_1$ and $h_2 \\in H_2$, then $h_1h_2 = h_2 h_1$, and\n3. For each $g \\in G$ there are $h_1 \\in H_1$ and $h_2 \\in H_2$ such that $g = h_1 h_2$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nConsider the function $\\phi(h_1, h_2) = h_1 \\cdot_G h_2$ which maps from $H_1 \\times H_2$ to $G$. This function is a homomorphism if\n$$\n\\begin{aligned}\n\\phi((h_1, h_2)\\cdot_{H_1\\times H_2} (h_1', h_2')) &= \\phi(h_1, h_2)\\cdot_G \\phi(h_1', h_2') \\\\\n\\phi(h_1 \\cdot_G h_1', h_2 \\cdot_G h_2') &= h_1\\cdot_G h_2 \\cdot_G h_1' \\cdot_G h_2' \\\\ \nh_1h_1'h_2h_2' &= h_1h_2h_1'h_2' \\\\\nh_1'h_2 &= h_2h_1'\n\\end{aligned}\n$$\nfor all $h_1, h_1' \\in H_1$, $h_2, h_2' \\in H_2$; the above condition is satisfied by (2) - \"If $h_1 \\in H_1$ and $h_2 \\in H_2$, then $h_1h_2 = h_2 h_1$\". This function is isomorphic if its kernel is the identity and its image is the entire group $G$. Suppose that $(h_1,h_2)$ lies in the kernel of $\\phi$:\n$$\n\\phi(h_1,h_2) = h_1\\cdot_G h_2 = e,\\ h_1 = h_2^{-1}\n$$\nwhich would mean that $h_1$ and $h_2$ are in the same subgroup, as any element in a group must also have its associated inverse. However, by (1) (\"$H_1 \\cap H_2 = \\{e\\}$\"), the only common element between the groups is the identity itself; thus, the kernel of $\\phi$ is the identity.\n\nFinally, the image of $\\phi$ is guaranteed to be the entirety of $G$ by property (3). Thus, $\\phi$ is an isomorphism between $H_1 \\times H_2$ and $G$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. If every non-identity element in a finite group $G$ has order $2$, $G$ is isomorphic to $C_2 \\times C_2 \\times ... \\times (C_2$ for a certain number of $C_2$). \n\nThis multiple direct product is the group consisting of elements of the form \n$$\n\\{(a_1, a_2, ..., a_n)\\ |\\ a_i \\in C_2\\}\n$$\nwith multiplication defined on a term-by-term basis, as before. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nFirstly, observe that the group $G$ must be abelian: for any element $g \\in G$ of order $2$ (which is every non-identity element, by our assumption), we have $g^2 = e$ and thus $g = g^{-1}$, yielding\n$$\ng_1 g_2 = g_1^{-1} g_2^{-1} = (g_2 g_1)^{-1} = g_2 g_1\n$$\nas $g_2 g_1$ is also an element of order 2 by virtue of being in $G$. \n\nNow consider what $G$ looks like. Any element $g_1 \\in G$ forms a subgroup with itself as the generator, $G_1 = \\{e, g_1\\} \\cong C_2$; if $G = G_1$, then $G \\cong C_2$ and we are done. If instead there is an element $g_2 \\neq g_1 \\in G$, then $g_2$ also forms a subgroup $G_2 = \\{e, g_2\\} \\cong C_2$; so will $g_1$ and $g_2$ together, which generate the subgroup $G_{1,2} = \\{e, g_1, g_2, g_1g_2\\}$. By the Direct Product Theorem, $G_1 \\cap G_2 = \\{e\\}$, $G$ is abelian, and some product of elements from $G_1$ and $G_2$ expresses all elements in $G_{1,2}$; thus we have\n$$\nG_{1,2} \\cong G_1 \\times G_2 = C_2 \\times C_2.\n$$\nIf $G = G_{1,2}$ (i.e. $|G| = 4$), then $G \\cong C_2 \\times C_2$. Otherwise, if there is one other element $g_3 \\neq g_1 \\neq g_2$ in $G$, repeat the above process: $G_{1,2,3}$, the group generated by $g_1, g_2$ and $g_3$ (which is $G$ itself), is isomorphic to the direct product $C_2 \\times C_2 \\times C_2$, and so on. This can be done with any number of finite $g_n$. $\\square$\n\n### Group classifications\n\nHere we classify all groups of order $8$ or less.\n\n**The identity/trivial group.** $\\{e\\}$ is the only group of order $1$.\n\n**Prime-order groups.** All groups of prime order $p$ are cyclic. By Legrange's Theorem, such groups have no \"actual\" subgroups (besides the trivial subgroup and itself); as such there can be no element within it with order less than $p$, as that element would generate a subgroup. Thus all elements within the group have order $p$, which, by definition, corresponds to a cyclic group.\n\nThis rules out groups of order $2$, $3$, $5$ and $7$, which are all isomorphic to cyclic groups.\n\n**Order 4.**\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. All groups $G$ of order $4$ are isomorphic to either $C_4$ or $C_2 \\times C_2=D_4$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nBy Lagrange's Theorem, an element $g \\in G$ has order $1$, $2$, or $4$. If $g$ has order $4$, then by definition $G$ is a cyclic group, as its elements are $\\{e, g, g^2, g^3\\}$; if $g$ has order $1$, $g$ is the identity; and if $g$ has order $2$, we have\n$$\ng = g^{-1}.\n$$\nSuppose that $G$ is not the cyclic group; it thus cannot have elements of order $4$, only $1$ (the identity exclusively) and three other elements of order $2$. Let $a, b \\in G$ be two distinct non-identity elements of $G$ with order $2$. Then we have $ab = (ab)^{-1} = b^{-1} a^{-1} = ba$, and thus $G$ is abelian; this is due to $ab \\in G$ and all elements of $G$ having order $2$, leading to $(ab)^{-1} = ab$. We also have $a \\neq ab$ and $b \\neq ab$ due to neither being the identity; thus, $a, b, ab$ and $e$ form a complete group. The subgroups of this group arise entirely from\n$$\n\\{a, e\\},\\ \\{b, e\\}\n$$\nwhich are both isomorphic to $C_2$, and their direct product $C_2 \\times C_2$ leads to $\\{a,b,ab,e\\}$. $\\square$\n\n\n**Order 6.**\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. All groups $G$ of order $6$ are isomorphic to either $S_3$ or $C_6$. (The two groups are not themselves isomorphic because $S_3$ has no element of order $6$.)\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nBy Lagrange's Theorem, an element $g \\in G$ has order $1$, $2$, $3$ or $6$. \n\nIf $g$ has order $6$, then $G$ is isomorphic to $C_6$ by definition; this yields the result concerning $C_6$. Otherwise, suppose that $G$ contains only elements of order $2$. By an above result, $G$ is thus isomorphic to $C_2 \\times C_2 \\times ... \\times C_2$, and has order $2^n$, not $6$; thus, $G$ cannot contain only elements of order $2$ and must contain elements of order $3$.\n\nNow suppose that $G$ contains only elements of order $3$; these elements will form subgroups of the form $G_a = \\{e, a, a^2\\}$. If $b \\in G$ is not in $G_a$, then $b^2$ will also not be in $G_a$, as $(b^2)^2 = b^4 = b$ is not in $G_a$. Thus $G$ has at least the elements $\\{e, a, b, a^2, b^2\\}$; if there is a single other element $c$ not equal to these five elements, $c$ also has order $3$ by assumption and $c^2$ will not equal any of the above (by the same argument as $b^2$). Therefore $G$ is at least $\\{e, a, b, c , a^2, b^2, c^2\\}$, which contains 7 elements and not 6 - a contradiction.\n\nThus $G$ must contain elements of both order $2$ and $3$. Let $a \\in G$ have order $2$, and $b$ have order $3$, with $a \\neq b \\neq e$; then we have\n$$\nG = \\{e, a, b, b^2\\}\n$$\nat minimum. $ab$ does not equal any of these elements; if $ab = e$ then $b = a^{-1} = a$ as $a$ has order $2$, but $b \\neq a$; if $ab = a$ then $b = e$, which is untrue; if $ab = b$ then $a  =e$, which is untrue; and if $ab = b^2$ then $a = b$ directly, which is untrue. Similarly, $ab^2$ is also part of $G$ and unequal to any of the above. Thus we have\n$$\nG = \\{e, a, b, b^2, ab, ab^2\\}\n$$\nwhich can be verified to be closed, and is of order $6$. Finally, we notice that the above maps exactly to the dihedral group $D_6$, with the rotation element $r = b$ and the reflection element $s = a$; in turn, $D_6 \\cong S_3$, leading to the desired result.\n\n**Order 8.**\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. All groups $G$ of order $8$ are isomorphic to one of the following: $C_8$, $C_4 \\times C_2$, $C_2 \\times C_2 \\times C_2$, the quaternion group $Q_8$, and the dihedral group $D_8$. \n\nThe groups are not themselves isomorphic because $C_8$ has an element of order $8$, which no other group does; $C_2 \\times C_2 \\times C_2$ has elements of order $2$ only, which no other grouop does; and $Q_8, D_8$ and $C_4 \\times C_2$ can be verified to not share elements with similar properties.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nThe simplest case is the cyclic group: if $G$ has an element of order $8$, then $G$ is isomorphic to $C_8$ by definition. Otherwise, by Lagrange's Theorem, $G$ has elements of order $1$, $2$ or $4$ (not $8$, as that would make $G$ cyclic.) Ignoring elements of order $1$, we have:\n\n   1. $G$ has only elements of order $2$. By an above result, this leads to $G \\cong C_2 \\times C_2 \\times C_2$.\n   2. $G$ has an element of order $4$: call this element $f$. Then the subgroup generated by $f$ alone, denoted $\\langle f \\rangle$, has the form\n   $$\n   \\{e, f, f^2, f^3\\}\n   $$\n   By the fact that left cosets partition $g$, we know that there must exist some $g \\in G$ and not in $\\langle f \\rangle$ such that the coset $g\\langle f\\rangle = \\{g, gf, gf^2, gf^3\\}$ are the remaining elements of $G$. The question is, what is the order of $g$?  If $g^2 \\in g\\langle f \\rangle$ we would have $g = f^n$ for some $n = 0, 1, 2, 3$, which is a contradiction because it would make $g\\langle f \\rangle = \\langle f \\rangle$. \n\n   Thus, $g^2 \\in \\langle f \\rangle$; if $g^2 = f$ then $g^4 = f^2 \\neq e$, yielding $g$ as an element of order greater than $4$ (and thus of order $8$) - a contradiction, as that would make $G$ cyclic. Thus we have $g^2 = e$ or $g^2 = f^2$.\n\n   If $g^2 = e$ and not $f^2$, consider the element $fg$; it cannot be part of the coset $\\langle f \\rangle$ because then we would have $g = f^n$ once again. Thus $fg \\in \\{g, gf, gf^2, gf^3\\}$, and $g^{-1}fg \\in \\langle f \\rangle =\\{e,f,f^2,f^3\\}$; $g^{-1}fg$ has order $4$ as $f$ has order $4$, so $g^{-1}fg$ must be either $f$ or $f^3$. If it is $f$, then $fg = gf$; the Direct Product Theorem then shows that $G = \\langle f \\rangle \\times \\langle g \\rangle = C_4 \\times C_2$. Otherwise if it is $f^3 = f^{-1}$, we have\n   $$\n   g^{-1}fg = f^{-1}\n   $$\n   which defines the dihedral group $D_8$, as $g$ is order $2$.\n\n   Finally, consider $g^2 = f^2$; once again $fg \\in g\\langle f\\rangle$ and $g^{-1}fg = f$ or $f^3$. If the former, we have again the direct product $C_2 \\times C_4$; if the latter, $g$ is order $4$, yielding a bijection with the quaternion group.","n":0.017}}},{"i":96,"$":{"0":{"v":"Group Structures","n":0.707},"1":{"v":"> My Discord group has the structure of a patriarchical cult-of-personality dictatorship brought to power by a nationalistic military junta.\n\n\n## Visualizing group structures\n\nLet's begin our discussion with the symmetric group $S_3$ (isomorphic to the dihedral group $D_6$), visualized here in all its Cayley diagram glory:\n\n![alt text](./assets/images/image-63.png)\n\nWe will focus on $S_3$ since it is one of the very few groups whose Cayley diagram does not resemble either a four-dimensional icosahedron, a pentagram used in rituals to summon forth the bowels of hell, or a GPU overheating accident waiting to happen. \n\nWhat do we notice about $S_3$? First, its structure is (appropriately, given the name) symmetric; it's three mini-structures stitched together into one, each looking something like\n\n![alt text](./assets/images/image-64.png)\n\nthis. Owing to the isomorphism $S_3 \\cong D_6$, one way of thinking of this is that the blue lines above represent multiplication by the reflection element $f$ while the red lines represent multiplication by a rotation of $120$ degrees. \n\nWhat gives rise to a group structure of this shape? From what we know about $D_6$, we have\n$$\nfrf = r^{-1} = r^2\n$$\nand indeed, this is the relation that *defines* the dihedral group just as $r^n = e$ defines the cyclic group. Starting from the identity (the leftmost bubble), we apply $f$, $r$ and $f$ in order (blue-red-blue), reaching the right-most bubble; this leads us to $r^2$, which can be taken back to the identity with another application of $r$ - hence forming the closed-loop structure above. \n\nImportantly, we know one thing: that it doesn't matter if we started with $e$ - indeed, if we had begun with $r$ or $r^2$, an application of $f$, then $r$, then $f$ in sequence would still take us to $r^{-1}$. This relationship defines $D_n$; it permeates $D_n$, and so the Cayley diagram above that represents that relationship also permeates the Cayley diagram of $D_n$.\n\n> Sort-of-but-not-really a <span style=\"background-color: #12ffd7; color: black;\">theorem</span>: groups are often defined by a single equation or equations that capture the relationships between its elements, like $r^n = e$ ($C_n$) or $frf = r^{-1}$ ($D_{2n}$). The Cayley diagram that captures that relationship will be the fundamental component from which the entire Cayley diagram of the group is built, because the relationship is true for every element in the group; this is known as **regularity**.\n\nFor instance, we see that the following Cayley diagram\n\n![alt text](./assets/images/image-65.png)\n\nis not at all a *regular* one; the structure of the red arrows on the top differs from its structure on the bottom. Denoting the red action as $r$ and the blue action as $b$, we have\n$$\n0 \\cdot r \\cdot r \\cdot r = 0 \\cdot r^3 = 3\n$$\nfrom the top half, and also $3 \\cdot r = 0$, yielding $r^4 = e$ and $r^2 \\neq e$; however, from the bottom half we have\n$$\n4\\cdot r = 5,\\ 5 \\cdot r =4\n$$\nyielding $r^2 = e$. THis is a direct result of the properties of the red action not being constant throughout the diagram.\n\nFrom all of this, we conclude one fact: just as groups are constructed out of individual elements, the structure of a group - as represented by its Cayley diagram - are constructed out of sub-structures that capture some relationship intrinsic to the group. How do we locate these sub-structures, and are they uniquely defined?  Two main methods present themselves.\n\n### Subgroups in group structures\n\nOne natural candidate for finding sub-structures in a group is looking towards the subgroups of that group; subgroups are groups entirely contained within another group, and their Cayley diagrams reflect that. For instance, we have\n\n![alt text](./assets/images/image-66.png)\n\n$C_3$ (drawn in green) a subgroup of $C_6$; and $C_6$ in turn a subgroup of $D_{12}$ or $C_{12}$, and so on. The above example shows subgroups to be exactly what we are looking for - sub-structures that repeat themselves over and over again to form the entire group: $0, 2, 4$ form a cyclic structure described by $C_3$, and so do $1$, $3$ and $5$ - the two combining together to form $C_6$. \n\nThe only issue is that $1$, $3$ and $5$ share only the *structure* of $C_3$, not its status as a group; $1$, $3$ and $5$ do not form a group due to the lack of an identity element. How do we capture the substructures of a group that have relationships resembling a subgroup, but are not subgroups in truth?\n\n### Cosets in group structures\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Let $H$ be a group, and $G$ its subgroup. A **left coset** of $G$, generated by the **representative** $a \\in H$ and denoted $aG$, is the set of elements \n$$\n\\{ag \\ |\\ g \\in G\\}\n$$\n> with the **right coset** generated by $a$, denoted $Ga$, defined correspondingly as\n$$\n\\{ga \\ |\\ g \\in G\\}\n$$\n> The necessity of defining both a right and left coset is that a left coset is not necessarily equal to a right coset, unless $H$\n\n(Not to be confused with *corset*, which is what Victorian-era ladies wore when they wanted to give themselves stomach problems and an incurable hunchback before the age of 25.)\n\nThe *algebraic properties* of cosets are crucial towards the final aim of this chapter - a proof of Lagrange's theorem - but for now, all we want to talk about is what cosets *are*, and what they look like. \n\nThe cosets of a subgroup copy that subgroup's structure. To best understand this, consider the example above: $C_3$ is a subgroup of $C_6$, and its structure is a loop of three elements positioned in a circle. The elements $\\{1,3,5\\}$ - or, when expressed as a coset, $1C_3 = \\{0+1, 2+1, 4+1\\} = \\{1,3,5\\}$ retain exactly that structure. \n\nAs such, cosets give us the language to express what the basic underlying structure of a group is: as we will soon see, a group can be decomposed, or *partitioned*, into the cosets of its subgroups. Take $D_6$ as an example:\n\n![alt text](./assets/images/image-69.png)\n\nThe smallest subgroup of $D_6$ (excepting the trivial subgroup $\\{e\\}$) is the group of two elements $D_2 = \\{e,f\\}$ outlined in green; $f$ satisfies $f^2 = e$, and so $\\{e,f\\}$ is closed. The basic structure given by $f^2 = e$ - two $f$-actions on an element, marked in blue, return us to the original element - is repeated for the elements $\\{r, rf\\}$ as well as $\\{r^2, r^2f\\}$; these two sets correspond exactly to $rD_2$ and $r^2D_2$!\n\nBut there's more; if we just look at the red arrows, we'll notice that **two** cyclic structures are embedded in $D_6$: one, the subgroup $C_3$, formed by $\\{e, r, r^2\\}$ - and the other, $\\{f, rf, r^2f\\}$, which is not a subgroup but retains the structure of $C_3$ by virtue of being its coset $fC_3$. \n\nThis is exactly what the cosets of a subgroup tell us: a subgroup's structure serves as a basic building block for the entire group's structure, and all of its cosets reveal how that basic structure is omnipresent throughout the larger group - $D_6$ containing three copies of the structure of $C_2$, or two copies of the structure of $C_3$.\n\nIn the next section, we'll explore the algebraic properties of cosets - and through them, we'll formalize, codify and prove what all of this really means.","n":0.029}}},{"i":97,"$":{"0":{"v":"Lagrange's Theorem","n":0.707},"1":{"v":"\n> One of Lagrange's greatest scientific discoveries was the Principle of Least Action, which states that objects tend to take the path requiring the least effort. Lagrange is a brilliant scientist and all, but I could've told you the same thing just by looking at my Sunday afternoon schedule. \n\n## Properties of cosets\n\nRecall from our previous discussion that cosets represented a way of expressing how the structure of a subgroup could form the entire group: a subgroup's structure is embedded within the structure of the larger group, and through the innate symmetries present within the structures of groups, cosets reveal how the larger group's structure is comprised of many repeated copies of the subgroup's.\n\nIn order to formulate this properly, though, we have to prove the following three things.\n\n1. > <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. The (left) cosets of any subgroup $G$ of a group $H$ span $H$; in other words, for any element $h \\in H$, there is at least one such coset $aG$ such that $h \\in aG$.\n\n    > <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n    By the definition of a subgroup, $e \\in G$; as such, the coset $hG$ includes the element $eh = h$ and $h \\in hG$. This means that the cosets of a subgroup are able to represent the entire group $H$.\n\n2. > <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. Let $H$ and $G$ be as above. As long as two cosets $aG$ and $bG$ for $a, b \\in H$ share a single element, $aG$ and $bG$ are identical cosets.\n\n    > <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\n    Let $k \\in aG$ and $k \\in bG$. By definition of a coset, for some $g_1, g_2 \\in G$ we have $a g_1 = b g_2 = k$; as such, we have \n    $$\n    b^{-1} a = g_2 g_1^{-1} \\in G\n    $$\n    by group closure, and\n    $$\n    a^{-1} b \\in G\n    $$\n    by group inverses.\n    \n    To show that two sets are identical, we must prove a bijection between the sets. For every $g \\in G$, we have $a^{-1} bg \\in G$ by closure; as such, the coset $aG$ contains all elements of the form $aa^{-1}bg = bg \\in bG$ and thus $aG$ maps completely onto $bG$. Conversely, $b^{-1}ag \\in G$ for all $g \\in G$ and thus $bG$ completely contains all elements of the form $bb^{-1}ag = ag \\in aG$. As the two cosets completely contain one another, they are necessarily identical.\n\n    Conversely, if the two cosets are identical, then the condition \n    $$\n    b^{-1} a \\in G\n    $$\n    is satisfied via $k \\in aG$ and $k \\in bG$ yielding $ag_1 = bg_2$.\n\n    > A corollary of this is that, if $a \\in G$, then the coset $aG$ is $G$ itself unchangeed as $aG$ has at least one common element with $eG$, the identity coset.\n\n    This implies two things. First, the cosets of a subgroup can be described by different *representatives*: $aG$ and $bG$ with $a \\neq b$ could be different names for the exact same coset. Second, even though a coset is not uniquely named, the unique cosets of a subgroup are non-overlapping (or *disjoint*): two cosets are either identical, or do not intersect at all.\n\n    Now all that's left to prove is \n\n3. > <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. The cosets of a subgroup $G$ all have the same size: that of $G$ itself.\n\n    > <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n    \n    By definition, a coset $aG$ with $a \\in H$ is the set $\\{ag\\ |\\ g \\in G\\}$. If none of the $ag$s thus defined are equal to one another, then $aG$ will have the same number of elements as $G$. \n    \n    Suppose for the sake of contradiction that for some $g_1 \\neq g_2 \\in G$, we have $ag_1 = ag_2$; as such $a = ag_2g_1^{-1}$, which would necessarily imply $g_2g_1^{-1} = e$ and $g_2 = g_1$ as the identity is unique. This is a contradiction, and no two $ag$s are thus equal.\n\n    (Alternatively, the function $f(g) = ag$ maps $G$ completely to $aG$; however, this function is also invertible, with $f^{-1}(g) = a^{-1}g$ mapping $aG$ to $G$. A bijection between the two sets is thus established, showing they have the same cardinality.)\n\n## Lagrange's Theorem\n\n> This section is the section entitled \"Lagrange's Theorem\" in the page entitled \"Lagrange's Theorem\" in the chapter entitled \"Lagrange's Theorem\". Hopefully you don't hold too much of a Lagrudge against me for this.\n\nThe culmination of all of the above properties leads us to the following two results:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The left cosets of a subgroup $G$ of $H$ **partition** $H$; they divide $H$ into sets $H_1, H_2, ..., H_n$ such that $H_1 \\cup H_2 \\cup ... \\cup H_n = H$ and for any $i, j = 1, ..., n$, we have $H_i \\cap H_j = \\emptyset$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. The partition follows naturally from the three lemmas proven above: the cosets of $G$ are of equal size, have null intersection, and completely span $H$.\n\n\nand, consequently,\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span> (the big one!): **Lagrange's Theorem.** If $G$ is a subgroup of $H$, then the size of $G$ divides $H$ (in the sense of $|G|$ being a factor of $|H|$.) \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Each coset of $G$ has size $|G|$, and the cosets of $G$ partition $H$. Thus the size of $H$ is \n$$\n|H| = |G| + |G| + ... + |G|\n$$\n> which is an integer multiple of $|G|$. (Nice job with your two-line big-boy proof, Mr. Lagrange.)\n\nIn particular, we define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **index** of a subgroup $G$ of a group $H$, denoted $|H: G|$, is defined as $\\frac{|H|}{|G|}$.\n\nLagrange's Theorem has several important consequences, one of which is the fact that they dramatically narrow down the potential candidates of subgroups of any group $H$: if the order of $H$ is prime, for instance, we know that the order of any subgroups of $H$ are either $|H|$ - in which case the subgroup is $H$ itself - or $1$, in which case the subgroup is the unimpressive and disappointing $\\{e\\}$. Other corollaries include\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. The order of an element $g$ of a finite group $G$ divides $|G|$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. We know that finite groups have elements of finite order; if an element $g$ had infinite-order, then there would be an infinite number of elements generatable from $g$ and the group $G$ would thus be infinite, not finite. As such, the group of elements $H = \\{g^a\\ |\\ a = 0, 1, ..., \\text{ord}(g) - 1\\}$ forms a finite subgroup of $G$; $|H| = \\text{ord}(g)$, and thus by Lagrange's Theorem we have $\\text{ord}(g)$ dividing $|G|$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. For any finite group $G$ and $g \\in G$, $g^{|G|} = e$: the exponent of $G$ divides $|G|$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. By the above, $|G| = k\\ \\text{ord} (g)$ for some positive integer $k$ and any $g \\in G$; thus $g^{|G|} = (g^{\\text{ord} (g)})^k = e^k = e$ by definition of $\\text{ord}(g)$.\n\n(The above arguments have been limited to left cosets; however, every one of the above properties - equal partitions, Lagrange's Theorem, the index - is true for right cosets as well!)\n","n":0.029}}},{"i":98,"$":{"0":{"v":"Groups and Homomorphisms","n":0.577},"1":{"v":"> Goddammit, we're back in the soup.\n","n":0.378}}},{"i":99,"$":{"0":{"v":"Subgroups","n":1},"1":{"v":"\n> PUBLIC SERVICE ANNOUNCEMENT: DO NOT, under ANY circumstances, search the word \"subgroup\" on Urban Dictionary. DO NOT search the word \"subspace\" on Urban Dictionary. DO NOT <br/><br/>\n~~Domgroup uwu nyaaaaa :3 sorry~~\n\n## Subgroups\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Call the group $(H,\\cdot_H,e_H)$ a **subgroup** of $(G, \\cdot_G, e_G)$ - denoted $(H, \\cdot_H, e_H) \\leq (G, \\cdot_G, e_G)$ - if $H$ and $G$ satisfy the following three conditions:\n\n1. $H \\subset G$. ($H$ is a subset of $G$)\n2. $e_H = e_G$.\n3. For all pairs of elements $a, b\\in H$, $a\\cdot_H b = a\\cdot_G b$.\n   \nSubgroups are a natural extension of the notion of a subset to groups: they inherit the group's identity and binary operation while also inheriting a subset of its set. Examples of subgroups: $(\\mathbb{Z}, +)$ to $(\\mathbb{R}, +)$; $(e, *)$, a group involving only the identity, to any group with the same operation $*$ and identity $e$; the group of divorced Hollywood actors to the group of Hollywood actors.\n\nAn easy way to check whether a given group is a subgroup of another is\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. The **subgroup lemma**: let $(G, \\cdot_G, e_G)$ be a group and $H$ be a non-empty subset of $G$. It is possible to find a unique binary operation $\\cdot_H$ and an identity $e_H$ such that $H \\leq G$ if and only if the following conditions are satisfied:\n1. $e_G \\in H$.\n2. For any $a, b \\in H$, $a \\cdot_G b \\in H$.\n3. For any $a \\in H$, $a^{-1} \\in H$.\n\n> Alternately, simply the condition that for any $a, b \\in H$, the element $a \\cdot_G b^{-1} \\in H$ suffices.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThe three given conditions are equivalent to stating that the four group axioms - closure, associativity, inverse, and identity - hold for $H$.\n\n1. **Closure**. Given by requirement (2) by definition.\n   \n2. **Associativity**. Suppose that $a, b, c$ are elements of $H$; as $H$ is a subset of $G$, $a, b, c$ are also elements of $G$, in which the binary operation $\\cdot_G$ is associative. Thus, the associativity of $\\cdot_G$ is inherited from $G$.\n   \n3. **Inverse**. Given by requirement (3).\n   \n4. **Identity.** Our requirement for $H$ forming a subgroup is that $H$ and $G$ have the same identities. Denote the identity of a group formed by $H$ as $e_H$; then \n    $$\n    e_H \\cdot_G e_H = e_H\n    $$\n    in the group $H$. In the group $G$, $e_H$ has an inverse $e_H^{-1}$ where $e_H e_H^{-1} = e_G$, the identity of group $G$. Then\n    $$\n    e_H e_H e_H^{-1} = e_H e_H^{-1}\n    $$\n    and as such\n    $$\n    e_H e_G = e_G\n    $$\n    leading to $e_H = e_G$.<br/>\n\nNow we prove that these three conditions can be condensed into the condition that, for non-empty $H$, any $a, b \\in H$ satisfy $a\\cdot_G b^{-1} \\in H$.\n\nFirst, suppose that $b = a$ (this element must exist as $H$ is non-empty). Then we have $a \\cdot_G a^{-1} \\in H$, and as $a$ is an element of $G$, $a \\cdot a^{-1} = e_G \\in H$. This is equivalent to the first given condition. \n\nSupposing that $a = e_G$, we have\n$$\ne_G \\cdot_G b^{-1} = b^{-1} \\in H\n$$\nif $b \\in H$. This gives rise to the third condition, and leads directly to\n$$\na, b\\in H \\to b^{-1} \\in H \\to a\\cdot_G (b^{-1})^{-1} = a \\cdot_G b \\in H\n$$\nsatisfying the second given condition. $\\square$\n\n***\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. Every subgroup of $(\\mathbb{Z}, +)$ is described by $(n\\mathbb{Z}, n)$ for some integer $n$, where the set $n\\mathbb{Z} = \\{nk | k \\in \\mathbb{Z}\\}$ is the set of integer multiples of $n$: for instance, $2\\mathbb{Z}$ describes the even integers while $3\\mathbb{Z}$ describes integer multiples of $3$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nFirst we prove the forward statemeent: that $n\\mathbb{Z}$ indeed satisfies the definition of a subgroup. Suppose that $a = nx$ and $b = ny$ for some $x, y \\in \\mathbb{Z}$. The inverse of $b$ under the operation $+$ is the additive inverse: $b^{-1} = -ny$, yielding\n$$\na + b^{-1} = nx - ny = n(x-y)\n$$\nwhich by definition is an element of $n\\mathbb{Z}$. By the lemma above, this indicates that $n\\mathbb{Z} \\leq \\mathbb{Z}$. \n\nConversely, suppose that $H$ is a subgroup of $\\mathbb{Z}$ and that $H \\neq \\{0\\}$, the trivial subgroup; then by the above lemma, we have\n\n1. The identity element $0 \\in H$.\n2. For $a, b \\in H$, $a+b\\in H$.\n3. For $a \\in H$, $a^{-1} = -a \\in H$.\n\nSuppose that $n$ and $m$ are elements of $H$ that have the smallest and second-smallest absolute values out of all non-identity elements in $H$, not necessarily uniquely; as $H$ is not the trivial subgroup, $H$ has at least two elements $n$ and $m$ such that $m>n$. \n\nWithout loss of generality, assume that they are both positive (as any element of $H$ must have their additive inverses also a part of $H$, by condition (3)). Now consider the element of $H$ given by\n$$\nm - n = m + n^{-1} \\in H\n$$\nwith $m > n$ leading to $0 < m - n < m$. Consider whether $m-n$ is smaller than $n$. If $m-n$ is smaller than $n$, $n$ would no longer be the smallest positive element; this would lead to a contradiction. Thus, $m-n \\geq n$; if $m - n$ was strictly larger than $n$, it would be the second-smallest positive number in $H$, not $m$ as we assumed - also a contradiction. The only remaining option is $m - n =n, m = 2n$; a similar argument on the $k$th and $k+1$th largest positive elements in $H$ yields that $H$ can only take the form\n$$\nH = n\\mathbb{Z}.\\ \\square\n$$\n","n":0.033}}},{"i":100,"$":{"0":{"v":"Preface","n":1},"1":{"v":"> This section will present a ubiquitous real-world example of groups with the ultimate example of de-abstracting and demystifying the abstract and mysterious language that Group Theory stems from. This is good, because it'll be the only anchor to the place we call \"the real world\" in all 100-something pages of content. The rest rapidly devolves into insane and deranged statements such as \"how is $t: \\textbf{Fam}(\\mathcal{A}) \\to \\text{Hom}(\\mathcal{A, \\mathbf{Enr(P)}})$ doing today?\" or \"the weather is pretty $O(2)\\backslash\\text{Isom}(SO(2))$ outside, don't you think?\".\n\n## Saga 1: The Horror of Hungary\n\n> Pre\"face\", heh. Get it? Get it? <br/><br/>Please clap.\n\nEveryone knows about the Rubik's Cube. It's fun, it's family-friendly,  it's one of the three most horrible things to come out of Hungary (the other two are Microsoft Excel and Viktor Orban); and, until Linux was invented in 1991, it was also the world's most effective diagnostic test for childhood autism. \n\nThe intrigue of the Rubik's Cube lies in its reputation for being \"easy to learn, impossible to master\"; the exact same property that draws people to chess, or macarons, or Flappy Bird. So what makes it \"easy to learn\", and what makes it \"impossible to master\"? We observe four different properties of the Cube that perhaps explain both these aspects in tandem:\n\n> <span style=\"background-color: #bc42f5; color: black;\">Observation 1</span>. The Rubik's Cube only has six possible moves at any given moment: rotating any of the six faces clockwise. \n\n![alt text](./assets/images/image-48.png)\n\nThis requires just a tiny bit of extra elaboration. \"But what about counterclockwise? WHAT ABOUT THE MIDDLE LAYER??\", you say calmly. These moves are just as easily understood as combinations of clockwise rotations: for instance, the rotation of the horizontal middle slice anticlockwise is the rotation of the top and bottom faces clockwise. \n\nAs such, every single permissible action - and every sequence of possible permissible actions, aside from the elusive \"twisting the top-right corner clockwise when no one's looking\" and the unsurpassable \"hurling the Rubik's Cube towards my window at $v = 0.1c$ after missing my PB by 0.01 picoseconds\" - can be described entirely by just six actions: $F, L, U, R, B$ and $D$.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Observation 2</span>.  Every move is deterministic: you know exactly what will happen when you turn a face, barring minor accidents and unpreventable Acts of God.\n\nThe French horn is God's Instrument: when you play it, what's coming out the other side sounds either like a choir of angels singing a song of divinity or a constipated fart delivered by a 500-pound morbidly-obese man after two straight meals at Taco Bell, and only God knows which one it'll be. The Rubik's Cube is **not** God's instrument. You understand exactly what'll happen at any and every moment, and you also understand that\n\n> <span style=\"background-color: #bc42f5; color: black;\">Observation 3</span>. All moves are reversible, and\n\n> <span style=\"background-color: #bc42f5; color: black;\">Observation 4</span>. Moves can be freely combined in any order.\n\nIf you know precisely how a cube is scrambled - each and every rotation, in their rightful order, that had produced the state the cube is currently in - then you also know how to solve it: reverse all the steps, one at a time, until the cube is returned to its untouched, pristine state.\n\nThese four observations collect together to form our first understanding of what a group is.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **group** is a set of **actions** (moves) $G$, a **binary operation** (a means of combining such actions) $*$, and an **identity** (\"do-nothing\" action) $e$ satisfying the following four rules:\n1. A combination of two actions $a$ and $b$, denoted $a * b$, is still a valid action that is a member of $G$. (This is true for any sequence of consecutive actions.)\n2. Every action $a$ has its inverse $a^{-1}$, such that $a * a^{-1} = e$, the \"do-nothing\" action.\n3. The \"do-nothing\" action $e$ does nothing: $a * e = a$.\n4. Actions are **associative**: $(a * b) * c = a* (b*c)$.\n\nIn particular, we call the basic actions $F, L, U, R, B$ and $D$ - single clockwise rotations of the six faces - the **generators** of the group that describes the Rubik's Cube.\n## Saga 2: Rubik's Platonic Solids\n\nRubik's Cubes come in all shapes and sizes. Some are cubes; some are pyramids. Some have numbers or faces on them. Some look vaguely pear-shaped, with a dull green color, a ring of metal around the top, and a sizzling sound alongside a vague smell of gunpowder - oh wait, silly me, I got confused with my WWII hand-grenade collection for a second. \n\n![alt text](./assets/images/image-50.png)\n\nThis is a (two-by-two) pyraminx. It is a two-hundred-gram hulking obelisk of nonbiodegradable plastic, lacking any bones, fat, and any connective tissue; an amalgamation of oceans upon oceans of liquified, strained, irretrievably destroyed, and artificially synthesized remnants of what was once purest crude oil in a past life. Its monolithic existence proves either that God is unaware of the horrors committed within his Kingdom, or impotent to intervene and stop them. But also you can turn it. \n\n(You might think even your 107-year-old great-great-grandmother with both Alzheimer's and Parkinson's could solve this. You would probably be right.)\n\nThere are certain similarities between a Rubik's Cube and a Pyraminx, beginning from the fact that both serve as the ultimate representations of mankind's declarations of war against the natural order. But also in how you can turn them.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Observation 1</span>. **Rotational symmetries.** Four rotations of the same face for a Rubik's Cube gets you back to where you once was; a Pyraminx gets you there in three.\n\nBut perhaps slightly more intriguing is\n\n> <span style=\"background-color: #bc42f5; color: black;\">Observation 2</span>. **Commutativity.** Actions on a Pyraminx result in the same state when performed in any order; turning the top tip does not interfere with turning any of the other tips. This property does not generally hold for a Rubik's Cube.\n\nA summary of the above two observations might go something like this. Both the (2x2) Pyraminx and the Rubik's Cube have certain symmetries; turning a face a certain number of times in a row reverts it back to its original state. These symmetries, however, differ wildly in their properties. A Rubik's Cube requires four turns to restore balance; a Pyraminx only three. A Rubik's Cube is non-commutative in general ($LLLL$ and $RRRR$, meaning left-face and right-face clockwise $90$-degree turns, both restore balance, and so does $LLLLRRRR$; but $LRLRLRLR$ results in something that looks a bit like a swastika and should generally be avoided), while a Pyraminx is not. \n\n(The two puzzles also differ slightly in their levels of complexity and their general target audiences: the Pyraminx has 256 possible configurations, while the Rubik's Cube has $43,252,003,274,489,856,000 .$)\n\nPutting everything together:\n\n1. **Groups involve symmetries.** The group for a Rubik's Cube is generated from the six fundamental actions, the clockwise turns of each face by $90$ degrees, that leave the cube unchanged - symmetrical - except for the colors; these actions can be combined in any order and freely reversed. Groups are a collection of ways to modify an object, such that the object remains the same in some way: a simple rectangle\n\n    ![alt text](./assets/images/image-52.png)\n\n    or a molecule\n\n    ![alt text](./assets/images/image-51.png)\n\n    and so on and so on. A **Cayley diagram**, like the one pictured above, is a neat way to describe the actions present in the group and how they affect the original object.\n\n2. **Groups involve different types of symmetries.** For a Rubik's Cube, four face-turns are required to go back to the \"do-nothing\" symmetry; for a Pyraminx, the number is three. Some symmetries are commutative; others are not.\n\nAnd thus we begin!","n":0.028}}},{"i":101,"$":{"0":{"v":"Homomorphisms","n":1},"1":{"v":"> I ![alt text](./assets/images/SnapBG.ai_1743479810295.png){width: 25px} HOMO <br/><br/><br/><br/><br/><br/>morphisms\n\n(Would totally buy a T-shirt with that on it, by the way.)\n\n## Homomorphisms\n\n> Yes, yes, everyone's heard the jokes: \"local topologist orders a coffee and a donut, ends up biting the coffee mug\" this, \"local topologist wakes up, tries fitting their head through a fidget spinner for an hour, gets arrested for public nudity\" that. But have you considered the fact that people with a single ear piercing are already homomorphic to a pretzel?\n\n\"Homomorphism\" means \"same shape\": if two objects are homomorphic, they have the same underlying shape or structure - donuts and coffee mugs, each being characterized by only having a single hole. In very basic and blasphemously un-rigorous terms, we begin our definition with this notion in mind:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (but not quite yet.) A homomorphism is a comparison - or, in slightly fancier math-speak, a map - from one thing to another that establishes how these two things have the same underlying structure.\n\nThe \"things\" in question could be any two objects that has ever existed, as long as they have identical underlying structures; and the map that establishes the homomorphism is simply any action that connects the two objects. A sphere and a coffee mug, by means of deformation; the integers to the rationals, by means of division; the frog to the handsome prince, by means of true love's kiss. \n\nIn our case, though, we are concerned with groups; and more specifically, we are concerned with **preserving the structures** of groups. So what does the structure of a group entail? ~~(Based on the number of discord groups I've been in, I'd bet on unitary dictatorship.)~~\n\n![alt text](./assets/images/image-61.png) \n\nConsider the cyclic group of integers $\\text{mod }3$: the set $\\{0, 1, 2\\}$, nothing more. The **structure** of this group is given by how the elements of the group interact with each other: $1$ and $2$ combine to form $0$, $2$ and $2$ to form $1$, and so on. \n\nIn other words, the elements themselves aren't important; what matters is the relationship between them. So the question to be asked is: if we replace $0, 1$ and $2$ with $a, b$ and $c$ respectively (and, it must be emphasized, in that order), do we still preserve the same set of relationships - that $1 +2 = 0$ and so $b + c = a$, and everything else besides?\n\nThis leads us to our first proper definition of what a group homomorphism entails.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The elements of a group $(G, \\cdot_G, e_G)$ combine with each other through the binary operation $\\cdot_G$: to see how $a, b \\in G$ interact, consider $a \\cdot_G b$. These combinations form what are called the **underlying structure** of $G$. <br/><br/>\n> Suppose there exists another group $(H, \\cdot_H, e_H)$, with its own binary operation and identity element. Call $H$ and $G$ **homomorphic** if there is a map $\\phi$ that takes any three elements $g_1, g_2, g_3 \\in G$ to three elements $h_1, h_2, h_3 \\in H$ and preserves the relationship between them: that is, if $g_1 \\cdot_G g_2 = g_3$, then $h_1 \\cdot_H h_2 = h_3$.\n\nSlightly more elegantly: there is a function $\\phi$ that takes every element in $G$ to some element in $H$ such that $g_1 \\cdot_G g_2 = g_3$ implies $\\phi(g_1) \\cdot_H \\phi(g_2) = \\phi(g_3)$. Even more elegantly: \"show me your true form!\"\n\nWith the above description in hand, call $\\phi$ the **homomorphism** that maps $G$ to $H$.\n\n***\n\n> <span style=\"background-color: #03cafc; color: black;\">Examples</span>.\n1. $\\phi(x) = x$, $x \\in G$ is a homomorphism from $G$ to $G$. (Cool.)\n2. $\\phi(x) = x$, $x \\in H$ is a homomorphism from $H$ to $G$ if $H \\leq G$. (Cool.)\n3. As seen from the multiplication table above, $\\phi(x) = e^{x\\frac{2\\pi i}{n}}$ is a homomorphism from the cyclic group comprised of numbers $\\text{mod }n$ and the cyclic group of rotational symmetries of an $n$-gon.\n4. $\\phi(x) = \\frac{mx}{n}$ is a homomorphism from the cyclic group $\\text{mod }n$ to the cyclic group $\\text{mod }m$, if $m = kn$ for some positive integer $k$.\n\n***\n\nThe above definition of a homomorphism encompasses more than immediately meets the eye.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. Suppose that $\\phi$ is a group homomorphism between groups $(G, \\cdot_G, e_G)$ and $(H, \\cdot_H, e_H)$, denoted $\\phi: G\\to H$. Then the following two group axioms are preserved between $G$ and $H$:\n1. **Identity:** $\\phi(e_G) = e_H$. The homomorphism $\\phi$ must map the identity of $G$ to the identity of $H$.\n2. **Inverse:** $\\phi(g)^{-1} = \\phi(g^{-1})$ for $g \\in G$, where the first inverse is taken with respect to $\\cdot_H$ and the second with respect to $\\cdot_G$. The homomorphism $\\phi$ must map an inverse to the same inverse.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n1. **Identity.** For all $g\\in G$, we have $\\phi(g) \\cdot_H \\phi(e_G) = \\phi(g\\cdot_G e_G) = \\phi(g)$ by definition of a homomorphism. As such, we write\n    $$\n    \\phi(g) \\cdot_H \\phi(e_G) = \\phi(g), \\forall g \\in G\n    $$\n    which leads to $\\phi(e_G) = e_H$ by definition of the identity.\n\n2. **Inverse.** Consider\n    $$\n    \\begin{aligned}\n    \\phi(g \\cdot_G g^{-1}) &= \\phi(g) \\cdot_H \\phi(g^{-1}) \\text{ by definition of $\\phi$} \\\\\n    &= \\phi(e_G) \\text{ by $g\\cdot_G g^{-1} = e_G$} \\\\\n    &= e_H \\text{ by the above} \\\\\n    \\end{aligned}\n    $$\n    leading to $\\phi(g^{-1}) = \\phi(g)^{-1}$ by definition of the inverse.\n\n## Isomorphisms\n\nIn the simplest terms, homomorphisms are structure-preserving maps from one group to another; but nowhere does it say that for two groups to be homomorphic, they have to be the same size or correspond exactly to one another. A homomorphism $\\phi: G \\to H$ is a function with domain equal to the set of $G$ by definition, but its *codomain* (range) need not be the entirety of $H$; a subset is enough. In visual terms, the structure of $G$ is *embedded* within $H$.\n\n![alt text](./assets/images/image-62.png)\n\nIf instead we required the two groups to have the exact same size, and for $\\phi$ to have its codomain equal $H$ - for there to be a *one-to-one* correspondence between $G$ and $H$, we would have\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Isomorphisms** are a one-to-one correspondence between groups: if a homomorphism $\\phi$ is an invertible (one-to-one) function which has $G$ as its domain and $H$ as its codomain, call $\\phi$ an **isomorphism** and write $G \\cong H$.\n\nIsomorphisms map two groups to one another **exactly**: the groups must have the exact same size (due to invertible functions being one-to-one by nature), and if $g$ is mapped to $h$ by an isomorphism $\\phi$, then $h$ is mapped to $g$ by its inverse. Isomorphic groups reach the highest level of similarity between two groups without actually reaching equality: they have the same number of elements, and the relationships between these elements follow the same underlying structure. We also call the one-to-one mapping represented by $\\phi$ a **bijection.**\n\n## Image and Kernel\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Suppose that $G$ is homomorphic to $H$ via the homomorphism $\\phi: G \\to H$. Call the set represented by the codomain of $\\phi$\n$$\n\\text{Im}(\\phi) = \\{\\phi(g)\\ |\\ g \\in G\\}\n$$\n> the **image** of $\\phi$; and the set represented by all elements of $G$ that map to the identity of $H$\n$$\n\\text{Ker}(\\phi) = \\{g \\in G\\ |\\ \\phi(g) = e_H\\}\n$$\n> its **kernel**.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. The image of $\\phi$ is a subgroup of $H$; the kernel of $\\phi$ is a subgroup of $G$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nBy definition of the homomorphism $\\phi$ and the above lemmas, $\\phi(e_G) = e_H$, meaning that $\\text{Im}(\\phi)$ is non-empty. As such, we can apply our second lemma for confirming the existence of a subgroup. Consider $x, y \\in G$ and $\\phi(x), \\phi(y) \\in H$. We have\n$$\n\\phi(x) \\cdot_H \\phi(y)^{-1} = \\phi(x) \\cdot_H \\phi(y^{-1}) = \\phi(x \\cdot_G y^{-1})\\in H\n$$\nmaking $\\text{Im}(H)$ a subgroup of $H$.\n\nSimilarly, for $\\text{Ker}(\\phi)$ we know that $e_G \\in \\text{Ker}(\\phi)$ from the above lemmas, making the kernel non-empty; supposing that $x, y \\in \\text{Ker}(\\phi)$, we have\n$$\n\\phi(x \\cdot_G y^{-1}) = \\phi(x) \\cdot_H \\phi(y^{-1}) = e_H \\cdot_H (e_H)^{-1} = e_H\n$$\nand thus $x \\cdot_G y^{-1} \\in \\text{Ker}(\\phi)$, confirming that the kernel is a subgroup.\n\nThe notions of an image and kernel give us a useful way of confirming whether a homomorphism is an isomorphism.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. let $\\phi: G \\to H$ be a homomorphism from groups $G$ to $H$. $\\phi$ is an isomorphism if and only if $\\text{Im}(\\phi) = H$ and $\\text{Ker}(\\phi) = e_G$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLet's tackle the forward statement first. Suppose that $\\phi$ is an isomorphism; thus $\\phi$ is invertible, yielding \n$$\nh = \\phi(\\phi^{-1}(h))\n$$\nfor all $h \\in H$; therefore the image of $\\phi$ encompasses all of $H$. If an element $g\\in G$ satisfies $\\phi(g) = e_H$, then we have\n$$\n\\phi^{-1}(\\phi(g)) = \\phi^{-1}(e_H) = e_G\n$$\nas we know that $\\phi(e_G) = e_H$ by the isomorphism lemmas above. This provides the forward statement.\n\nConversely, let $\\text{Im}(\\phi) = H$ and $\\text{Ker}(\\phi) = e_G$. If $\\text{Im}(\\phi) = H$, then every element $h \\in H$ can be written $h = \\phi(g_h)$ for some $g \\in G$. Now define a new function $\\psi$, such that\n$$\n\\psi(h) = g_h, \\forall h \\in H\n$$\nwhich satisfies $\\phi(\\psi(h)) = h$ by definition. To prove that $\\psi$ is the inverse to $\\phi$, we must prove that it also satisfies $\\psi(\\phi(g)) = g$ for all $g \\in G$. We have\n$$\n\\phi(g^{-1}\\cdot_G \\psi(\\phi(g))) = \\phi(g^{-1}) \\cdot_H \\phi(\\psi(\\phi(g))) = \\phi(g)^{-1} \\cdot_H \\phi(g) = e_H\n$$\nand as such $g^{-1} \\cdot_G \\psi(\\phi(g)) = e_G$ by the fact that $\\text{Ker}(\\phi) = e_G$, meaning that $g = \\psi(\\phi(g))$. This leads to $\\psi = \\phi^{-1}$, and thus to the fact that $\\phi$ is invertible. $\\square$\n\n## A final word on cyclic groups\n\nAll this fuss about homomorphisms and isomorphisms allows us to study cyclic groups more closely: more precisely, the fact that its basic structure - a certain action, or *generator*, combining with itself to create the entire group - also underlies many other groups, or subgroups of many other groups.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. All cyclic groups $G$ are isomorphic to either the group $C_n$ (the group of rotational symmetries of an $n$-gon) or the *infinite-order cyclic group* $(\\mathbb{Z}, +, 0)$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nAn isomorphism requires an invertible function $\\phi$ that maps each element of $G$ to a unique element in $C_n$ (which has $n$ elements). As outlined by the theorem, there are two possible cases: either $G$ is isomorphic to a finite cyclic group, or to the infinite-order cyclic group $\\mathbb{Z}$.\n\nFirst consider the case that the group $G$ is finite, with $n$ elements including the identity; by definition of a cyclic group, suppose that $G$ has generator $a$: $G = \\{a^0, a^1, ..., a^{n-1}\\}$ with $a^n = e_G$. $n$ is thus the smallest natural number $k$ satisfying $a^k = e_G$; by definition we also have $a^{2n} = a^{3n} = ... = e_G$. Construct the homomorphism $\\phi: C_n \\to G$, with $C_n$ containing the elements $\\{\\omega^0, \\omega^1, ..., \\omega^{n-1}\\}$ with $\\omega = e^{\\frac{2\\pi i}{n}}$, such that\n$$\n\\phi(\\omega^k) = a^k\n$$\nfor $k = 0, 1, ..., n-1$. This satisfies the basic condition for a homomorphism:\n\n$$\n\\phi(\\omega^{m} \\cdot_C \\omega^{n}) = \\phi(\\omega^{m+n}) = a^{m+n} = a^m \\cdot_G a^n = \\phi(\\omega^m) \\cdot_G \\phi(\\omega^n)\n$$\n\nRecall that for a homomorphism to be an isomorphism $C_{n} \\to G$, two conditions are necessary and sufficient: first that the homomorphism $\\phi$ has image equalling $G$, and second that $\\phi$ has kernel $e_{C_n} = \\omega^0$. \n\nThe first condition is satisfied by construction; $\\phi(\\omega^k) = a^k$, and thus $\\phi$ has an image encompassing all of $G$. Assume for the sake of contradiction that $\\text{Ker}(\\phi) \\neq \\{\\omega^0\\}$; then for some $0 \\leq k \\leq n-1$, we have $\\phi(\\omega^{k}) = a^{k} = e_G$ by construction of $\\phi$. As  $n$ is the smallest natural number $k$ satisfying $a^k = e_G$, this reaches a contradiction; thus $\\phi$ is an isomorphism.\n\nIf $G$ is instead infinite, consider the same homomorphism $\\phi$, now from $(\\mathbb{Z}, +, 0)$ to $G$ instead:\n$$\n\\phi(k) = a^k\n$$\nfor all $k \\in \\mathbb{Z}$. As above, this also satisfies the basic requirements for a homomorphism; the same argument involving $\\text{Ker}(\\phi)$ also reaches a contradiction, leading to $\\phi$ being an isomorphism. $\\square$\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. For any group $G$ and a group element $g \\in G$, define the **order** of $g$, denoted $\\text{ord}(g)$, as the smallest number $n$ satisfying $g^n = e_G$. If no such number exists, we say that $g$ has **infinite order**: $\\text{ord}(g) = \\infty$.\n\nThe subset of $G$ generated entirely by powers of $g$ - $\\{g^0, g^1, ..., g^n\\}$ - forms a cyclic subgroup of $G$ that is isomorphic to $C_n$.","n":0.022}}},{"i":102,"$":{"0":{"v":"Group Families","n":0.707},"1":{"v":"> Remember to smile and say \"cheese.\"\n\nThere are many different types, or \"families\", or groups. Five simple ones - cyclic, symmetric, alternating, abelian and dihedral; and all the rest, from Lie groups to the Tits group to the simple \"gentle giant\" group which has order $808 017 424 794 512 875 886 459 904 961 710 757 005 754 368 000 000 000$; from the Cox ring to the Mordell-Weil group of elliptic surfaces, whose basis is provided by the Cox-Zucker machine. \n\nThis chapter is not concerned with any of those special cases, all of which have names that sound like either a '90s emo-rock boy-band, a stage-name for a WWE wrestler, or the pseudonym of an adult-film actor. Instead, we're concerned with **symmetry groups**: groups that arise from all the ways you could spin, flip, and kick around a physical object and respect its symmetries by still keeping its shape the same. We start off with:\n\n\n## Dihedral groups\n\n> ![alt text](./assets/images/SnapBG.ai_1743601080724.png){width: 25px}: *\"Say my name.\"*<br/>\n> \"You're... you're ${\\displaystyle (\\mathbb {Z} _{3}^{7}\\times \\mathbb {Z} _{2}^{11})\\rtimes \\,((A_{8}\\times A_{12})\\rtimes \\mathbb {Z} _{2}).}$\" <br/>\n> *\"You're **goddamn right.**\"*\n\nDihedral groups describe the symmetries of a regular $n$-gon (in order: the pentagon, the hexagon, the heptagon, and the whyamIstillstudyingmathnowmylifeisgon.) To be exact, define such a shape geometrically as follows:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Call a **regular $n$-gon** a collection of $n$ vertices in the complex plane, given in counterclockwise order starting from the real axis by\n$$\n1, e^{\\frac{2\\pi i}{n}}, e^{2\\frac{2\\pi i}{n}},..., e^{(n-1)\\frac{2\\pi i}{n}}\\dots\n$$\n![alt text](./assets/images/image-59.png)\n\nThus define the dihedral group in terms of the *isometries* of the $n$-gon, as follows:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. An **isometry** of a physical object (e.g. a polygon) is any transformation upon it that maintains the distances between its vertices: something that preserves the exact shape of the object and does not deform, stretch, or compress it in any way. This could be a translation, a reflection, or a rotation, to name just a few examples. <br/><br/>\nThe **dihedral group** of an $n$-gon, denoted $D_{2n}$, is the group of all actions performable on the $n$-gon within the complex plane $\\mathbb{C}$ that send the $n$-gon to itself; not only do these actions preserve distances, but they keep the $n$-gon in the exact same position.\n\nFor instance, notice that the polygon pictured above is symmetrical about the $\\text{Re}$-axis; as such, a reflection about that axis will map the polygon to itself. A rotation about the origin by one of the interior angles - $900/7$ degrees for a septagon - also satisfies this requirement.\n\nMore formally, suppose that each of these isometries can be described as functions of complex numbers: $f, g: \\mathbb{C \\to C}$ that map one complex number to another, mapping all the points of the $n$-gon to the exact same points - though not necessarily in the same order. Thus we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Let the dihedral group of a $n$-gon $D_{2n}$ - the $n$th dihedral group - be defined formally as $(D_{2n}, \\circ, Id)$ where the binary operation $f \\circ g$ for isometries $f, g \\in D_{2n}$ entails \"performing $g$ followed by $f$\", or a composition of $f$ and $g$; and $Id$ is the identity isometry, given by the complex-valued function $Id(x) = x$. <br/><br/>We propose that the triple $(D_{2n}, \\circ, Id)$ satisfies all of the group axioms and has order $2n$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLet's verify the group axioms one by one.\n\n1. **Closure.** Suppose that $f$ and $g$ are both elements of $D_{2n}$; by definition, they map the $n$ vertices of the $n$-gon - $d_1, d_2, ..., d_n$ - to $d_1, d_2, ..., d_n$, though not necessarily in that order. As such,\n    $$\n    (f \\circ g)(d_1, ..., d_n) = f(g(d_1, ..., d_n)) = f(d_1, ..., d_n) = d_1, ..., d_n\n    $$\n    playing **extremely** fast and loose with notation ($f(d_1, ..., d_n)$ means not what you think it does, but instead the $n$ complex numbers $f(d_1), f(d_2), ..., f(d_n)$.) Therefore, $f \\circ g$ is also an isometry and is an element of $D_{2n}$.\n\n2. **Associativity.** $\\circ$ is a composition of functions, and is thus associative.\n3. **Identity.** The isometry $Id$ is an identity by definition.\n4. **Inverse.** The two functions $f$ and $g$ that preserve the $n$-gon are:\n- *Rotation by* $\\frac{2k\\pi}{n}$, given by $f(z) = z \\cdot e^{\\frac{2k\\pi i}{n}}$ for integers $k = 0, 1, 2, ..., n-1$. For example, $f(z) = z \\cdot e^{\\frac{2\\pi i}{n}}$ maps the point $1$ to $e^{\\frac{2\\pi i}{n}}$, $e^{\\frac{2\\pi i}{n}}$ to $e^{\\frac{4\\pi i}{n}}$, and so on; until evey point is mapped to the next point going counterclockwise. This is a distance-preserving isometry because\n    $$\n    |f(z_1) - f(z_2)| = |e^{\\frac{2k\\pi i}{n}}||z_1 - z_2| = |z_1 - z_2|\n    $$\n    and is thus a member of $D_{2n}$.\n- *Reflection about the real axis*, given by $g(z) = \\bar{z}$, the complex conjugate. This is also an isometry, as\n    $$\n    |g(z_1) - g(z_2)|^2 = (\\bar{z_1} - \\bar{z_2})\\overline{(\\bar{z_1} - \\bar{z_2})} = (z_1 - z_2) \\overline{(z_1 - z_2)} = |z_1 - z_2|^2\n    $$\n    and thus $|g(z_1) - g(z_2)| = |z_1 - z_2|$.\n\n- Both the reflection and the rotation have well-defined inverses that are contained within $D_{2n}$: an arbitrary rotation has $f^{-1}(z) = z\\cdot e^{-k\\frac{2\\pi i}{n}} = z \\cdot e^{(n-k)\\frac{2\\pi i}{n}} \\in D_{2n}$, and the reflection is its own inverse.\n\nNow all that's left to prove is that the rotation and the reflection are the **only two possible elements of $D_{2n}$.**\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. $D_{2n}$ is entirely described by the set $\\{Id, f, f^2, ..., f^{n-1}, g, fg, f^2 g, ..., f^{n-1} g\\}$, where $f$ denotes rotation by $\\frac{2\\pi}{n}$ counterclockwise, $g$ denotes reflection across the real axis, and $Id$ is the identity isometry. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\nSuppose that $h \\in D_{2n}$ is an isometry in $\\mathbb{C}$. By the properties of $D_{2n}$, we know that as $1$ is a point on the $n$-gon, $h(1)$ must also be a point on the $n$-gon - $h(1) = e^{k\\frac{2\\pi i}{n}}$ for $k = 0, 1, ..., n-1$. Thus we have\n$$\n(f^{-k} \\circ h)(1) = f^{-k}(h(1)) = e^{-k\\frac{2\\pi i}{n}}e^{k \\frac{2\\pi i}{n}} = 1\n$$\nwhere $f$ is rotation by $\\frac{2\\pi}{n}$; as such the transformation $r = f^{-k} \\circ h \\in D_{2n}$ keeps $1$ invariant. As $r$ is an element of $D_{2n}$, it is an isometry; and thus the distance between $r(1)$ and $r(e^{\\frac{2\\pi i}{n}})$, a vertex connected to $1$ by a single edge, must remain the same as the distance between $1$ and $e^{\\frac{2\\pi i}{n}}$ originally. $r(e^{\\frac{2\\pi i}{n}})$ thus shares an edge with $r(1) = 1$; therefore either \n$$\nr(e^{\\frac{2\\pi i}{n}}) = e^{\\frac{2\\pi i}{n}}\n$$\nin whose case $r$ must be the identity isometry $Id$, via the argument that $r$ preserves distances and $r(e^{-\\frac{2\\pi i}{n}})$ must still share an edge with $1$ but cannot be $r(e^{\\frac{2\\pi i}{n}}) = e^{\\frac{2\\pi i}{n}}$, and thus $r(e^{-\\frac{2\\pi i}{n}}) = e^{-\\frac{2\\pi i}{n}}$; and so on and so forth with all other vertices; as such \n$$\nr = f^{-k} \\circ h = Id,\\ h = f^k\n$$\nfor $k = 0, 1, ..., n-1$; or, alternatively,\n$$\nr(e^{\\frac{2\\pi i}{n}}) = e^{-\\frac{2\\pi i}{n}}\n$$\nmeaning that \n$$\ns = (g\\circ r)(e^{\\frac{2\\pi i}{n}}) = e^{\\frac{2\\pi i}{n}}\n$$\nwhere $g$ is a reflection about the real axis; as such $s$ keeps both $e^{\\frac{2\\pi i}{n}}$ and $1$ invariant, and so by the above argument with $r$, $s$ is also the identity. Therefore\n$$\ng \\circ r = Id,\\ r = f^{-k} \\circ h = g^{-1} = g,\\ h = f^k\\circ g \n$$\nproving the general result. Thus $D_{2n}$ is completely described by the set $\\{Id, f, f^2, ..., f^{n-1}, g, fg, ..., f^{n-1}g\\}$ with a total of $2n$ elements.\n\n> It's worth noting that $D_{2d}$ is **not** abelian: for instance, a rotation followed by a reflection is not the same as a reflection followed by a rotation.\n\n## Permutation groups\n\n> How many ways can you arrange the letters of the word *KILL-ME-NOW*, ensuring that the dashes are not adjacent to each other and the knife is not adjacent to the student?\n\nThe puns strike again in [[Groups.Permutation Groups.Permutations and Cycles]]!\n\n## Cyclic groups\n\n> I knew a group theorist who once fought in World War II. Four sticks of C-4 were thrown at his feet and he literally just spun around in a circle without moving. Anyways, he's dead now.\n\n*Cyclic groups* describe objects with rotational symmetry: triangles, benzene molecules, Shrek's head, and German windmills. Of course they can't call them rotational groups; that honor goes to $SO(2)$, which does a great job at describing how a circle rotates and a piss-poor job at anything else. Not \"symmetric groups\", either; that title is reserved solely for permutation groups, which contain all the seventy bazillion ways you could take a list of things and scramble them up to make them less symmetric. Who the hell came up with these names!?\n\nThat's right, Galois did. Of course it was him. Now if anyone ever asks me \"if you could have dinner with any historical figure, alive or dead, who would it be?\", I would tell them 1) Galois, and 2) pretty fricking dead. \n\n> Galois died at the age of 20 from a newly-acquired diet of a bullet to the stomach. Contrary to what historians have posited, his death in a duel was no accident; in fact, it was due to two tortured math undergrads from the far future who invented a time machine solely to convince Galois to fight and stop him from unleashing any further horrors upon our world.\n\nCyclic groups are differentiated from dihedral groups in that they do not contain reflections; more importantly, however, is a single property that lends them the name \"cyclic\".\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **cyclic group** is a group with a **generator element** $\\omega$ whose set contains only the powers of $\\omega$:\n$$\n\\{\\omega^0 = Id, \\omega^1, ..., \\omega^{n}, ...\\}\n$$\n> where $\\omega^k$ for some $k \\in \\mathbb{Z}$ denotes repeating the binary operation of the group $k$ times with $\\omega$ on itself.\n\nA special case of cyclic groups is the group $C_n$ described by the set of $n$th roots of unity, under multiplication as its binary operation and $1$ as its identity:\n$$\nC_n = (\\{z \\in \\mathbb{C}\\ |\\ z^n = 1\\}, \\times, 1)\n$$\n> More precisely, $C_n$ permits the elements $1, e^{\\frac{2\\pi i}{n}} , e^{2\\frac{2\\pi i}{n}}, ..., e^{(n-1)\\frac{2\\pi i}{n}}$, with the property that with $\\omega = e^{\\frac{2\\pi i}{n}}$, we have\n$$\nC_n = \\{\\omega^0, \\omega^1, ..., \\omega^{n-1}\\}.\n$$\nIn this case, $\\omega$ - the smallest non-identity rotation in the group, that by $\\frac{2\\pi}{n}$ radians counterclockwise - is the *generator* of $C_n$; by definition of $\\omega$ as a root of unity, $n$ repeated actions of $\\omega$ on an object cycles back to the identity - $\\omega^n = 1$. Such a group represents the complete list of the rotational symmetries of an $n$-gon.\n\nIt's worth noting, though, that cyclic groups need not be related to rotations. Consider the set of all numbers that exist in $\\text{mod }5$:\n$$\nC_5 = (\\{0, 1, 2, 3, 4\\}, +, 0)\n$$\nin which case each element is a \"power\" of the generator $1$ (with $a^k$ being defined as $a + ... + a$ $k$ times with addition being the binary operation to be repeated, and $a^0 = 0$). Under such a definition, we have $1^5 = 1 + 1 + 1 + 1 + 1 = 0 \\text{ mod 5}$ - looping back to the identity. This is rather colorfully depicted by the multiplication table below:\n\n![alt text](./assets/images/image-60.png)\n\nIn fact, $\\mathbb{Z}$ itself - or any group of the form $n\\mathbb{Z}$, for that matter - is a cyclic group; a set of (additive) powers of $n$ that go on to infinity instead of looping back to bite its own tail like a better-behaved cyclic group would.\n\nIt's very much worth noting here, as a preface to things to come, that cyclic groups form what could be considered the basic building blocks of group theory: they are themselves formed from a single element - its generator - and in any other group, where if $a$ is an element then $a^k$ is an element by definition, the set\n$$\n\\{Id, a, ..., a^{n-1}\\}\n$$\nwhere $a^n = Id$ is itself a cyclic group by definition. In fact, all abelian groups - themselves one of the simplest types of groups - can be constructed out of cyclic groups! To explore this concept more, we turn to one final idea to cap off the entire chapter.","n":0.022}}},{"i":103,"$":{"0":{"v":"Basic Definitions","n":0.707},"1":{"v":"\n> $e$\n\n## What are groups?\n\nRecall our informal definition for what a group is, distilled and contextualized within the borders of a Rubik's Cube:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **group** is a set of **actions** (moves) $G$, a **binary operation** (a means of combining such actions) $*$, and an **identity** (\"do-nothing\" action) $e$ satisfying the following four rules:\n1. A combination of two actions $a$ and $b$, denoted $a * b$, is still a valid action that is a member of $G$. (This is true for any sequence of consecutive actions.)\n2. **Inverse.** Every action $a$ has its inverse $a^{-1}$, such that $a * a^{-1} = e$, the \"do-nothing\" action.\n3. **Identity.** The \"do-nothing\" action $e$ does nothing: $a * e = a$.\n4. **Associativity.** Actions are **associative**: $(a * b) * c = a* (b*c)$.\n\nThese are known as the **axioms** of a group; they encompass everything that's needed to define what a group is.\n\nAll that's left to define a group *properly* (to be pronounced in a proper Westminster accent, infused with the appropriate amount of humble snobbery and sounding for all the world like it'll colonize you and ransack you for spices then leave you with seven decades of crippling post-independence debt) is the following list of formalities:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Formally denote a group as the triplet $(G, \\cdot, e)$, where:\n1. The elements of the set $G$ correspond to the actions above;\n2. The element $e \\in G$ is the **identity element**; and\n3. $\\cdot$ denotes the associative binary operation.\n\n> (As an aside, why is associativity a fundamental requirement of groups and commutativity not so? Because groups are collections of actions, and actions - things that take something and modify that thing, also known as *functions* - are always associative:\n$$\n(f\\circ g)\\circ h(x) = f(g(h(x))) = f\\circ (g\\circ h(x))\n$$\n> by definition that $f \\circ g(x) = f(g(x))$.)\n\nAs Smash Mouth famously opined in his hit single \"All Star\", when the definitions start coming, they don't stop coming. Digging deeper into the four given axioms reveals other intrinsic properties true for every group:\n\n\n\n> <span style=\"background-color: #ffb812; color: black;\">Propositions</span>. **Basic group properties**.\n\n> <span style=\"background-color: #ffb812; color: black;\">Property 1</span>. The identity element $e$ and the inverse $a^{-1}$ to any $a \\in G$ are both unique.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nSuppose there are two elements of the group $G$, denoted $e$ and $e'$, that both satisfy $e \\cdot a = a$ and $e' \\cdot a = a$ for every element $a \\in G$. Then we have\n$$\ne \\cdot e' = e\n$$\nand\n$$\ne \\cdot e' = e'\n$$\nboth by definition of the identity, and so $e = e'$.\n\n***\n\nSimilarly, suppose that $g$ and $g'$ are both inverses of $a$, such that $ag = ag' = e$. Denote also an inverse of $a$ as $a^{-1}$ (any one of $g$ or $g'$). By the inverse axiom, we have\n$$\nag = ag' = e \\iff a^{-1}ag = a^{-1}ag' \\iff g = g'.\\ \\square\n$$\n\n(We're making the business decision to drop the $\\cdot$ symbol like sponsors around the world dropping Kanye West after his seventieth tweet about Jewish space lasers.)\n\n> <span style=\"background-color: #ffb812; color: black;\">Property 2</span>. Inverses and identities are both commutative: if $a^{-1} \\in G$ is an inverse of $a$ in the group $(G, \\cdot, e)$, then $a^{-1} a = a a^{-1} = e$. Simultaneously, $ae = e a = a$. \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nFirst we prove that inverses are commutative: $aa^{-1} = a^{-1}a = e$. This is equivalent to proving that $a$ is the inverse of $a^{-1}$ if $a^{-1}$ is the inverse of $a$. We have, by the inverse axiom,\n$$\ne = a^{-1}(a^{-1})^{-1}\n$$\nwhere we do not yet know that $(a^{-1})^{-1} = a$ yet. Thus\n$$\n\\begin{aligned}\ne = a^{-1}(a^{-1})^{-1} &= (a^{-1}e)(a^{-1})^{-1} \\\\\n&= (a^{-1}aa^{-1})(a^{-1})^{-1} \\\\\n&= a^{-1}a(a^{-1}(a^{-1})^{-1}) \\\\\n&= a^{-1}ae \\\\\n&= a^{-1}a,\n\\end{aligned}\n$$\nrelying simply on associativity. \n\n> A corollary of the uniqueness of inverses is that, as $a^{-1} a = e$, we have the inverse of $a^{-1}$, $(a^{-1})^{-1}$, equalling $a$.\n\nTihis allows us to prove also that the identity is commutative. By the identity axiom, we have\n$$\n\\begin{aligned}\nae &= a \\\\\na(a^{-1} a) &= a \\text{ using the above,} \\\\\n(aa^{-1})a &= a \\\\\nea &= a\n\\end{aligned}\n$$\nfor all $a \\in G$.\n\n## Abelian groups and subgroups\n\n![alt text](./assets/images/image-53.png)\n\n> Get me out of here.\n\nAbelian groups are the squares in a world of rectangles. The circles in a world of ovals. The vanilla ice-cream in a world of wasabi soy-sauce split and garlic soft-serve. The Keir Starmer in a world of Boris \"Long John\" Johnsons. Vanilla is the best flavor of ice cream and I will die on that hill; and that's why we define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. An **abelian** group satisfies all of the above four group axioms - associativity, inverse, identity, and closure - alongside **commutativity**: for any two elements $a, b$ in an abelian group $G$ with binary operation $\\cdot$, $a\\cdot b = b \\cdot a$.\n\nYou may have noticed that \"abelian\" is not capitalized, while things like \"Gaussian\" or \"Euclidean\"\" frequently are. This is likely because Abel's head hasn't been commemorialized in print on a 20-euro-banknote yet, of which a possible contributing factor is his death at the ripe old age of 26. Galois, who followed him, cemented his position in the Pantheon of mathematics in a single stroke then proceeded to also keel over from some obscure duel at 21 years old. \n\nAre you sensing a pattern yet? Because I am, and if a tragic indoors car-crash dismembers every single participant of this Group Theory class sometime next year, I promise I'll pay for your funeral.\n\nAbelian groups are the most familiar - and, for the most part - least traumatizing - sub-example of groups. To list just a few examples, all of $(\\mathbb{R}, +)$, $(\\mathbb{Q}, +)$, and $(\\mathbb{N}, +)$ are abelian groups; as is $(\\mathbb{R^*}, \\times)$ where $\\mathbb{R^*}$ is the reals excluding $0$; the group of octonions, quaternions, duonions (complex numbers), onions, etc., etc.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Denote by $|G|$ the **order** of the group $(G, \\cdot, e)$: the number of elements within the set $G$. If $|G|$ is finite, call its corresponding group a **finite group**; otherwise, call it an **infinite group**.\n\nAnd that's a wrap!\n\n## Visualizing groups\n\nOne last comment on our well-meaning but ultimately foolish and misguided attempts at visualizing groups. Earlier, we introduced what was called a **Cayley diagram** for the group of all actions on a physical object that preserves its symmetries, which looked a little something like this:\n\n1. Begin with the object in front of you.\n\n![alt text](./assets/images/image-54.png)\n\n2. Draw numbers on the physical object. (If it's alive, please obtain its full consent beforehand.)\n\n![s](./assets/images/image-55.png)\n\n3. Think of every move that keeps the object occupying the same space while potentially shuffling the numbers.\n\n![alt text](./assets/images/image-56.png)\n\n4. Draw all $250! \\approx 1.08 \\times 10^{490}$ possible combinations of moves, preferably contained within one A4 sheet of paper or Word document for easier distribution.\n   \n5. ???\n\n6. Profit!\n\n![alt text](./assets/images/image-57.png)\n\nThis is all well and good, but what if we wanted to know not only how each individual action affects the object, but how two actions can combine together - and what patterns exist in such combinations, e.g. how four turns equals the identity for a Rubik's Cube? This necessitates the use of a **multiplication table**, also known as a **Cayley table**:\n\n![alt text](./assets/images/image-58.png)\n\nThese tell us how two actions in the set $G$ combine together, color-coded such that identical outcomes are marked with the same color. We sometimes prefer them to Cayley diagrams because they have pretty colors annd make our brains squirt out all the feel-good chemicals.\n\n","n":0.029}}},{"i":104,"$":{"0":{"v":"Group Actions","n":0.707},"1":{"v":"> \"Group action\" is a good term to describe whatever goes on every Friday at 9:30 p.m. sharp in Classroom 301 when all the lights are off.\n\n","n":0.192}}},{"i":105,"$":{"0":{"v":"Orbits and Stabilizers","n":0.577},"1":{"v":"\n> (Insert really terrible yo mama joke about orbits here)\n\nOrbits and stabilizers represent analogues to a function's range and its fixed-points for group actions. In particular, the two relate together in a way that gives us insight into the sizes of groups that act on particular sets.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **orbit** of an element $x \\in X$, denoted $\\text{orb}(x)$, is the set of all elements that $x$ can be mapped to by a group action from $G$ on $X$ represented by the homomorphism $\\phi$:\n$$\n\\text{orb}(x) = G(x) = \\{y \\in X\\ |\\ \\phi(g)(x) = y,\\ g \\in G\\}\n$$\n\n> Additionally, call the action defined above **transitive** if the orbit of all $x \\in X$ is the entire set $X$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **stabilizer** of ane element $x \\in X$, denoted $\\text{stab}(x)$, is the set of all $g \\in G$ that keep $x$ the same under the group action $\\phi$ (as above).\n\n$$\n\\text{stab}(x) = G_x = \\{g\\ |\\ \\phi(g)(x) = x,\\ g\\in G\\}\n$$\n\nIncidentally, \"stab $x$\" is also what my sixth-grade self wanted to do after getting assigned one too many algebra worksheets for homework.\n\nJust as the image and kernel of a homomorphism form subgroups, we have\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. The stabilizer of $x$ is a subgroup of $G$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. Let $a, b \\in \\text{stab}(x)$. Then the element $a \\cdot_G b^{-1}$ acts on $x$ via $\\phi$ as follows:\n$$\n\\phi(a\\cdot_G b^{-1})(x) = \\phi(a)\\phi(b^{-1})(x) = \\phi(a)(x) = x\n$$\nas $\\phi(b^{-1})(x) = \\phi(b)^{-1}(x) = x$ via\n$$\nb \\in \\text{stab}(x) \\iff \\phi(b)(x) = x \\iff x =\\phi(b)^{-1}(x) \n$$\nby definition of an inverse. Thus $a \\cdot_G b^{-1} \\in \\text{stab}(x)$. $\\square$\n\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. The orbits of all elements $x \\in X$ partition $X$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThe definition of a partition is twofold:\n1. The unions of the orbits of different $x$ is $X$. By definition, $\\text{orb}(x)$ contains the element $\\phi(e)(x) = x$ and so each orbit of $x$ contains $x$ itself. The union of all $x$ forms the entire set $X$.\n2. The orbits of $x \\in X$ do **not** have to be of the same size, but they have to be disjoint: the intersection of two orbits is the null set. Suppose that $a \\in \\text{orb}(x)$ and $a \\in \\text{orb}(y)$ for $x \\neq y \\in X$. Then we have\n$$\na = \\phi(g_1)(x) = \\phi(g_2)(y)\n$$\nAnd thus\n$$\n\\phi(g_2)^{-1}\\phi(g_1)(x) = \\phi(g_2^{-1}\\cdot_G g_1)(x) = y\n$$\nyielding\n$$\ny \\in \\text{orb}(x)\n$$\nby definition. Thus for any $w \\in \\text{orb}(y)$, we have\n$$\nw = \\phi(g)(y) = \\phi(g)\\phi(g_2^{-1}\\cdot_G g_1)(x) \\in \\text{orb}(x)\n$$\nand $w \\in \\text{orb}(x)$; this means that $\\text{orb}(y) \\subset \\text{orb}(x)$, but the same argument in reverse establishes $\\text{orb}(x) \\subset \\text{orb}(y)$. $\\square$\n\n***\n\nNow here's the big one. The larger the stablizer of an element $x \\in X$, the lower the number of elements $g \\in G$ that can move it around via the group action; and the lower the number of elements that move $x$ around, the lower the number of possibilities for where $x$ can be moved - i.e. the size of the orbit of $x$. As such, we expect that the size of the orbit is \"inversely proportional\" - in some vague, informal sense - to the size of the stabilizer; and indeed\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. (THE BIG ONE.) **Orbit-Stabilizer Theorem**, ~~named after co-discoverers John Orbit and Edward Stabilizer in 1887.~~ Let a group $G$ act on a set $X$ by means of a group action. Then the size of the orbit of an element $x \\in X$ under that group action multiplied by the size of the stabilizer of $x$ is the size of $G$:\n$$\n|\\text{orb}(x)||\\text{stab}(x)| = |G|.\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. \n\n**Proof sketch**. When a proof needs a TL;DR right before it, you know it's some hardcore stuff. We utilize Lagrange's Theorem along with the fact that $\\text{stab}(x)$ is a subgroup of $G$ to obtain\n$$\n|\\text{stab}(x)||G:\\text{stab}(x)| = |G|\n$$\nwhere $|G:\\text{stab}(x)|$ is the index of $\\text{stab}(x)$, equal to the number of unique cosets formed by $\\text{stab}(x)$. Comparing with the equation above necessitates showing that $|G:\\text{stab}(x)| = |\\text{orb}(x)|$, which can be demonstrated via a bijection between the cosets of $\\text{stab}(x)$ and the elements of $\\text{orb}(x)$.\n\nTo prove this bijection, we require\n\n1. Every left coset of $\\text{stab}(x)$ corresponds to exactly one element of $\\text{orb}(x)$. Let $g\\ \\text{stab}(x)$ be a coset of $\\text{stab}(x)$ with $g \\in G$; any element $gh \\in g\\ \\text{stab}(x)$ has \n$$\n\\phi(gh)(x) = \\phi(g)\\phi(h)(x) = \\phi(g)(x)\n$$\nas $\\phi(h)(x) = x$ by definition of the stabilizer. Thus the left coset $g\\text{ stab}(x)$ corresponds to $\\phi(g)(x)$, an element of $\\text{orb}(x)$.\n\n2. Every element of $\\text{orb}(x)$ corresponds to exactly one element of $\\text{stab}(x)$: $\\phi(g)(x)$ corresponds only to $g\\ \\text{stab}(x)$, and no other coset. Suppose that an element $g'h \\in g'\\text{ stab}(x)$ with $g' \\neq g$ has\n$$\n\\phi(g' h)(x) = \\phi(g)(x) \\iff \\phi(g') = \\phi(g)\n$$\nAnd so\n$$\n\\phi((g')^{-1}g) = e\n$$\nand thus\n$$\n\\phi((g')^{-1}g)(x) = x,\\ (g')^{-1}g \\in \\text{stab}(x)\n$$\nand as such\n$$\ng'\\ \\text{stab}(x) = g'(g')^{-1}g\\ \\text{stab}(x) = g\\ \\text{stab}(x)\n$$\nestablishing the bijection. $\\square$","n":0.035}}},{"i":106,"$":{"0":{"v":"Group Actions","n":0.707},"1":{"v":"## What are group actions?\n\nAt the very beginning of this course, we were instructed to de-abstractify groups by thinking of them as something that rotates a Rubik's Cube. (Rubik's Cubes were only invented in the 1970s, which means people in the 1960s probably learned group theory using disco balls.) Then we were taught the virtues of groups as abstract objects, where we shoved about $7^{10^{e^{69}}}$ different definitions of amusing words like \"homomorphisms\" down our throats and refused to acknowledge the existence of real-world things like triangles or cubes. \n\nUnfortunately, now we have to think of groups as something that rotates or flips or changes a real-world object again. This course is flip-flopping more than an American politician during election season, and it's cutting down on my already-limited will to live.\n\n> I HAVE A THEORY. What if Abel, Galois, etc., etc., didn't die of stupid things like getting sick or going to a duel, but instead died out of disgust from studying group theory too much? I have a sample size of 2, so it's a scientifically-proven fact - group theorists all die tragically eaarly deaths.\n\nThe permutation group contains all the ways you could change the order of a set. The dihedral group contains all the symmetries of a polygon. When a group element is interpreted as a way to modify some object such that the remains *symmetric*, i.e. the modification preserves some structure of the object - the number of elements in a set, the space a polygon occupies - we call it a\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Group action.** A **group action** of a group $G$ on a set of $n$ elements $X$ (which may represent letters, numbers, the vertices of a polygon, etc., etc.) is a way to interpret every element of $G$ as a permutation of that set. More formally, it is the homomorphism\n$$\n\\phi(g),\\ \\phi: G \\to \\text{Sym}(X)\n$$\n> where $\\text{Sym}(X) = S_n$ is the symmetric group of $X$, containing all possible permutations of the $n$ elements of $X$. \n\n(A quick *nota bene*: \"symmetry\", i.e. \"symmetric group\", means in this context \"something which maps a set onto itself\" - i.e. keeping the stuff inside something exactly the same, though not necessarily the order. Just like what that one back-alley doctor did to my organs after an appendicitis surgery.)\n\nFor instance, suppose that $\\{1,2,3\\}$ represent the three vertices of a triangle placed upright in clockwise order, with $1$ at the top; then the element $e^{\\frac{2i\\pi}{3}}$, a rotation by 120 degrees counterclocckwise, would permute the set of vertices into $\\{2,3,1\\}$. Thus, it maps straightforwardly onto the permutation $(123)$; and so do all other elements of either the cyclic group $C_3$ or the dihedral group $D_6$. The simplest example of a group action is the *trivial action*: every element in $G$ maps to the \"do-nothing\" permutation. This is an action that says it's an action but does nothing in reality, which is what my mom says about me every weekend.\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. $\\theta: G \\times X \\to X$, defined by $\\theta(g,x) = \\phi(g)(x)$ where $\\phi$ is a function mapping from $G \\to \\text{Sym}(X)$, is a group action if and only if it satisfies the following three properties:\n1. For all $g \\in G$ and $x \\in X$, $\\theta(g,x) \\in X$.\n2. For all $x \\in X$, $\\theta(e_G, x) = x$.\n3. For all $g,h \\in G$ and $x \\in X$, $\\theta(gh,x) = \\theta(g,\\theta(h,x))$; the action $gh$ is equivalent to the action $h$ followed by $g$.\n\n\n(Remember that $X$ is not a group; it is a set!)\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nIn essence, our proof needs to show two things: 1) that the above three properties are necessary and sufficient to define a homomorphism $\\phi$, and 2) $\\phi$ maps from $G$ to $\\text{Sym}(X)$. From the last property, we have\n$$\n\\phi(gh)(x) = \\phi(g)\\phi(h)(x)\n$$\nand thus\n$$\n\\phi(g\\cdot_G h) = \\phi(g) \\phi(h)\n$$\n\n\nNow to prove that $\\phi(g)$ maps to $\\text{Sym}(X)$. Define $\\phi(g)(x)$ as a function that maps $x$ to $\\theta(g,x)$; by definition, this is a permutation on $X$ if it maps from the set $X$ to itself and is also invertible (a bijection, i.e. one-to-one - permuting every element in $X$ to another unique element). By rule (1), $\\theta(g,x)$ maps to $X$; by rule (2), we have\n$$\n\\theta(e_G, x) = \\theta(gg^{-1}, x) = \\theta(g,\\theta(g^{-1}, x))=\\phi(g)\\phi(g)^{-1}(x) = x\n$$\nfor any $g \\in G$, thus making $\\phi(g)^{-1}$ the inverse of $\\phi(g)$ with $\\phi(g)$ being invertible. Therefore, $\\phi(g) \\in \\text{Sym}(X)$. \n\nThe reverse statement results by a straightforward application of the definition of a homomorphism.\n\n***\n\nSimilarly to homomorphisms, we define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Kernel** of an action. The **kernel** of a group action $G$ on $X$, given by a homomorphism $\\phi: G \\to \\text{Sym}(X)$, is the kernel of the homomorphism $\\phi$: it is all group elements $g$ taken to the identity permutation.\n\nNote that an application of the **<sup>FIRST</sup> ISOMORPHISM THEOREM!!!** shows that, as $\\phi$ is a homomorphism, the quotient group $G/\\text{Ker}(\\phi)$ is isomorphic to the image of $\\phi$ (or, in other words, a subgroup of $\\text{Sym}(X)$.)\n\nSome group actions are perfectly suited to the set they act on: for instance, the dihedral group $D_{2n}$ is *isomorphic* to the symmetric group of the set of vertices of its corresponding $n$-gon. The kernel of these group actions is the identity alone, as they map perfectly to the symmetric group; call these group actions\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Faithful**. A group action is **faithful** if its kernel is exactly $\\{e_G\\}$.\n\nThis is why you should be cuddling in bed with Dummit and Foote's *Abstract Algebra* rather than with a romantic partner: girlfriends and boyfriends cheat, but (a few) group actions are always faithful. (Coincidentally, group actions are also the only action mathematicians are probably ever going to get.)\n","n":0.033}}},{"i":107,"$":{"0":{"v":"Conjugation","n":1},"1":{"v":"## Examples of group actions\n\n### Left regular action\n\nSome group actions act on the group themselves: that is to say, a function $\\phi$ can be defined that takes every element $g\\in G$ to a permutation of the elements of $G$. The simplest such action is the\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Left regular action**. For a group $G$, define the **left regular action** of $G$ on itself via the function $\\phi(g)(x) = g\\cdot_G x$: the action of $g$ on $x$ is just $g$ multiplied by $x$. This action is faithful and transitive.\n\nThe fact that this *is* an action at all derives from the group axioms; its faithfulness derives from $gx = x$ implying necessarily that $g = e$, by the uniqueness of the identity; and its transitiveness derives from $\\phi(yx^{-1})(x) = y$ for any $x, y\\in G$.\n\nThrough the framework of a regular action, we are now equipped to prove\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Cayley's Theorem.** **Every single group** is isomorphic to a subgroup of some symmetric group.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nVia the left regular action, a homomorphism $\\phi: G \\to \\text{Sym}(G)$ can be defined whose kernel is $\\{e\\}$; by the **<sup>FIRST</sup> ISOMORPHISM THEOREM!!!**, we then have\n$$\nG/\\text{Ker}(\\phi) = G/\\{e\\} = G \\cong \\text{Im}(\\phi) \\leq \\text{Sym}(G).\n$$\n\n***\n\nOn the surface, Cayley's theorem seems like a surprising result; symmetric groups just describe ways of swapping the order of things around, after all - so how can groups of all kinds, from quaternions to the integers to a dihedral group acting on polygons, be encapsulated in nothing more than those permutations? Once you think about it, though, it's not really that weird! Let's say $g \\in G$ is a product of two elements $g_1 g_2$:\n$$\ng = g_1 g_2\n$$\nAll that Cayley's theorem says is that multiplication by $g_1$ can be thought of as the permutation that makes $g_2$ swap places with $g$ - something which holds true for all combinations of elements in $G$, as per the left regular action.\n\n***\n\n### Left coset action\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Let $G$ be a group and $H$ its subgroup. Define the transitive **left coset action** of $G$ on the left cosets of $H$ via the function\n$$\n\\phi(g_1)(gH) = g_1gH,\\ g_1, g \\in G\n$$\n> i.e. left multiplication by $g_1$ on the coset $gH$. \n\nThis satisfies the definitions for a group action via closure ($\\phi(g_1)(gH) = g_1gH$ is still a left coset of $H$), identity ($\\phi(e_G) (gH) = gH$), and the homomorphism property\n$$\n\\phi(g_1 g_2)(gH) = g_1g_2(gH) = g_1(g_2 (gH)) = \\phi(g_1)\\phi(g_2)(gH).\n$$\nTransitivity can be shown via\n$$\n\\phi(ab^{-1})(bH) = aH,\\ \\forall a, b \\in G\n$$\nwith the left coset action reducing to the left regular action when $H = \\{e\\}$.\n\n## The conjugation action\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Let $a, b$ both be elements of a group $G$. Define the **conjugation** of $a$ by $b$ as the group element given by $c= bab^{-1} \\in G$; conversely, if there exists some $b \\in G$ such that $c = bab^{-1}$, call $a$ and $c$ **conjugates**.\n\nWhat is the point of conjugation? If we interpret multiplication by an element in a group as \"doing something\" - e.g. multiplication by a permutation is switching the order of a set, and multiplication by a cyclic group element is a rotation - then the expression $bab^{-1}$ just means \"doing $b$ first, then $a$, then reversing $b$\". In non-abelian groups, this is markedly different from just doing $a$; we are *placing $a$ in the context of having done $b$ first*.\n\n(To get a sense for why these two things might be different, compare \"undressing in your own home\" vs. \"driving to the closest kindergarten in your neighborhood, undressing once you've gotten there, and driving back home.\" Though I suspect you probably won't be getting to the \"driving back home\" part if you do this.)\n\nBut that doesn't mean the conjugates $bab^{-1}$ and $a$ are *completely* different; we've placed the action $a$ in a different context, but it's still the action $a$. For that reason, several similarities arise; for instance, the conjugate of an element has the same order as the element itself, with\n$$\n(bab^{-1})^k = ba^k b^{-1} = e \\iff a^k = e.\n$$\n\nThrough conjugation, we define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **conjugation action** of a group $G$ on itself is defined via the function\n$$\n\\phi(g)(x) = gxg^{-1},\n$$\n> i.e. conjugating the element $x \\in G$ by the element $g \\in G$.\n\nThis is a well-defined group action by virture of closure ($gxg^{-1} \\in G$ by group closure), identity ($exe^{-1} = x$), and multiplicativity with\n$$\n\\begin{aligned}\n\\phi(g_1g_2)(x) &= (g_1 g_2)(x)(g_1 g_2)^{-1} \\\\\n&= g_1 g_2 x g_2^{-1} g_1^{-1} \\\\ \n&= g_1(g_2 x g_2^{-1})g_1^{-1} \\\\\n&= \\phi(g_1)(\\phi(g_2)(x)).\n\\end{aligned}\n$$\n\nThe orbits and stabilizers of $G$ under the conjugacy action have special names: in particular, we define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Conjugacy classes**. The **conjugacy class** of a group element $a \\in G$ is its orbit under the conjugacy action (which partition $G$, by a previous result):\n$$\n\\text{ccl}(a) = \\{g' \\in G: (\\exists g\\in G)\\ gag^{-1} = g' \\}\n$$\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Centralizers.** The centralizer of an element $a \\in G$ is its stabilizer under the conjugacy action:\n$$\nC_G(a) = \\{g \\in G:ga=ag\\}\n$$\n>i.e. the elements which commute with $a$ (as $ga = ag$ implies $gag^{-1}=a$, or invariance under conjugation). The set of elements in $G$ which commute with **all** other elements of $G$ is called the **center** of $G$:\n$$\nZ(G)\\ (\\text{or }C_G) = \\{g \\in G:\\forall a, ga=ag\\}\n$$\n\nFrom the expression $gag^{-1}$ alone - also the expression for a change of basis in linear algebra - we immediately observe some connection between conjugation and normal subgroups, themselves defined as subroups $H < G$ where $gHg^{-1} = H$. And indeed; not only do conjugation actions provide the most natural way for a group to act on normal subgroups, normal subgroups themselves are *formed* out of conjugacy classes.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. If $H \\triangleleft G$, then $G$ acts on $H$ through a conjugation action.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nWe've already proven above that the conjugation action satisfies properties of identity and multiplicativity; by definition of a normal subgroup we have $ghg^{-1} \\in H$ for any $g \\in G$, so the conjugation action defined by\n$$\n\\phi(g)(h) = ghg^{-1}\n$$\nis also an element of $H$, thus satisfying closure. $\\square$\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. If $H \\triangleleft G$, then $H$ is a union of some number of conjugacy classes of $G$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nBy definition of a normal subgroup, we have $ghg^{-1} \\in H$ for every $h \\in H$ and $g \\in G$; this exactly matches the definition of a conjugacy class $\\text{ccl}(h)$, and so $\\text{ccl}(h) \\subset H$ for every $h \\in H$. Thus, $H$ is the union of conjugacy classes of its elements.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. A group $G$ acts on the set of subgroups of $G$, denoted $X$, through a conjugation action.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nWe need to prove that the function defined by\n$$\n\\phi(g)(H) = gHg^{-1}\n$$\nfor $g, g_1 \\in G$ and $H \\in X$, where $gHg^{-1}$ is understood to be a coset, is a well-defined group action. If $H$ is a subgroup of $G$, then for any $a, b \\in H$, we have $a\\cdot_G b^{-1} \\in H$; therefore, $gHg^{-1}$ has the property that\n$$\n(gag^{-1}) \\cdot_G (gbg^{-1}) = g(ab)g^{-1} \\in H\n$$\nbut $g(ab)g^{-1}$ is an element of $gHg^{-1}$ as $ab \\in H$, so $gHg^{-1}$ is also a subgroup of $G$. The identity property and the multiplicativity property are inherited from the conjugation action itself. $\\square$\n\nIn particular, we call the stabilizer of this group action the\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Normalizer**. For a subgroup $H < G$, call the **normalizer** of $H$ the set of all elements $g \\in G$ such that the conjugate of $H$ by $g$ is still $H$: in other words, it is the stabilizer of $H$ under the conjugation action\n$$\nN_G(H) = \\{g \\in G: gHg^{-1} = H\\}\n$$\n> or the set of all elements $g \\in G$ such that $H$ can be seen as a normal subgroup. The normalizer of a normal subgroup is the entire group $G$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. The normalizer $N_G(H)$ of a subgroup $H< G$ is itself a subgroup of $G$; seen this way, it is the largest subgroup of $G$ which $H$ is a normal subgroup.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. If $a, b \\in N_G(H)$, then we have\n$$\nab^{-1}(H)(ab^{-1})^{-1} = ab^{-1}(H)ba^{-1}=a(b^{-1}Hb)a^{-1}\n$$\n> where $b^{-1}Hb$ and $aHa^{-1}$ are both $H$ by definition of $b \\in N_G(H)$ and thus $b^{-1} \\in N_G(H)$, resulting in\n$$\na(b^{-1}Hb)a^{-1} = aHa^{-1} = H\n$$\n> and thus $ab^{-1} \\in N_G(H)$, proving it is a subgroup. $\\square$\n\n***\n\n## Cauchy's Theorem\n\nAs a final farewell to group actions, we prove\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Cauchy's Theorem.** Let $G$ be a finite group and $p$ be a prime number that divides the order of $G$, $|G|$. Then $G$ has at least $p-1$ elements of order $p$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n**TL;DR**: Define a set of elements $X = \\{(g_1, g_2, ..., g_p)\\}$ based on $G$ with the property that $g_1g_2...g_p = e$; if $g_1 = g_2 = ... = g_p = g \\in G$, then $g^p = e$ and so $g$ is of order $p$ or $1$ (if $p$ is prime). Consider a group action that takes the form of a permutation on $X$ and use the orbit-stabilizer theorem to find the number of elements of orbit size $1$, i.e. are unpermutable and thus have the form $(g,g,g,g,...,g)$. These elements $g$, excepting the identity, have order $p$ in $G$.\n\nLet $G$ have order $n$, and suppose that a prime number $p$ divides $n$. Define the set $X$ as a subset of the group $G^p = G \\times G \\times ... \\times G$, the direct product of $G$ with itself $p$ times, as follows:\n$$\nX = \\{(g_1, g_2, ..., g_p) \\in G^p: g_1g_2...g_p = e_G\\}\n$$\ni.e. all elements in $G^p$ whose product is the identity of $G$. Due to group inverses, the first $p-1$ elements of $G$ within any element of $X$ can be chosen arbitrarily; the last element $g_p$ is thus defined uniquely by\n$$\ng_p = (g_1g_2...g_{p-1})^{-1}\n$$\nwhich always exists as an element of group $G$. As such, there are a total of $n^{p-1}$ unique elements of the set $X$: $p-1$ elements to choose, and $n$ elements total in group $G$ to choose from without replacement. \n\nNow define the group action of the cyclic group $C_p$, generated by the powers of a generator element $c$, on an element $x = (g_1, g_2, ..., g_p) \\in X$ as a single application of a $p$-cycle on $x$:\n$$\n\\phi(c^k) = (g_1g_2...g_p)^k(g_1, g_2, ..., g_p)\n$$\nwhere $(g_1g_2...g_p)$ denotes a $p$-cycle in the sense of a permutation, i.e.\n$$\n(g_1g_2...g_p)(g_1, g_2, ..., g_p) = (g_2, ..., g_p, g_1).\n$$\nThis satisfies the conditions for a group action because it is innately multiplicative (due to the composition of cycles), has $\\phi(c^0) = \\phi(e)$ equalling the identity permutation, and maps from $X$ to $X$ as any permutation of an element of $x$ still multiplies to the identity of $G$. \n\nBy the Orbit-Stabilizer Theorem, the size of the orbit of any $x \\in X$ under this group action multilied by the size of its stabilizer is the size of $C_p$, which is $p$:\n\n$$\n|\\text{orb}(x)||\\text{stab}(x)| = p\n$$\n\nAs $p$ is prime, we have either $\\text{orb}(x) = 1$ or $\\text{orb}(x) = p$. Suppose that in $X$, there are $r$ elements with orbits of size $1$ and $s$ elements with orbits  of size $p$; the orbits of elements of $X$ partition $X$, yielding\n$$\nr + sp = n\n$$\nBut $n$ is a multiple of $p$, so we have\n$$\nr + sp = kp\n$$\nfor some $k > s$ (as the identity element of $X$, given by $(e,e,e,e,e,...,e)$, has an orbit of size $1$ as it cannot be permuted, and so $r$ is positive and $kp$ is larger than $sp$). THus we have\n$$\nr = (k-s)p\n$$\nwith $k-s$ at least $1$; there are at least $p$ elements with an orbit of size $1$ under a group action by cyclic group $C_p$. This means that they are invariant under any permutation; in other words, these elements are of the form\n$$\n(a,a,a,a,...,a)\n$$\nand by definition of $X$ - the product of all elements of $x \\in X$ yields the identity of $G$, $e_G$ - we have\n$$\na \\cdot a \\cdot ... \\cdot a = a^p = e_G.\n$$\nThe identity of $G$ itself is one of these $p$ elements, but it has order $1$; if $a^p = e_G$, we can be sure that no positive integer $k$ has $a^k = e_G$ because $p$ is a prime and so $k$ cannot divide $p$, unless $k$ is $1$. The identity is the only element with order $1$ in $G$, and so there are (at least) $p-1$ total elements with order $p$. $\\square$","n":0.022}}},{"i":108,"$":{"0":{"v":"Conjugacy Classes of Permutations","n":0.5},"1":{"v":"\n> \"Conjugacy\" might just be the only word with enough power to traumatize a mathematician, a linguist, and a divorce lawyer all at once.\n\n## Conjugate permutations\n\nWe know that group elements which are conjugate to one another share many properties: as a previous page described, if $g$ and $k$ are conjugate to one another via $k = hgh^{-1}$, then $k$ is nothing more than \"performing $g$ in the context of $h$\". In particular, they share a common order; however, with permutations, they share something much stronger.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. If a cycle is conjugate to another cycle via some $\\sigma \\in S_n$, then the two cycles are permutations of one another via $\\sigma$:\n$$\n\\sigma \\in S_n,\\ \\sigma(a_1 a_2 ... a_k)\\sigma^{-1} = (\\sigma(a_1)\\sigma(a_2) ... \\sigma(a_k)).\n$$\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n(For the rest of this section, permutations in $S_n$ act on the set $X = \\{1, 2, ..., n\\}$.)\n\nThe equality of the above two permutations hinges on whether they act the same way on each element $x \\in X = \\{1, 2, ..., n\\}$. Consider the element $\\sigma(x)$ for every $x \\in X$; $\\sigma$ is an invertible and thus a bijective mapping from $X$ to itself. \n\nFor the left-hand side, we have\n$$\n\\sigma(a_1 a_2 ... a_k)\\sigma^{-1}(\\sigma (x)) = \\sigma(a_1 a_2 ... a_k)(x)\n$$\nby associativity. If $x= a_i$ for $i = 1, ..., k$, we thus have\n$$\n\\sigma (a_{k+1})\n$$\nor, if $i = k$, then $\\sigma(a_1)$. Alternatively, if $x$ is not one of the $a_i$s, then the output is simply $x$ itself. Thus, the left-hand side permutation acts on $X$ via\n$$\n\\begin{cases}\n\\sigma(x_i) \\to \\sigma(x_{i+1}),\\ i = 1, 2, ..., k-1\\\\\n\\sigma(x_k) \\to \\sigma(x_1),\\ i = k \\\\\n\\sigma(x) \\to \\sigma(x), x \\notin \\{a_1, ..., a_k\\}\n\\end{cases}\n$$\nwhich precisely defines the cycle $(\\sigma(a_1), \\sigma(a_2), ..., \\sigma(a_k))$. (Considering how the permutations act on $\\sigma(X)$ is equivalent to considering how they act on the set $X$, as $\\sigma$ is a bijection from $X$ to $X$.)\n\nThe primary implication of this lemma is that\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>.\nAny two cycles of the same length are conjugate to one another.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLet $\\sigma = (a_1 a_2 ... a_k)$ and $\\tau = (b_1 b_2 .. b_k)$ be two permutations in $S_n$. Then the permutation defined by the product of transpositions\n$$\n\\rho = (a_1 b_1) (a_2 b_2) ... (a_k b_k)\n$$\nwhich, with the case $a_i = b_i$ being treated as a \"do nothing\" permutation,  maps $\\sigma$ onto $\\tau$ via $\\tau = \\rho\\sigma\\rho^{-1}$ by the above lemma.\n\nThis leads us to\n\n><span style=\"background-color: #12ffd7; color: black;\">Theorem</span>.\nTwo permutations are conjugate if and only if their disjoint cycle decompositions feature permutations of the same length: for instance, two permutations in $S_4$ comprised of disjoint cycles of length $1, 1, 2$ are conjugate.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLet $\\sigma$ and $\\tau$ be two permutations in $S_n$, with $\\sigma$ having disjoint cycle decomposition\n$$\n\\sigma = (a_1^1 a_2^1 ... a_{k_1}^1) ... (a_1^m a_2^m ... a_{k_m}^m)\n$$\nwith cycles of lengths $k_1$, $k_2$, ..., $k_m$. If $\\sigma$ and $\\tau$ are conjugates via some permutation $\\rho \\in S_n$, i.e. $\\rho \\sigma \\rho^{-1} = \\tau$, then we have\n$$\n\\begin{aligned}\n\\rho \\sigma \\rho^{-1} &= \\rho (a_1^1 a_2^1 ... a_{k_1}^1) ... (a_1^m a_2^m ... a_{k_m}^m) \\rho^{-1} \\\\\n&= (\\rho (a_1^1 a_2^1 ... a_{k_1}^1) \\rho^{-1}) ... (\\rho(a_1^m a_2^m ... a_{k_m}^m)\\rho^{-1}) \\\\\n&= (\\rho(a_1^1) \\rho(a_2^1) ... \\rho(a_{k_1})^1)...(\\rho(a_1^m) \\rho(a_2^m) ... \\rho(a_{k_m})^m) = \\tau\n\\end{aligned}\n$$\nby the above lemma. The cycles of a disjoint cycle decomposition must contain all of $X$, i.e. $a_1^1, ..., a^1_{k_1}, a_2^1, ..., a^m_1, ..., a_{k_m}^m$ are exactly the elements of $X$; and as any permutation $\\rho$ is a bijective mapping from $X$ to $X$, the set given by \n$$\n\\rho(a_1^1), ..., \\rho(a_{k_1}^1),..., \\rho(a_1^m), ..., \\rho(a_{k_m}^m)\n$$\nare also exactly the elements of $X$. Therefore, $\\rho \\sigma \\rho^{-1}$ in the form expressed above is a disjoint cycle decomposition; and as disjoint cycle decompositions are unique, and given that $\\rho \\sigma \\rho^{-1} =\\tau$, the above must also be the unique disjoint cycle decomposition of $\\tau$. Thus, $\\sigma$ and $\\tau$ have disjoint cycle decompositions involving cycles of equal lengths.\n\nConversely, suppose that $\\sigma, \\tau \\in S_n$ are two permutations with disjoint cycle decompositions involving cycles of equal lengths:\n$$\n\\sigma = (a_1^1 a_2^1 ... a_{k_1}^1) ... (a_1^m a_2^m ... a_{k_m}^m)\n$$\nand\n$$\n\\tau = (b_1^1 b_2^1 ... b_{k_1}^1) ... (b_1^m b_2^m ... b_{k_m}^m).\n$$\nDefine the permutation\n$$\n\\rho(a^i_{j}) = b^i_j\n$$\nwhich satisfies the properties of a permutation because the $a^i_j$s and the $b^i_j$s are both the set $X$, and no two $a^i_j$s nor any two $b^i_j$s are equal by the Disjoint Cycle Decomposition theorem. Thus $\\rho$ is a bijection from $X$ to $X$, and is a permutation in $S_n$. By this construction we directly obtain\n$$\n\\rho \\sigma \\rho^{-1} = \\tau\n$$\nthrough the above lemma, proving the statement. $\\square$\n\n## Cycle types and conjugacy classes\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. From the above statements, we know that two permutations with disjoint cycle decompositions whose cycles are of equal lengths are conjugate; call the number of $1$-cycles, $2$-cycles, ..., $n$-cycles in the disjoint cycle decomposition of a permutation $\\sigma \\in S_n$, denoted $\\sigma_1$, $\\sigma_2$, ..., $\\sigma_n$ respectively, the **cycle type** of $\\sigma$. This is usually written $1^{\\sigma_1} 2^{\\sigma_2}...n^{\\sigma_n}$.\n\nBy construction, two permutations with the same cycle type are conjugate; in other words, they belong to the same *conjugacy class* (i.e. the same orbit under the conjugation action). Moreover, we also have the following:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. Recall that the **centralizer** of an element $g$ in a group $G$ as its stabilizer under the conjugation action:\n$$\nC_G(g) = \\{h \\in G: hgh^{-1} = g\\}\n$$\n> or, alternatively, as all the elements in $G$ that commute with $g$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Lemma</span>. \n> Suppose that a permutation $\\sigma \\in S_n$ has cycle type $1^{\\sigma_1} 2^{\\sigma_2}...n^{\\sigma_n}$. Then the order of the centralizer of $\\sigma$ is\n$$\n|C_{S_n}(\\sigma)| = 1^{\\sigma_1}(\\sigma_1!)2^{\\sigma_2}(\\sigma_2!) ... n^{\\sigma_n}(\\sigma_n!).\n$$\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nThe centralizer of $\\sigma$ is all permutations $\\tau \\in S_n$ such that $\\tau \\sigma \\tau^{-1} = \\sigma$. Suppose that $\\sigma$ has disjoint cycle decomposition\n$$\n\\sigma = (a_1^1 a_2^1 ... a_{k_1}^1) ... (a_1^m a_2^m ... a_{k_m}^m)\n$$\nleading to\n$$\n\\tau \\sigma \\tau^{-1} = (\\tau(a_1^1) \\tau(a_2^1) ... \\tau(a_{k_1}^1)) ... (\\tau(a_1^m) \\tau(a_2^m) ... \\tau(a_{k_m}^m)).\n$$\nCycles remain the same if a permutation acts on them in two ways: 1) if the permutation cycles them, i.e.\n$$\n(\\tau(a_1)\\tau(a_2) ... \\tau(a_k))\n$$\nis a cycle of $(a_1 a_2 ... a_k)$, or 2) if the permutation swaps them with another cycle of the same length, e.g. a $3$-cycle with another $3$-cycle. For a $k$-cycle ($k = 1, 2, ..., n$), there are $\\sigma_k!$ ways to permute the $\\sigma_k$ cycles in the disjoint cycle decomposition of $\\sigma$; for each of these ways, there are $k^{\\sigma_k}$ ways to cycle all $\\sigma_k$ of these cycles. Thus, we obtain\n$$\n|C_{S_n}(\\sigma)| = 1^{\\sigma_1}(\\sigma_1!)2^{\\sigma_2}(\\sigma_2!) ... n^{\\sigma_n}(\\sigma_n!)\n$$\nin total.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. Let $\\sigma \\in S_n$ have cycle type $1^{\\sigma_1} 2^{\\sigma_2}...n^{\\sigma_n}$. Then the size of the conjugacy class of $\\sigma$, denoted $|\\text{ccl}(\\sigma)|$, is \n$$\n|\\text{ccl}(\\sigma)| = \\frac{n!}{1^{\\sigma_1}(\\sigma_1!)2^{\\sigma_2}(\\sigma_2!) ... n^{\\sigma_n}(\\sigma_n!)}.\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. $S_n$ has order $n!$; an application of the Orbit-Stabilizer theorem on the orbit of $\\sigma$, which is $\\text{ccl}(\\sigma)$, and the stabilizer of $\\sigma$, which is $C_{S_n}(\\sigma)$, yields the above expression.\n\nWe know that normal subgroups only arise from the unions of conjugacy classes; as such, knowing exactly *what* the conjugacy classes are (permutations of the same cycle type) and *how big* each conjugacy class is grants us valuable insight in searching for normal subgroups.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Find the normal subgroups of $S_4$.\n\nThe possible cycle types of a permutation $\\sigma \\in S_4$ are $1^42^0 3^0 4^0$ (the identity permutation); $1^2 2^1 3^0 4^0$ (a single transposition); $1^1 2^0 3^1 4^0$ (a $3$-cycle); $1^0 2^2 3^0 4^0$ (two transpositions); and $1^0 2^0 3^0 4^1$ (a $4$-cycle). \n\nThe identity permutation stands alone as a conjugate class; using the formula above, the sizes of the other four conjugacy classes are $6$, $8$, $3$, and $6$ respectively. By Lagrange's Theorem, subgroups are of order 1, 2, 3, 4, 6, 8, 12 and 24; any subgroup will contain the identity conjugacy class.\n\nThe union of the identity conjugacy class and the class of two transpositions is size $4$, which is $C_2\\times C_2$ and a normal subgroup.\n\nIf a normal subgroup contains the conjugacy class of single transpositions, then it must be $S_4$ itself as any symmetric group is generated out of single transpositions.\n\nIf a normal subgroup contains the conjugacy class of $3$-cycles - which, when combined with the identity conjugacy class, yields a set of size $9$, and must then be either a subgroup of size $12$ or $24$ by Lagrange's Theorem - then we would have (for size $12$)\n$$\n1 + 8 + 3 = 12\n$$\nthrough the union of the identity conjugacy class, the class of $3$-cycles, and the class of two transpositions of size $3$; this is $A_4$. For size $24$, we simply have $S_4$.\n\nFinally, if a normal subgroup contains the conjugacy class of $4$-cycles, then by\n$$\n(1234)(1324) = (142)\n$$\nwe know that it must also contain at least one $3$-cycle, and by the requirement that normal subgroups are unions of entire conjugacy classes, it must contain every single $3$-cycle. This yields $S_4$, as it would lead to a subgroup of size $1 + 6 + 8 = 15$ which can only be $S_4$ by Lagrange's Theorem.\n\nTherefore, we yield the normal subgroups\n$$\n(\\{e\\}, C_2\\times C_2, A_4, S_4)\\triangleleft S_4. \n$$\n\n## Conjugacy classes in alternating groups\n\nIn $S_n$, the conjugacy classes are formed simply by all permutations of a certain cycle type; in $A_n$, things are muddied by the fact that odd permutations are no longer accessible to us for conjugation. For instance, any two $3$-cycles are conjugate in $S_3$ by virtue of having the same cycle type, e.g. $(123)$ and $(213)$; however, they are not conjugate in $A_3 \\cong C_3$ because $A_3$ is an abelian group, and elements in an abelian group are conjugate if and only if they are equal via\n$$\ngag^{-1} = gg^{-1}a = a.\n$$\n\nCloser inspection gives us the following result:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Suppose that $\\sigma \\in A_n$; denote the conjugacy classes of $\\sigma$ in $S_n$ and $A_n$ by $\\text{ccl}_{S_n}(\\sigma)$ and $\\text{ccl}_{A_n}(\\sigma)$ respectively. Then $\\text{ccl}_{A_n}(\\sigma)$ is either half the size or the same size as $\\text{ccl}_{S_n}(\\sigma)$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>\n\n\n We have $|S_n| = 2|A_n|$; by the Orbit-Stabilizer theorem, we have\n$$\n\\begin{cases}\n|S_n| = |\\text{ccl}_{S_n}(\\sigma)||C_{S_n}(\\sigma)| \\\\\n|A_n| = |\\text{ccl}_{A_n}(\\sigma)||C_{A_n}(\\sigma)|\n\\end{cases}\n$$\nand thus\n$$\n|\\text{ccl}_{S_n}(\\sigma)||C_{S_n}(\\sigma)| = 2|\\text{ccl}_{A_n}(\\sigma)||C_{A_n}(\\sigma)|.\n$$\nWe know that $|C_{A_n}(\\sigma)|$ is also a factor of $|C_{S_n}(\\sigma)|$ ($A_n$ is a subgroup of $S_n$, and so $C_{A_n}$ is a subgroup of $C_{S_n}$); in fact, we know very precisely that $C_{A_n}$ is either half the size of $C_{S_n}$ or exactly the same size as $C_{S_n}$: every subgroup of $S_n$ is either half even or completely even, i.e. either half in $A_n$ or completely in $A_n$, with the former resulting in \n$$\n|C_{A_n}| = \\frac{1}{2}|C_{S_n}|\n$$\nand the latter in\n$$\n|C_{A_n}| = |C_{S_n}|.\n$$\nSubstituting this into the equation above results in\n$$\n|\\text{ccl}_{A_n}(\\sigma)| = 2|\\text{ccl}_{S_n}(\\sigma)|\n$$\nor \n$$\n|\\text{ccl}_{A_n}(\\sigma)| = |\\text{ccl}_{S_n}(\\sigma)|\n$$\nas desired. $\\square$\n\n> <span style=\"background-color: #12ffd7; color: black;\">Corollary</span>. Call a conjugacy class $\\text{ccl}_{S_n}(\\sigma)$ that is halved in size in $A_n$ a **split** conjugacy class. If the stabilizer of $\\sigma$ in $S_n$ is completely even, the conjugacy class splits; otherwise, it does not split.\n\nThis is derived directly from the proof above.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Describe the conjugacy classes of $A_4$ and derive its normal subgroups.\n\nTo start with, let's describe the conjugacy classes of $S_4$. From the above example, we have\n\n| Conjugacy class | Order of class |\n| --------------- | -------------- |\n| $1$-cycle, $\\{e\\} | 1 |\n| Single transpositions | 6 |\n| Double transpositions | 3 |\n| $3$-cycles | 8 |\n| $4$-cycles | 6 |\n<br/>\n\nIn which:\n- The identity conjugacy class is not split (as all its elements are also in $A_4$)\n- The conjugacy class of single transpositions are odd, and are not in $A_4$.\n- The conjugacy class of double transpositions lie in $A_4$. Their centralizers contain an odd element (e.g. $(12)(34)$ is stabilized by $(12)$), and as such we know they are half odd; thus, this conjugacy class is not split in $A_4$.\n- The conjugacy class of $3$-cycles lie in $A_4$. The Orbit-Stabilizer theorem shows that the size of their centralizer is $24 / 8 = 3$ (there are $8$ $3$-cycles); as such, every element in this subgroup has order $3$ and is therefore a $3$-cycle (which is even). Thus the centralizer of this conjugacy class lies entirely in $A_4$, and so this class is split into two classes of size $4$. \n- The conjugacy class of $4$-cycles are odd and do not lie in $A_4$.\n\nThus, the conjugacy classes of $A_4$ are:\n- The identity conjugacy class (not split, order $1$)\n- The conjugacy class of double transpositions (not split, order $3$)\n- The two conjugacy classes formed from $3$-cycles (split, each of order $4$)\n\nBy Lagrange's Theorem, the normal subgroups of $A_4$ can be of order $1$, $2$, $3$, $4$, $6$ or $12$; our conjugacy classes have orders $1, 3$ and $4$. Thus we either have\n$$\n1 = 1\n$$\nwhich is the identity normal subgroup $\\{e\\}$; \n$$\n1 + 3 = 4\n$$\ni.e. the union of the identity subgroup and the double transposition conjugacy class. This is $C_2 \\times C_2$, and forms a normal subgroup. Finally, any normal subgroup including a single split conjugacy class of $3$-cycles (of order $4$) must also include the identity; and so it has order\n$$\n1+ 4= 5\n$$\nBut all other conjugacy classes in $A_4$ are either of order $4$ or order $3$, which cannot add to $5$ to result in $6$; thus, the normal subgroup corresponding to this case must have order $12$ by Lagrange's Theorem, and will be $A_4$. Therefore, the normal subgroups of $A_4$ are $\\{e\\}, C_2 \\times C_2, A_4 \\triangleleft A_4$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. $A_5$ is simple.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\n$S_5$ has the following conjugacy classes:\n\n| Conjugacy class | Size |\n|-|-|\n|$\\{e\\}$|$1$|\n| Single transpositions (e.g. $(12)$)| $5 \\choose 2$ $= 10$| \n| $3$-cycles (e.g. $(123)$) | $15$ \n| $4$-cycles (e.g. $(1234)$) | $30$\n| $5$-cycles (e.g. $(12345)$) | $24$ \n| Double transpositions (e.g. $(12)(34)$) | $5 \\choose 2$ $3 \\choose 2$ $=15$\n<br/>\n\nOf these conjugacy classes, $\\{e\\}$, $3$-cycles, $5$-cycles and double transpositions are in $A_5$; double transpositions are not split, $5$-cycles are split (as they are centralized by a group of order $5$, which must be the cyclic group of $5$-cycles, which are even), and $3$-cycles are not split because $(12)$, which is odd, centralizes $(345)$. This leads to the following conjugacy classes for $A_5$:\n\n\n| Conjugacy class | Size |\n|-|-|\n|$\\{e\\}$|$1$|\n| $3$-cycles (e.g. $(123)$) | $15$ \n| $5$-cycles, class $1$ | $12$ \n| $5$-cycles, class $2$ | $12$ \n| Double transpositions (e.g. $(12)(34)$) | $5 \\choose 2$ $3 \\choose 2$ $=15$\n<br/>\n\nIf $G$ is a normal subgroup of $A_5$, then by Lagrange's Theorem $|G|$ must divide $5!/2=60$; moreover we see from the size of the conjugacy classes that, excepting the identity subgroup $\\{e\\}$, the minimal size of a normal subgroup (formed from the union of conjugacy classes) is $12 + 1 = 13$. The possible values for the order of $G$ are thus all the divisors of $60$ greater than $13$: $15, 30$ and $60$. None of these can be formed from the sums of the orders of the above conjugacy classes excepting $60$, which is the union of all of them at once; thus, the only normal subgroups of $A_5$ is $\\{e\\}$ and itself, and so $A_5$ is simple. $\\square$","n":0.02}}},{"i":109,"$":{"0":{"v":"Differential Equations","n":0.707},"1":{"v":"What the frick did you just fricking say about me, you little initial condition? I’ll have you know I graduated top of my class in the MIT, and I’ve been initiated in numerous secret techniques on Al-Gebra, and I have over 300 confirmed integrations. I am trained in gorilla mathematics and I’m the top math wiz in the entire set of US computational forces. You are nothing to me but just another differential equation. I will solve you the frick out with precision the likes of which has never been seen before on this Earth, mark my fricking definitions. You think you can get away with simplifying that fraction that way over the Internet? Think again, factorial. As we speak I am contacting my secret network of underground mathematicians across the world and your domain is being traced right now so you better prepare for the tangent, maggot. The tangent that wipes out that little unknown variable you call your life.You’re fricking summed, sequence. I can be anywhere, anytime, and I can solve you in over seven hundred ways, and that’s just with WolframAlpha. Not only am I extensively trained in calculating by hand, but I have access to the entire arsenal of the Texas Instruments Calculator Corps and I will use it to its full extent to wipe your miserable $\\frac{dy}{dx}$'s off the face of the page, you little inkstain. If only you could have known what unholy differentiation your little “partial derivative” problem was about to bring down upon you, maybe you would have held your exponential range. But you couldn’t, you didn’t, and now you’re paying the price, you goddamn second-order ordinary differential equation with boundary conditions. I will prove fury all over you and you will drown in it. You’re solved dead, limit.","n":0.058}}},{"i":110,"$":{"0":{"v":"Systems of Differential Equations","n":0.5},"1":{"v":"bottom text","n":0.707}}},{"i":111,"$":{"0":{"v":"Phase-Portrait Analysis","n":0.707},"1":{"v":"Recall that our solution to the linear system of differential equations \n$$\n\\mathbf{\\dot{x}} = \\mathbf{Ax}\n$$\nwas the sum of eigenvector products\n$$\n\\mathbf{x}=c_1e^{\\lambda_1 x }v_1 + c_2e^{\\lambda_2 x}v_2 + \\dots + c_ne^{\\lambda_n x}v_n.\n$$\nLet us write this in the form \n$$\n\\mathbf{x} =y_1v_1 + y_2 v_2 + \\dots y_n v_n.\n$$\nEigenvectors are extremely valuable to us because they are essentially the \"axes\" upon which our solution $\\mathbf{x}$ acts on - if we vary only a single variable $y_k$, then we know that the solution has only changed along the eigenvector $v_k$. This allows us to build up a \"phase-portrait\" of our solution $\\mathbf{x}$: a graph of how the vector $\\mathbf{x}$ changes as $y_1, y_2, \\dots, y_n$ vary, taking these as our axes. \n\nFor our sanity's sake, let's consider only the two-dimensional case (with $y_1$ and $y_2$ as axes) and analyze the differing examples that arise out of the different forms of $y_1$ and $y_2$.\n\n## Sinks and sources\n\nConsider the case where $y_1 = c_1e^{\\lambda_1 x}, y_2 =c_2e^{\\lambda_2 x}$ have real eigenvalues $\\lambda_1$, $\\lambda_2$ with the same sign: $\\lambda_1\\lambda_2>0$.\n\nIf both eigenvalues are negative, then both $y_1$ and $y_2$ are exponentially decaying functions; as the value of $x$ grows, $y_1$ and $y_2$ become smaller and smaller multiples of $v_1$ and $v_2$ and eventually move \\it towards \\normalfont the origin. Points like these are known as \"stable sinks\", or simply sinks:\n\n![](assets/images/DE-ch4-stablesink.jpg)\n\nIf both eigenvalues are instead positive, both $y_1$ and $y_2$ grow exponentially and thus travel *away* from the origin when $x$ gets large, causing the arrows to point outward instead. We call this an *unstable source* point:\n\n![](assets/images/DE-ch4-unstablesource.jpg)\n\nOther vectors close to the eigenvectors can be drawn asymptotically to the eigenvectors, as shown:\n\n![](assets/images/DE-ch4-stablesink-2.jpg)\n\nIf $\\lambda_1, \\lambda_2$ are both real but have opposite signs, we obtain a *saddle point* as follows:\n\n![](assets/images/DE-ch4-saddlenode.jpg)\n\nThis originates from one of $y_1, y_2$ growing exponentially and the other decaying exponentially. \n\n\n## Spiral points\n\nIf $\\lambda_1$, $\\lambda_2$ are complex conjugates (they must be, given that they are the solutions to a quadratic equation), we have three different cases. Let $\\lambda_{1,2} = a \\pm bi$, resulting in $y_{1,2} = e^{ax}(\\cos{bx}\\pm\\sin{bx})$. Then we have a phase portrait that looks like a spiral: \n\n![](assets/images/DE-ch4-spiral.jpg)\n\nWhy a spiral? Because if we consider the $\\cos bx + \\sin bx$ term on its own, that multiplied by a vector gives us an ellipse (as we know ellipses have parametric equations $x=a\\cos t, y=b\\sin t$). But when the $e^{ax}$ term comes in, this \"ellipse\" either expands exponentially, creating an outward spiral, or collapses inward exponentially, creating an inward spiral. Alternatively, if the real part is zero, the phase portrait is simply two ellipses centered at the origin.\n\n\n\n\n","n":0.048}}},{"i":112,"$":{"0":{"v":"Nonlinear Dynamical Systems","n":0.577},"1":{"v":"\nThis section provides an analogue to the technique of stability analysis for a single autonomous ODE, and extends it to a system of ODEs. Consider the second-order system of DEs \n\n$$\n\\begin{cases}\n        \\dot{x}=f(x,y)\\\\\n        \\dot{y}=g(x,y)\n    \\end{cases}\n$$\n\nwhere $x$ and $y$ are functions in $t$. This is an \\it autonomous \\normalfont system, in the sense that the independent variable $t$ does not appear anywhere in the system. Solving the equations can be nigh-impossible, but through analyzing their fixed points and their stabilities, we can learn many things about them. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Equilibrium points.)** As with a single equation, we define an equilibrium point or fixed point of a system of ODEs (such as the one shown above) as a point $(x_0,y_0)$ for which $\\dot{x}=\\dot{y}=0$.\n\nWe observe that this occurs when $f(x_0,y_0)=g(x_0,y_0) = 0$.\n\nTo determine the stability of the equilibrium point $(x_0,y_0)$, we consider - as before - a small perturbation $(x_0+\\alpha, y_0+\\beta)$ from $(x_0,y_0)$. As such we have\n\n$$\n\\begin{aligned}\n        \\frac{d}{dt}(x_0+\\alpha)&=\\frac{d\\alpha}{dt}\\text{ (left-hand side)} \\\\\n        &=f(x_0+\\alpha, y_0+\\beta)\\text{ (right-hand side)} \\\\ \\\\\n        \\frac{d}{dt}(y_0+\\beta)&=\\frac{d\\beta}{dt}\\text{ (left-hand side)} \\\\\n        &=g(x_0+\\alpha, y_0+\\beta)\\text{ (right-hand side)}\n    \\end{aligned}\n$$\n\nand thus by the multivariate Taylor expansion, we have \n\n$$\n\\begin{cases}\n        \\frac{d\\alpha}{dt} =f(x_0+\\alpha, y_0+\\beta) \\approx f(x_0,y_0)+\\alpha \\frac{\\partial f}{\\partial x}(x_0,y_0) + \\beta\\frac{\\partial f}{\\partial y}(x_0,y_0)\\\\\n        \\frac{d\\beta}{dt} =g(x_0+\\alpha, y_0+\\beta) \\approx g(x_0,y_0)+\\alpha \\frac{\\partial g}{\\partial x}(x_0,y_0) + \\beta\\frac{\\partial g}{\\partial y}(x_0,y_0)\n    \\end{cases}\n$$\n\nignoring all second-order and above terms. As $f(x_0,y_0)=0$ by definition of an equilibrium point, we have \n\n$$\n\\begin{bmatrix}\n        \\dot{a} \\\\\\dot{b}\n    \\end{bmatrix}=\\begin{bmatrix}\n        f_{x} & f_{y} \\\\\n        g_{x} & g_{y}\n    \\end{bmatrix}\\begin{bmatrix}\n        \\alpha \\\\ \\beta\n    \\end{bmatrix}\n$$\n\nwhich is a system of differential equations that we can analyze using the methods demonstrated above. \n","n":0.062}}},{"i":113,"$":{"0":{"v":"Linear Systems of Differential Equations","n":0.447},"1":{"v":"## Transforming higher-order DEs into systems\n\nConsider the generalized higher-order (constant-coefficient) differential equation:\n\n$$\ny^{(n)}(x)+a_{n-1}y^{(n-1)} + \\dots + a_2 y''(x) + a_1y'(x)+a_0 y = g(x)\n$$\n\nIs there any way for us to simplify this into something easier to calculate? It turns out we need only to use a system of equations, with a set of $n$ new variables:\n\n$$\n\\begin{cases}\n        y_1 = y \\\\\n        y_2 = y_1'\\ (=y') \\\\\n        y_3 = y_2'\\ (=y'')\\\\\n        \\dots \\\\\n        y_n = y_{n-1}' (=y^{(n-1)}) \\\\        \n    \\end{cases}\n$$\n\nand thus we have, from the generalized equation above, \n\n$$\n\\begin{aligned}\n        y_n' &= -a_{n-1}y^{(n-1)} - \\dots - a_2 y''(x) - a_1y'(x)-a_0y \\\\\n        &=-a_{n-1}y_n - \\dots - a_2y_3 - a_1y_2 - a_0y_1\n    \\end{aligned}\n$$\n\nwith each equation being a first-order differential equation (no derivatives above the first derivative of each $y_k$). In essence, we have transformed an $n$th-order DE into a system of $n$ first-order DEs. \n\nCalling this a simplification of the original equation might seem the same as saying that topologically speaking, a coffee mug is equivalent to a donut, but observe the spark of genius lurking beneath the mundane waters of these blessed equations. As it turns out, it is extremely easy to rewrite these equations in matrix form: \n\n$$\n\\frac{d}{dx}\\begin{bmatrix}\n        y_1\\\\\n        y_2\\\\\n        y_3\\\\\n        \\vdots\\\\\n        y_n\n    \\end{bmatrix}=\n    \\begin{bmatrix}\n        0 & 1 & 0 & \\dots & 0 \\\\\n        0 & 0 & 1 & \\dots & 0 \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n        0 & 0 & 0 & \\dots & 1 \\\\\n        -a_0 & -a_1 & -a_2 & \\dots & -a_{n-1}\n    \\end{bmatrix}\n    \\begin{bmatrix}\n        y_1\\\\\n        y_2\\\\\n        y_3\\\\\n        \\vdots\\\\\n        y_n\n    \\end{bmatrix} + \n    \\begin{bmatrix}\n        0\\\\\n        0\\\\\n        0\\\\\n        \\vdots\\\\\n        g(x)\n    \\end{bmatrix}\n$$\n\nOr, alternatively, writing the matrix on the left-hand side as $\\mathbf{x}$, the first matrix on the right-hand side as $\\mathbf{A}$, and the second matrix as $\\mathbf{F}$, we have (simply!):\n\n$$\n\\dot{\\mathbf{x}} = \\mathbf{A}\\mathbf{x} + \\mathbf{F}\n$$\n\nThis is what would happen if linear algebra and differential equations went to a party together (only hypothetically speaking, since mathematicians aren't invited to parties), got aggressively drunk, found themselves tangled within each other's luscious locks, and birthed a love-child through mitosis; it's where the two converge, and it's where we can study both together. The rest of this section will be entirely focused on the study of systems of this form.\n\n## Eigenvalue solutions to linear systems of DEs\n\nHow do we begin to solve systems of the form \n\n$$\n\\dot{\\mathbf{x}} = \\mathbf{A}\\mathbf{x} + \\mathbf{F}?\n$$\nTo gain a foothold on these, let's consider the most simple case of all: a case where we simply have \n$$\n\\mathbf{F} = 0\n$$\nyielding a homogeneous equation, and \n$$\n\\mathbf{A}=\\begin{bmatrix}\n        a_1 & 0 & \\dots & 0 & 0 \\\\\n        0 & a_2 & \\dots & 0 & 0 \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n        0 & 0 & \\dots & a_{n-1} & 0 \\\\\n        0 & 0 & \\dots & 0 & a_n\n    \\end{bmatrix}\n$$\nresulting in the system of equations \n$$\n \\begin{cases}\n        y_1' = a_1 y_1 \\\\\n        y_2' = a_2 y_2 \\\\\n        \\dots \\\\\n        y_n' = a_n y_n\n    \\end{cases}\n$$\nall of which are solvable as $y_k = Ae^{a_k x}$. If we can focus our efforts in transforming the general case $\\dot{\\mathbf{x}} = \\mathbf{A}\\mathbf{x} + \\mathbf{F}$ into this simplified (and directly solvable) case, we'll have figured out a method to solve many systems of differential equations. \n\nNote that in the simplified case, the matrix $\\mathbf{A}$ is diagonal; our end goal should therefore be to convert every matrix $\\mathbf{A}$ into a diagonal matrix, too - one that gives us precisely the scale factor of the matrix along every axis, and nothing else. \n\nAs it turns out, we know precisely how to do that. Suppose that $\\mathbf{A}$ has $n$ eigenvalues $\\lambda_{1,2,\\dots,n}$ satisfying $\\det |A-\\lambda I|=0$. Define the eigenvalue matrix $\\Lambda$ as the diagonal matrix\n\n$$\n\\Lambda=\\begin{bmatrix}\n        \\lambda_1 & & & \\\\\n        &\\lambda_2 & & \\\\\n        & & \\ddots & \\\\\n        & & & \\lambda_n\n    \\end{bmatrix}\n$$\n\nand define $\\mathbf{V}$ as the matrix of eigenvectors $v_1, v_2, \\dots, v_n$, each $n \\times 1$, corresponding to $\\lambda_{1,2,\\dots,n}$:\n\n$$\n\\mathbf{V} = \\begin{bmatrix}\n        v_1 & v_2 & \\dots & v_n\n    \\end{bmatrix}\n$$\n\nWe know that, by definition, $Av_k = \\lambda_k v_k$. Thus, we have \n\n$$\n\\begin{aligned}\n        \\mathbf{V \\Lambda} &= \\begin{bmatrix}\n            \\lambda_1 v_1 & \\lambda_2 v_2 & \\dots & \\lambda_n v_n\n        \\end{bmatrix}\\\\\n        &=\\begin{bmatrix}\n            v_1(1)\\lambda_1 & v_2(1)\\lambda_2 & \\dots & v_n(1)\\lambda_n \\\\\n            v_1(2)\\lambda_1 & v_2(2) \\lambda_2 & \\dots & v_n(2) \\lambda_n \\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            v_1(n) \\lambda_1 & v_2(n) \\lambda_2 & \\dots & v_n(n)\\lambda_n\n        \\end{bmatrix} \\\\\n        &= \\begin{bmatrix}\n            \\lambda_1v_1 & \\lambda_2v_2 & \\dots & \\lambda_n v_n\n        \\end{bmatrix}\\\\\n        &=\\begin{bmatrix}\n            Av_1 & Av_2 & \\dots & Av_n\n        \\end{bmatrix}\\\\\n        &=\\mathbf{AV}\n    \\end{aligned}\n$$\n\nwhere $v_i(j)$ denotes the $j$th-row element of vector $v_i$. As such, we have $\\mathbf{AV=V\\Lambda}$ and finally \n\n$$\n\\mathbf{A= V\\Lambda V^{-1}},\\ \\mathbf{\\Lambda = V^{-1}AV}\n$$\n\nfor invertible $\\mathbf{V}$; this is the process of *diagonalization*, and it allows us to convert any matrix into a diagonal matrix of eigenvectors $\\mathbf{\\Lambda}$ by multiplying it with two other matrices.\n\nHow does this help our cause in solving systems of differential equations? We return to \n\n$$\n\\dot{\\mathbf{x}} = \\mathbf{A}\\mathbf{x} + \\mathbf{F}\n$$\n\nand consider only the homogeneous case for a moment (like we do for normal differential equations):\n\n$$\n\\dot{\\mathbf{x}} = \\mathbf{A}\\mathbf{x}\n$$\n\nLet us impose the change of coordinates $x = \\mathbf{Tz}$ for some $n \\times n$ matrix $\\mathbf{T}$, and $n\\times 1$ matrix $\\mathbf{z}$. Thus we have \n\n$$\n\\mathbf{T\\dot{z}} = \\mathbf{A}\\mathbf{Tz}\n$$\nand \n$$\n\\mathbf{\\dot{z}} = \\mathbf{T^{-1}}\\mathbf{A}\\mathbf{Tz}\n$$\nwhich is just our equation for a diagonal matrix of eigenvectors above! Thus we take $\\mathbf{T} = \\mathbf{V}$, the matrix of eigenvectors for $\\mathbf{A}$, and obtain simply \n$$\n \\mathbf{\\dot{z}} = \\mathbf{\\Lambda} \\mathbf{z} = \\begin{bmatrix}\n        \\lambda_1 & & & \\\\\n        & \\lambda_2 & & \\\\\n        & & \\ddots & \\\\\n        & & & \\lambda_n\n    \\end{bmatrix}\\mathbf{z}\n$$\nwhich gives solutions $z_1 = c_1e^{\\lambda_1 x},\\ z_2 = c_2e^{\\lambda_2 x}, \\dots, z_n = c_ne^{\\lambda_n x}$ with $\\mathbf{x=Vz}$, $\\mathbf{V}$ as the eigenvector matrix of $\\mathbf{A}$, and $\\lambda_{1,2,\\dots,n}$ as the eigenvalues of $\\mathbf{A}$. In other words, we have\n$$\n\\begin{aligned}\n        \\mathbf{x} &= \\mathbf{Vz}\\\\\n        &= \\begin{bmatrix}\n            v_1 & v_2 & \\dots & v_n\n        \\end{bmatrix}\\begin{bmatrix}\n            c_1e^{\\lambda_1 x} \\\\\n            c_2 e^{\\lambda_2 x} \\\\\n            \\vdots \\\\\n            c_ne^{\\lambda_n x}\n        \\end{bmatrix}\\\\ \\\\\n        &=\\begin{bmatrix}\n            v_1(1) & v_2(1) & \\dots & v_n(1) \\\\\n            v_1(2) & v_2(2) & \\dots & v_n(2) \\\\\n            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n            v_1(n) & v_2(n) & \\dots & v_n(n)\n        \\end{bmatrix}\n        \\begin{bmatrix}\n        c_1e^{\\lambda_1 x} \\\\\n        c_2 e^{\\lambda_2 x} \\\\\n        \\vdots \\\\\n        c_ne^{\\lambda_n x}\n        \\end{bmatrix}\\\\ \\\\\n        &=\\begin{bmatrix}\n            v_1(1)c_1e^{\\lambda_1 x} + v_2(1)c_2 e^{\\lambda_2 x} + \\dots + v_n(1)c_ne^{\\lambda_1 x} \\\\\n            v_1(2)c_1e^{\\lambda_1 x} + v_2(2)c_2 e^{\\lambda_2 x} + \\dots + v_n(2)c_ne^{\\lambda_1 x} \\\\\n            \\dots \\\\\n            v_1(n)c_1e^{\\lambda_1 x} + v_2(n)c_2 e^{\\lambda_2 x} + \\dots + v_n(n)c_ne^{\\lambda_1 x} \\\\\n        \\end{bmatrix}\\\\ \\\\\n        &= c_1e^{\\lambda_1 x }v_1 + c_2e^{\\lambda_2 x}v_2 + \\dots + c_ne^{\\lambda_n x}v_n\n    \\end{aligned}\n$$\n\nWhich is the sum of all $e^{\\lambda_k x}$ multiplied by their respective eigenvectors. This is the crucial link between eigenvalues and systems of differential equations; however, it is important to note that not all matrices are as nicely diagonalizable as the generalized example shown above. Matrices may have less than $n$ unique eigenvalues, or have complex eigenvalues, or so on.\n\nThere also exists another avenue towards the conclusion that eigenvalues are linked to systems of differential equations. Suppose we know, by divine inspiration, that functions of the form $y_k=Ae^{\\lambda_k x}$ for $k=1,2,\\dots,n$ were needed to solve the system $\\dot{\\mathbf{x}} = \\mathbf{A}\\mathbf{x}$ (a reasonable assumption to make, since every equation in the system is a linear sum of $y_1, y_2, \\dots, y_n$ and their derivatives, and so we want $y_k$ and $y_k'$ to be in the same form). Let us assume, then, that \n\n$$\n\\mathbf{x} = \\mathbf{v}e^{\\lambda x}\n$$\n\nfor some vector $\\mathbf{v}$, and thus $\\mathbf{\\dot{x}} = \\lambda \\mathbf{x}$. Substituting into $\\mathbf{\\dot{x}} = \\mathbf{Ax}$, we have \n\n$$\n\\lambda \\mathbf{x} = \\mathbf{Ax}\n$$\n\nwhich is the defining equation for eigenvalues of the matrix $\\mathbf{A}$, and corresponding eigenvectors $\\mathbf{v}$. Thus, the solution to the system is the sum of all\n\n$$\n\\mathbf{x} = \\mathbf{v}e^{\\lambda x}\n$$\n\nwith $\\lambda$ an eigenvalue of $\\mathbf{A}$ and $v$ a corresponding eigenvector - the same solution as the one we found above through diagonalization.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The solution to the $n \\times n$ homogeneous linear system of constant-coefficient differential equations\n$$\n\\dot{\\mathbf{x}} = A\\mathbf{x}\n$$\n> is given by $\\mathbf{x} = \\sum_{i=1}^n v_i e^{\\lambda_i x}$, where $v_i$ and $\\lambda_i$ denote the $i$th eigenvector and eigenvalue of $A$ respectively.\n\n","n":0.027}}},{"i":114,"$":{"0":{"v":"Series Solutions to Differential Equations","n":0.447},"1":{"v":"\nUp until this point, we've made many advances on the topic of linear differential equations. We now have a systematic method to determine the solutions of a constant-coefficient DE completely. Using various methods like the Wronskian, Abel's theorem, and variation of parameters, we can even determine particular solutions to any second-order linear DE. But what if the homogeneous equation we are considering does not have constant coefficients - for instance, equations like: \n\n$$\n4x^3y'' - (x^2+1)y' + xy = 0\n$$\n\nFor differential equations which do not have particularly \"nice\" (elementary) solutions, power series solutions are the best we can hope for - that is, solutions in the form\n\n$$\ny=\\sum_{n=0}^{\\infty} c_n x^n\n$$\n\n(or, equivalently, $c_n (x-x_0)^n$), for finite coefficients $c_0, c_1, \\dots$, under the assumption that such a power series will converge. \n\nBefore we begin our study of power series solutions to differential equations, we need to lay out a few basic principles regarding power series: namely, the notions of convergence/absolute convergence/divergence, the concept of the radius of convergence, and the application of the ratio test.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Convergence, absolute convergence, and divergence.)** The power series\n$$\ny=\\sum_{n=0}^{\\infty} c_n (x-x_0)^n\n$$\n> is said to ***converge* **at a point $x$ if the limit \n$$\n\\lim_{k\\to\\infty}\\sum_{n=0}^{k} c_n (x-x_0)^n\n$$\n> exists; conversely, if the limit diverges to infinity, the power series is said to ***diverge*.**<br/><br/>\n> The power series is said to **absolutely converge** at a point $x$ if the sum of the absolute value of all its terms \n$$\n\\lim_{k\\to\\infty}\\sum_{n=0}^{k} |c_n (x-x_0)^n|\n$$\n> also converges. If a series is absolutely convergent at $x$, then it is also regularly convergent at $x$, though the converse is not necessarily true. <br/><br/>\nThere is a non-negative number $\\rho$ such that, for all $|x-x_0|<\\rho$, the series converges absolutely, and for all $|x-x_0|>\\rho$, it diverges. Call $\\rho$ the ***radius of convergence.***\n\nTo determine the convergence of a power series, we can apply\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span> **(Ratio test)**. For a fixed value of $x$ and for $a_n\\neq 0$, consider the limit of the ratio between two consecutive terms: \n\n$$\n\\lim_{n\\to\\infty} |\\frac{c_{n+1}(x-x_0)^{n+1}}{c_{n}(x-x_0)^n}|=\\lim_{n\\to\\infty} |x-x_0||\\frac{c_{n+1}}{c_n}|=\\lim_{n\\to\\infty}|x-x_0|L\n$$\n\n> If this ratio is less than 1, the terms of the power series are shrinking and the series converges absolutely; if it is greater than 1, the terms are increasing and thus the series diverges. If the ratio is 1, then no conclusive statement can be made.\n\nFor a convergent power series, taking its derivative is equivalent to adding up the derivatives of all of its terms; that is,\n$$\n    y'=\\sum_{n=0}^{\\infty} (c_n (x-x_0)^n)'\\ \\ \\ \\ \\  (=\\sum_{n=0}^{\\infty} nc_n (x-x_0)^{n-1})\n$$\nimplying that $y$ is infinitely differentiable at $x_0$. If $y$ indeed has a power series representation at $x=x_0$, we refer to $y$ as *analytic*. This gives us everything we need to establish two more important classifications: ordinary and singular points. Let us return to the general form of our second-order linear DE:\n\n$$\n    p(x)y''+q(x)y'+r(x)y=0\n$$\nwhich can also be written as \n\n$$\n    y''+\\frac{q(x)}{p(x)}y'+\\frac{r(x)}{p(x)}y=0\n$$\n\n","n":0.046}}},{"i":115,"$":{"0":{"v":"Singular and Ordinary Points","n":0.5},"1":{"v":"\nWe define\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. (Singular and ordinary points.) If at a certain point $x_0$ the Taylor series (power series) for $\\frac{q(x)}{p(x)}$ and $\\frac{r(x)}{p(x)}$ do not exist - most commonly because $p(x_0)$ is zero - that point will be a \\it singular point\\normalfont. Conversely, if the Taylor series of both functions at $x=x_0$ do exist (the functions are analytic), then $x=x_0$ is an ordinary point.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>.\n(Regular and irregular singularities.) Let $x_0$ be a singular point of the aforementioned DE. Then if $\\frac{q(x)}{p(x)}$ contains no more than the first power of $(x-x_0)$ in its denominator when simplified, and $\\frac{r(x)}{p(x)}$ contains no more than the second power $(x-x_0)^2$ in its denominator, $x=x_0$ is a regular singularity. Otherwise, we deem it to be an irregular singularity.\n\nWhy is this so? For some singular points (namely, where $\\frac{q(x)}{p(x)}$ and $\\frac{r(x)}{p(x)}$ are undefined because $p(x)=0$ at $x=x_0$), the answer is obvious: the coefficients themselves are undefined, meaning that the differential equation would not make sense. However, if - as the definition of a regular singularity goes - the maximum power of $(x-x_0)$ is 2 in $\\frac{r(x)}{p(x)}$, and 1 in $\\frac{q(x)}{p(x)}$, the power series solution for the DE \n$$\ny=\\sum_{n=0}^{\\infty} c_n (x-x_0)^n\n$$\ncan cancel out these powers of $(x-x_0)$ as long as the first few terms are zero. This leads us to the solution\n$$\ny=\\sum_{n=0}^{\\infty} c_n (x-x_0)^{(n+\\sigma)},\n$$\nalso known as a Frobenius series, where $\\sigma$ can be any complex number with $c_0 \\neq 0$. This allows us to \"skip\" the first few terms of the Taylor series, and cancel out these pesky $x-x_0$ terms in the denominator that result in dividing by zero. If instead the singularity is irregular, then the same method no longer applies; power series solutions may not exist, or may behave strangely, at that particular point.\n\nFinally, at ordinary points where everything is well-defined, analytic, and has power series expansions, we simply have two linearly independent solutions of the form \n$$\ny=\\sum_{n=0}^{\\infty} c_n(x-x_0)^n\n$$\nabout the point $x=x_0$. \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **(Fuch's theorem).** The second-order differential equation \n$$\ny''+p(x)y'+q(x)y=g(x)\n$$\n> has a power series solution expressible in the form\n$$\ny=\\sum_{n=0}^{\\infty} c_n (x-x_0)^{(n+\\sigma)}\n$$\n> near $x=x_0$ if $p(x)$, $q(x)$ and $g(x)$ are analytic at that point, or that point is a regular singular point. If instead $x=x_0$ is an ordinary point, there exists two linearly independent solutions of the form \n$$\ny=\\sum_{n=0}^{\\infty} c_n(x-x_0)^n.\n$$","n":0.051}}},{"i":116,"$":{"0":{"v":"Resonance of Solutions","n":0.577},"1":{"v":"In the previous section, we mentioned the notion of an *indicial equation* that determined the index of the Frobenius series in the case of a regular singular point. As with differential equations with constant coefficients, it may be that this indicial equation yields repeated roots or roots $\\sigma_1,\\ \\sigma_2$ that have special characteristics. Let's examine these cases one by one:\n\n1. $\\sigma_2-\\sigma_1$ is not an integer. In this case, the two Frobenius series\n$$\n    \\sum_{n=0}^{\\infty}c_n(x-x_0)^{n+\\sigma_{1,2}}\n$$\nare independent solutions to the differential equation. \n2. $\\sigma_2-\\sigma_1$ is an integer (including when the two are equal). In this case, one series is contained within the other, so the Frobenius method cannot find two independent series. One independent solution will be \n$$\n    y_1=\\sum_{n=0}^{\\infty}c_n(x-x_0)^{n+\\sigma_2}\n$$\nfor $\\sigma_2\\geq \\sigma_1$, as dictated by the Frobenius series, but the other independent solution will usually be in the form\n$$\n    y_2=\\ln(x-x_0)y_1+\\sum_{n=0}^{\\infty}b_n(x-x_0)^{n+\\sigma_2}\n$$","n":0.086}}},{"i":117,"$":{"0":{"v":"Examples of Series Solutions","n":0.5},"1":{"v":"Let us present an example of the method of power series near an ordinary point. \n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>.  Find solutions as power series in $x$ of the differential equation\n$$\ny''+xy=0.\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\nFirst, we note that $x=0$ is an ordinary point - $x$ is clearly analytic here. As such, the usual power series solution \n\n$$\ny=\\sum_{n=0}^{\\infty} c_n(x)^n\n$$\n\nwill work in this case. Substituting into the equation yields \n\n$$\ny'=\\sum_{n=1}^{\\infty} nc_n(x)^{n-1}\n$$\n\n(note that the range of the sum has gone from $n=0$ to $n=1$, as the derivative of the first term in $y$, a constant, is zero). And thus:\n\n$$\ny'' = \\sum_{n=2}^{\\infty} n(n-1)c_n(x)^{n-2}\n$$\nSubstituting into the equation yields\n$$\n\\sum_{n=2}^{\\infty} n(n-1)c_n(x)^{n-2}+ \\sum_{n=0}^{\\infty} c_n(x)^{n+1} = 0.\n$$\n We want to shift the ranges of the two sums so that they have the same range and can be merged into a single sum. Inspection gives the first sum as \n$$\n\\sum_{n=0}^{\\infty} (n+2)(n+1)c_{n+2}x^n\n$$\nand thus the left-hand side of the DE as \n$$\n2c_2 + \\sum_{n=1}^{\\infty}((n+2)(n+1)c_{n+2}+c_{n-1})x^n = 0\n$$\nIt is important to note here that if a power series is zero, then all of its terms must be zero. Thus we can write: \n$$\n\\begin{cases}\n            2c_2=0 \\iff c_2 = 0 \\\\\n            (n+2)(n+1)c_{n+2}+c_{n-1} = 0 \\iff c_{n+2}=-\\frac{c_{n-1}}{(n+1)(n+2)},\\ n\\geq 1\n        \\end{cases}\n$$\nwhere two independent solutions arise from the values of $c_1$ and $c_2$.\n\n****\n\nLet's now consider an example of solutions near a regular singular point.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Consider the differential equation \n$$\n 4xy''+2(1-x)y'-y=0.\n$$\n> Find solutions as power series in $x$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\nFirst, we convert the above equation to a form with coefficient of $y''$ equal to 1:\n$$\ny''+\\frac{(1-x)}{2x}y'-\\frac{1}{4x}y = 0\n$$\n\n$x=0$ is a regular singular point (as the power of $x$ in the denominators of $\\frac{1-x}{2x}$ does not exceed 1, and in $\\frac{1}{4x}$, it does not exceed 2). We know from Fuch's Theorem that there exists a solution of the form \n\n$$\n        y=\\sum_{n=0}^{\\infty}c_n x^{n+\\sigma}\n$$\nwith corresponding derivatives\n$$\n y'=\\sum_{n=0}^{\\infty}c_n(n+\\sigma) x^{n+\\sigma-1}, \\ y'' =\\sum_{n=0}^{\\infty}c_n(n+\\sigma)(n+\\sigma -1)x^{n+\\sigma-2}\n$$\n Substituting into the equation gives:\n$$\n\\sum_{n=0}^{\\infty}[4c_n(n+\\sigma)(n+\\sigma-1)+2c_n(n+\\sigma)]x^{n+\\sigma-1} - \\sum_{n=0}^{\\infty}[2c_n(n+\\sigma)+1]x^{n+\\sigma} = 0\n$$\nand thus\n$$\n\\begin{cases}\n            [4c_0(\\sigma)(\\sigma-1)+2c_0(\\sigma)]x^{\\sigma}-1=0 \\\\\n            \\sum_{n=0}^{\\infty}[2c_{n+1}(n+\\sigma)(2(n+\\sigma+1)+1)-2c_n(n+\\sigma)-c_n]x^{n+\\sigma} = 0\n        \\end{cases}\n$$\nThe first term, involving only $\\sigma$, is known as the *indicial equation* (determines the value of the index):\n$$\n4c_0(\\sigma)(\\sigma-1)+2c_0(\\sigma) = 0 \\iff \\sigma(\\sigma-1)+\\frac{\\sigma}{2}=0,\\ \\sigma=0,\\ \\frac{1}{2}\n$$\nFrom the second term we obtain the recurrence relation\n$$\n2c_{n+1}(n+\\sigma)(2(n+\\sigma+1)+1)-2c_n(n+\\sigma)-c_n=0\n$$\nor equivalently\n$$\nc_{n+1}=\\frac{1+2(n+\\sigma)}{2(n+\\sigma)(2(n+\\sigma+1)+1)}c_n\n$$\nwhere $\\sigma = 0,\\ \\frac{1}{2}$ can be plugged in to obtain two independent solutions.\n\n","n":0.052}}},{"i":118,"$":{"0":{"v":"Second-Order Differential Equations","n":0.577},"1":{"v":"> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **second-order** ordinary differential equation in $x$ and $y$ is one whose highest-order derivative of $y$ is the second derivative. Most methods that apply to second-order ODEs also apply to higher-order equations as well. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **linear** second-order ODE is one that does not have any higher powers of $y, y', y''$ beyond the 1st power. As non-linear second-order ODEs are difficult to discuss analytically, the following sections will focus on solutions to linear ODEs.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Homogeneous** second-order ODEs are ones that can be written in the form $P(x)y'' + Q(x)y' + R(x) = 0$ (RHS is 0); inhomogeneous equations are such that the RHS is non-zero (some function of $x$).\n\nAs discussed, once the corresponding homogeneous equation has been solved, the inhomogeneous equation soon follows; the more fundamental problem is the solution of the inhomogeneous equation.\n\n","n":0.081}}},{"i":119,"$":{"0":{"v":"Second-Order Difference Equations","n":0.577},"1":{"v":"This section concerns equations of the form\n$$\nay_{n+2}+by_{n+1}+cy_{n} = f_n,\n$$\nalso known as recurrence relations of order 2. The method we can use for solving these is extremely similar to the one we used for constant-coefficient DEs: eigenfunctions.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. **(Eigenfunctions for difference equations). **\n\nSuppose that some operator $D[y_n] = y_n$ relates $y_n$ to $y_{n+1}$. As such, the above equation can be rewritten as:\n$$\naD^2[y_n] + bD[y_n] + cy_n = f_n\n$$\nThe eigenfunction of this operator is $y_n = \\lambda^n$ for some constant $\\lambda$. This is because, by definition, $y_{n+1} = \\lambda^{n+1}$ (simply by substituting $n$ with $n+1$), and thus \n$$\n D[y_n] = y_{n+1} = \\lambda^{n+1} = \\lambda y_n\n$$\nwhich is a constant multiple of $y_n$.\n\nUsing the exact same approach as we applied to differential equations, we solve the corresponding homogeneous difference equation $aD^2[y_n] + bD[y_n] + cy_n = 0$ first by substituting in the eigenfunction $y_n = \\lambda^n$:\n\n$$\na\\lambda^{n+2} + b\\lambda^{n+1} + c\\lambda^n = 0 \\iff a\\lambda^2 + b\\lambda + c = 0\n$$\n\nyielding a characteristic equation of degree 2. Similarly to differential equations, if the two roots are equal, then we have $y_n = (An+B)\\lambda^n$ for constants $A$, $B$. All that's left is to substitute in initial values of $y_n$ to find the constants, and we're done! \n\nTo deal with the pesky inhomogeneous term on the right-hand side, we once again resort to our tried-and-true, sophisticated, and wondrously erudite method of guessing. \n\nIf the right-hand side is an exponential function $k^n$ ($k \\neq \\lambda$), we guess $k^n$ or some multiple of this. (Be aware that $k$ is a constant!)\n\nFinally, if the right-hand side is a polynomial, we guess a polynomial of the same degree ($y_n = n^p + ..., y_{n+1} = (n+1)^p + ...$)\n\n\n\n","n":0.06}}},{"i":120,"$":{"0":{"v":"Physical Systems","n":0.707},"1":{"v":"After much suffering, we have escaped the section where my endless onslaught of inaccuracies and awe-inspiring handwaves have attracted the murderous intent of all mathematicians around the world. Unfortunately, that does not mean the murderous intent is gone. It's going to be back with a vengeance, magnified to three times of its original size, and directed towards me by physicists rather than mathematicians (who are much more murderous, on average). \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. If you're going to be murdered by someone, you'd rather be murdered by a physicist than a mathematician. This is because getting shoved into a particle accelerator and annihilated at the speed of light is a painless death, while getting stoned to death by someone hurling chalk at you is extremely painful. Don't ask me how I know.\n\nAs this section will hopefully elucidate, one of the true marvels of differential equations - second-order ones especially - is their prevalence in nature; countless physical models, from classical mechanics to electricity, rely on the same second-order DEs to tell their stories. \n\nWe begin this discussion with the simple model of a mass on a spring.\n\n![alt text](assets/images/DE-ch3-3.6.png)\n\nConsider an object with mass $m$, and hence weight $W=mg$, hanging on a spring with original length $l$. Due to the application of the mass, the spring elongates to a length $l + L$; by Hooke's law, we know that when the extension $L$ is suitably small (within the limit of proportionality), the spring experiences a restoring force $F_s = -kL$ in the opposite direction to the mass's weight, where $k$ is the constant of proportionality of the spring. When the spring is in equilibrium, the net force is zero and thus we have\n$$\n    mg = kL.\n$$\nHow does the system reach such an equilibrium? To answer this, we must study the displacement of the mass at time $t$ from this initial position, where the spring's extension is $L$. Call this displacement $u(t)$, or simply $u$, measured positively downwards from $l+L$. Thus, as shown in the figure, the length of the spring at any time is given by $l + L + u$, and hence the restoring force is given by \n$$\n F_s(t) = -k(L+u(t))\n$$\nThis equation is always true even if $L+u$ is negative (the spring is compressed), as in that case the restoring force would be positive. The mass will always apply a constant weight $W=mg$ downward. We must also take into account the existence of a resistive force on the mass, due to either air resistance (or resistance from other mediums), energy dissipation, friction, or some other source. We assume that this resistive force is proportional to the speed $u'(t)$ of the mass, and directed upward:\n$$\nF_d(t) = -\\gamma u'(t)\n$$\nwhere $\\gamma > 0$ is the \\it damping constant\\normalfont. This equation demonstrates that no matter the direction of the motion of the mass, the resistive force is always in an opposite direction. Finally, there may be an external force applied to the mass that varies with time; this may be because the spring is moving, or because of some periodic source that forces the mass to move. We call this force $F(t)$. \n\nTo summarize, our system involves the following forces:\n\n1. A constant weight $W=mg$, directed downward.\n2. A restoring force $F_s(t) = -k(L+u(t))$, originating from the tension in the spring and directed in the opposite direction to its displacement.\n3. A resistive force $F_d(t) = -\\gamma u'(t)$, opposite in direction to the mass's motion. Note that this simple model is not always accurate and can be questioned.\n4. A variable external force $F(t)$.\n\nBy Newton's Second Law, we have:\n$$\nF_{net} = ma(t) = mu''(t)\n$$\nwith $F_{net}$ being given by\n$$\n F_{net} = mg + F_s(t) + F_d(t) + F(t) = mg - k(L+u(t)) - \\gamma u'(t) + F(t)\n$$\nfinally yielding the inhomogeneous constant-coefficient second-order differential equation\n$$\nmu''(t) + \\gamma u'(t) + ku(t) = F(t)\n$$\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span> **(Second-order differential equation modeling of a physical system).** The differential equation that governs the displacement of a spring $u(t)$ over time is given by\n$$\nmu''(t) + \\gamma u'(t) + ku(t) = F(t)\n$$\n> where $\\gamma$ governs a damping force that is proportional to velocity $u'(t)$, $k$ governs a restoring force that is proportional to the displacement $u(t)$, $mu''(t)$ is the net force by Newton's Second Law, and $F(t)$ is a variable external force.\n\nNote that this equation makes several simplifying assumptions, namely that it assumes the restoring force is always under the limit of proportionality, that the resistive force is proportional to speed, and that the weight of the spring itself is negligible. \n\n## Undamped free vibrations\n\n\"Free\" vibrations are ones where the external force $F(t)$ is zero (as opposed to \"forced\" vibrations, which are forced to happen by the external force). If we suppose that there is no damping, then the damping constant $\\gamma$ is zero as well. This gives us the equation\n\n$$\nmu''(t)+ku(t) = 0\n$$\n\nwhich gives solutions\n\n$$\n u = A\\cos \\omega_0 t + B\\sin \\omega_0 t\n$$\n\nwhere $\\omega_0^2 = \\frac{k}{m}$. We can conveniently rewrite this in the form $u = R\\cos(\\omega_0 t - \\delta) = R(\\cos\\delta \\cos \\omega_0 t + \\sin \\delta \\sin \\omega_0 t)$, as comparing coefficients obtains\n\n$$\n\\begin{cases}\nA = R\\cos \\delta, B = R\\sin\\delta \\\\\nR^2 = A^2 + B^2, \\frac{B}{A} = \\tan \\delta\n\\end{cases}\n$$\n\nFrom $u = R\\cos(\\omega_0 t - \\delta)$ we observe that the *amplitude* of the vibrations is $R$, the *period* is $\\frac{2\\pi}{\\omega_0}$, the *phase* is $\\delta$ (displaced $\\delta$ radians rightward from normal), and the *natural frequency* is $\\omega_0$. \n\nIf no external force is applied, the system will continue oscillating without stopping, and will always vibrate at frequency $\\omega_0$ regardless of initial conditions (though the amplitude will be affected). The period is proportional to $\\sqrt{m}$ and inversely proportional to $\\sqrt{k}$, so larger masses vibrate more slowly and stiffer springs vibrate faster.\n\n## Damped free vibrations\n\nIn this case, the external force $F(t)$ remains zero but the damping coefficient $\\gamma > 0$. Our equation is \n$$\nmu''(t) + \\gamma u'(t) + ku(t) = 0\n$$\nwith characteristic equation\n$$\nm\\lambda^2 + \\gamma \\lambda + k = 0\n$$\nyielding solutions\n$$\n\\lambda_{1, 2} = \\frac{-\\gamma \\pm \\sqrt{\\gamma^2 - 4km}}{2m}\n$$\nThus, we will examine a few cases based on the value of $\\gamma$.\n1. $\\gamma < \\sqrt{4km}$. This is known as *underdamping*; the damping coefficient is very small, and the resistive force will also be fairly small in this case. As such, the solutions for $\\lambda$ are complex with negative real part ($\\gamma > 0$). We thus have \n$$\nu(t) = e^{-\\frac{\\gamma}{2m}t}(A\\cos \\mu t + B\\cos \\mu t), \\mu = \\frac{\\sqrt{4km-\\lambda^2}}{2m}\n$$\n![alt text](assets/images/DE-ch3-underdamping.png)\n\n2. $\\lambda = \\sqrt{4km}$. This is the *critical damping* case, where the damping given ensures that the system returns to equilibrium in the shortest time. In this case, we write\n$$\nu(t) = (At+B)e^{-\\frac{\\gamma}{2m}t}\n$$\n![alt text](assets/images/DE-ch3-criticaldamping.png)\n\n3. $\\lambda > \\sqrt{4km}$. This is the \\it overdamping \\normalfont case, where the damping force is too large and the system returns to equilibrium slower than if critically damped. The equation gives\n$$\nu(t) = Ae^{\\lambda_1 t} + Be^{\\lambda_2 t}.\n$$\n![alt text](assets/images/DE-ch3-overdamping.png)\n\n(Images courtesy of Dexter Chua. Thanks!)\n\nThese three cases each yield different forms of solutions for $u(t)$, and each correspond to a different physical scenario. It is very important to note that in every case, the value of $u(t)$ decays to 0 as $t \\to \\infty$; this confirms our expectations that damping (resistive forces) would lead to the system gradually being pushed back to equilibrium.\n\nIn the latter two cases (critical and over-damping), no oscillations occur; in the underdamping case, however, the system does begin oscillating (though at a decreasing amplitude). Similar to the undamped case, we can rewrite $u(t)$ for underdamping as \n$$\nu(t)=e^{-\\frac{\\gamma}{2m}t}(A\\cos\\mu t + B\\sin\\mu t) = Re^{-\\frac{\\gamma}{2m}t}\\cos(\\mu t - \\delta)\n$$\n\nfor certain $R$ and $\\delta$, with $\\mu = \\frac{\\sqrt{4km-\\gamma^2}}{2m}$ being referred to as the *quasi-frequency*. Analogously, $\\frac{2\\pi}{\\mu}$ is the *quasi-period*.\n\nTo examine the effect of underdamping on the system, let us compare the quasi-frequency to the undamped frequency $\\omega_0 = \\sqrt{\\frac{k}{m}}$. We find that \n$$\n    \\frac{\\mu}{\\omega_0} = \\frac{\\sqrt{4km-\\gamma^2}}{\\sqrt{4km}} = \\sqrt{1-\\frac{\\gamma^2}{4km}} \\approx 1-\\frac{\\gamma^2}{8km}\n$$\nwhere the final approximation is due to the binomial expansion, when $\\gamma$ is very small. Thus, the effect of underdamping is to slightly reduce the frequency of the system (and slightly increase the period). As $\\frac{\\gamma^2}{4km}$ gradually approaches 0, the frequency approaches 0, transforming the system into one that is critically damped.\n\n## Forced vibrations\n\nForced vibrations occur when the external force $F(t)$ is nonzero, producing an inhomogeneous second-order DE. Usually, this force will be an oscillating force of the form $F(t) = F_0 \\cos \\omega t$ where $F_0$ and $\\omega$ represent the amplitude and frequency, respectively. This results in the following differential equation:\n\n$$\nmu''(t) + \\gamma u'(t) + ku(t) = F_0 \\cos \\omega t\n$$\n\nwhich, assuming that the solution of the inhomogeneous equation is $u = c_1u_1 + c_2u_2$, yields the solution\n\n$$\n u(t) = c_1u_1 + c_2u_2 + A\\cos\\omega t + B\\sin\\omega t\n$$\n\nfor some constants $A$, $B$. As shown in the above section, $c_1u_1 + c_2u_2$ decays to 0 as $t \\to \\infty$; thus, we refer to $u_c(t) = c_1u_1 + c_2u_2$ as the *transient solution*. As $t$ grows larger, it will eventually become negligible in magnitude compared to $U(t) = A \\cos \\omega t + B\\sin \\omega t$, which persists indefinitely and is referred to as the *steady-state solution* or the *forced response*.\n\n![alt text](assets/images/DE-ch3-transient.png)\n\nThe transient part of the solution allows for any initial conditions to be satisfied. However, it vanishes very quickly compared to the steady-state solution.\n\n## Resonance\nIf our steady-state solution is rewritten algebraically as \n\n$$\nU(t) = R\\cos (\\omega t - \\delta)\n$$\nwe are able to observe that its amplitude $R = \\sqrt{A^2 + B^2}$ is dependent directly on $A$ and $B$, and thus dependent on the value of $F_0$. Through algebraic manipulation it is possible to show that:\n$$\n\\begin{cases}\n    R = \\frac{F_0}{\\Delta} \\\\\n    \\Delta = \\sqrt{m^2(\\omega_0^2 -\\omega^2)^2 + \\gamma^2\\omega^2} \\\\\n    \\omega_0^2 = \\frac{k}{m}\n\\end{cases}\n$$\nwith $\\omega_0$ referring to the system's natural frequency. We observe that when $\\omega \\to 0$, $R \\approx \\frac{F_0}{m\\omega_0^2} = \\frac{F_0}{k}$; and when $\\omega \\to \\inf$, $R \\approx 0$. Between those two extremes, there may be a value of $\\omega$ that yields a maximum value of $R$; differentiation reveals that such a value $\\omega_{max}$ lies at \n$$\n\\omega_{max}^2 = \\omega_0^2(1-\\frac{\\gamma^2}{2mk})\n$$\nwhich only exists if $\\frac{\\gamma^2}{2mk} < 1$; otherwise, $R$ is monotonously decreasing. We observe that when $\\gamma$ is small, $\\omega_{max}$ is very close to $\\omega_0$, the natural frequency. This phenomenon is known as *resonance*: at or near the natural frequency, the amplitude of the oscillations reaches a peak. This can have both positive and negative consequences, such as amplifying vibrations produced by vehicles across bridges and potentially causing a collapse.\n\n![alt text](assets/images/DE-ch3-resonance.png)\n\n","n":0.024}}},{"i":121,"$":{"0":{"v":"Nonhomogeneous Equations","n":0.707},"1":{"v":"We begin with an important result that will be used throughout our discussion in this section:\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. \"Nonhomogeneous\" shall be used instead of \"inhomogeneous\" (with certain exceptions, e.g. when I forget to).\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.     I think \"nonhomogeneous\" sounds cooler. $\\square$\n\nNow that that's out of the way, we move on to less important things. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>.  Nonhomogeneous second-order DEs are ones of the form \n$$\n    L[y](t) = g(t)\n$$\n> where $L[y](t)$ is as defined above, and $g(t)$ is any function of $t$ that is nonzero. The equation\n$$\n    L[y](t) = 0\n$$\n> is thus referred to as the homogeneous equation corresponding to $L[y](t) = g(t)$, and was discussed at length in the previous section. \n\nTo proceed, we formulate several statements that may seem trivial, but are important to our upcoming discussion:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Solutions to inhomogeneous equations. **    (Solutions to inhomogeneous equations). If $Y_1$ and $Y_2$ are both solutions to a certain inhomogeneous DE $L[y](t) = g(t)$, then $Y_1-Y_2$ is a solution to the corresponding homogeneous equation. \n\nThis can be easily proven by substituting $Y_1$ and $Y_2$ into the DE; thus, we have $Y_1 - Y_2 = c_1 y_1 + c_2 y_2$ for fundamental solutions $y_1$ and $y_2$ and constants $c_1, c_2$. As such, it quickly follows that \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Solutions to inhomogeneous equations. ** The general solution to $L[y](t) = g(t)$ can be fully described by\n\n$$\n        y(t) = c_1y_1 + c_2 y_2 + Y(t)\n$$\n\n>  where $c_1 y_1 + c_2 y_2$ is the general solution to the homogeneous equation and $Y(t)$ is a specific solution to the inhomogeneous equation.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.     As proven by the above theorem, for any two solutions $Y_1$ and $Y_2$ to $L[y](t) = g(t)$, we have $Y_1 - Y_2 = c_1y_1 + c_2y_2$. Thus set $Y_1 = y(t)$ and $Y_2 = Y(t)$ to obtain the desired result.\n\nThe implications of the above results inform us on how to approach such inhomogeneous equations:\n\n1. First, we should approach the corresponding homogeneous equation and obtain its solutions.\n2. We should then find some specific solution $Y(t)$ to the inhomogeneous equation.\n3. The general solution will be the combination of the results of the above two steps.\n\nThe second step of finding a specific solution, in particular, is of interest to us in this section. To this end, we present two different approaches:\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. **(Method of undetermined coefficients)**.\n\n...\"Method of undetermined coefficients\" is a more civilized way to say guessing. The method is guessing. \n\nThe idea behind this method is that the right-hand side $g(t)$ of the inhomogeneous equation can enable us to make educated guesses about what the solution might look like; for example, if it's an exponential function, then it's a bit unlikely that the solution will be a polynomial. We will assume the solution to be in some form (e.g. $e^{ax}$ where $a$ is unknown), set the coefficients of that form to be unknown, and then plug it into the DE to find the coefficients.\n  \nA table of what solution to guess based on the form of $g(t)$ is shown below:\n \n|$g(t)$ | Solution to guess |  \n| --- | --- |\n| Exponential ($e^{at}$) | $be^{at}$ |\n| \\sin kt, \\cos kt$ | $A \\sin kt + B \\cos kt$|\n| $P_n(t)$ (Polynomial of deg $n$) | Polynomial of the same degree |\n| $P_n(t)e^{at}$ | $Q_n(t)e^{at}$ |\n| $P_n(t)\\sin kt, P_n(t)\\cos kt$ | $Q_n(t)\\sin kt + R_n(t)\\cos kt$ |\n<br/>\n    \n\n...where $P_n(t), Q_n(t), R_n(t)$ represent polynomials of degree $n$ for some $n$. Essentially, whenever we have an elementary function multiplied by a polynomial for $g(t)$, guess a polynomial of the same degree for the solution. The proof of the above cases is left to the reader. (Yes, I'm making you do my dirty work.)\n\nOne thing to note about this method: the degenerate sub-case. (And no, that is not a description of myself.) What happens if the solution we guess turns out to be part of the homogeneous equation's solution? For instance, what if we guess $Ae^{4x}$, but $e^{4x}$ is a fundamental solution to the corresponding homogeneous equation? In this case - like in the repeated root case for constant coefficients before it - we multiply our original guessed solution with $t$, then proceed as normal. If the solution is still degenerate, then $t^2$ will do.\n\nThe main limitation of this method is its scope: it does not apply to anything other than linear or multiplicative combinations of elementary functions (trig, exponential, algebraic) - for instance, $\\sin e^t$ sould lead to absolute collapse. So what happens if the right-hand side of the inhomogeneous equation is more complicated?\n\n****\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. **(Method of variation of parameters)**. This method presents a general method for solving inhomogeneous second-order DEs, applicable to any equation; in fact, it is powerful enough to derive a general formula for the solution of any such equation. However, it involves considerably greater work and effort than the undetermined coefficients method, and the latter is still preferable when the right-hand side is a fairly simple function. \n\nWe begin by returning to our general inhomogeneous DE:\n$$\ny'' + p(t)y' + q(t) = g(t)\n$$\nand its corresponding homogeneous DE:\n$$\ny'' + p(t)y' + q(t) = 0\n$$\nAt this point, let us assume that the solutions to the homogeneous DE (the *complementary function*) are already known to be encompassed by the fundamental solutions $y_1$ and $y_2$:\n$$\ny(t) = c_1 y_1 + c_2 y_2\n$$\nRecall how, in previous sections (repeated roots case in constant coefficients), we managed to obtain one fundamental solution ($y_2 = Ate^{\\lambda t}$) off the other fundamental solution ($y_1 = e^{\\lambda t}$) by setting $y_2 = v(t) y_1$, leading to a differential equation in $v$.\n\nWe will try something similar here. Let the specific solution to the inhomogeneous DE - i.e. the *particular integral* - be $y_p(t)$, and assume that it is in the form\n\n$$\n y_p = v_1 y_1 + v_2 y_2\n$$\n\nwhere $v_1$ and $v_2$ are both functions of $t$. If we try to plug this in to the DE immediately, we obtain a sealed eldritch monstrosity from the ninth circle of hell that should have never been unleashed onto our plane of existence because repeated applications of the product rule leads to more than ten terms. \n\nMore importantly, if we just plug this into the DE with nothing else, we have two variables/functions that need to be solved for ($v_1$ and $v_2$) and only one equation. This would yield an infinite number of solutions; we only need one solution for $y_p(t)$, so let us impose another equation on $v_1$ and $v_2$.\n\nBut what is this other equation we should impose? Let's try finding $y_p'$ to get more information, as we want $y_p'$ to be as simple as possible:\n\n$$\ny_p' = v_1'y_1 + v_1y_1' + v_2'y_2 + v_2y_2'\n$$\n\nOur top priority is to make sure that $v_1$ and $v_2$ do not have second-order derivatives, because they are the variables we want to solve for and a second-order DE is much harder than a first-order one to solve. Thus, we don't want any $v_1'$ or $v_2'$ terms in $y_p'$. For this reason, we propose that our other equation is as follows:\n\n$$\n v_1'y_1 + v_2'y_2 = 0\n$$\n\nso that no $v_1'$ or $v_2'$ terms appear in $y_p'$ and thus no second-order derivatives appear in $y_p''$. We continue along this vein:\n\n$$\ny_p' = v_1'y_1 + v_1y_1' + v_2'y_2 + v_2y_2' = v_1y_1' + v_2y_2'\n$$\n\nfrom our other equation, and thus \n\n$$\n y_p'' = v_1'y_1' + v_1y_1'' + v_2'y_2' + v_2y_2''\n$$\n\nThis gives us enough information to plug in back to the original DE:\n\n$$\n\\begin{aligned}\n    y_p'' + p(t)y_p' + q(t)y_p &= g(t) \\\\\n    v_1'y_1' + v_1y_1'' + v_2'y_2' + v_2y_2'' + p(t)(v_1y_1' + v_2y_2') + q(t)(v_1y_1 + v_2y_2) &= g(t) \\\\\n    v_1(y_1'' + p(t)y_1' + q(t)y_1) + v_2(y_2''+p(t)y_2' + q(t)y_2) + v_1'y_1' + v_2'y_2' &= g(t)\n\\end{aligned}\n$$\n\nwhere everything in the parentheses in the final equation is zero due to $y_1$ and $y_2$ being solutions to the homogeneous equation. As such, we finally obtain\n\n$$\n v_1'y_1' + v_2'y_2' = g(t)\n$$\n\nas well as our other equation\n\n$$\nv_1'y_1 + v_2'y_2 = 0\n$$\n\n which gives us a system of equations in $v_1'$ and $v_2'$:\n\n$$\n\\begin{cases}\n    v_1'y_1' + v_2'y_2' = g(t) \\\\\n    v_1'y_1 + v_2'y_2 = 0\n\\end{cases}\n$$\n\nThis gives us enough information to find the general solution.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **(General solution for inhomogeneous second-order differential equations).** The particular integral $y_p(t)$ for any inhomogeneous second-order DE, with its corresponding homogeneous equation having fundamental solutions $y_1$ and $y_2$, is given by:\n\n$$\n\\begin{cases}\n    y_p(t) = v_1y_1 + v_2y_2 \\\\\n    v_1 = -\\int \\frac{g(t)y_2}{W(y_1,y_2)(t)} \\ dt \\\\\n    v_2 = \\int \\frac{g(t)y_1}{W(y_1,y_2)(t)}\n\\end{cases}\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nProceed from the system of equations above: multiplying the second equation by $\\frac{y_1'}{y_1}$ obtains\n$$\nv_1'y_1' + v_2'\\frac{y_1'y_2}{y_1} = 0\n$$\nand subtracting from the first obtains\n$$\nv_2'(\\frac{y_1y_2' - y_1'y_2}{y_1}) = g(t)\n$$\nwhich very conveniently contains a Wronskian:\n$$\nW(y_1,y_2) = y_1y_2' - y_1'y_2\n$$\nand hence\n$$\nv_2' = \\frac{g(t)y_1}{W(y_1,y_2)(t)}\n$$\nSubstituting into the second equation gives\n$$\n\\begin{aligned}\n    v_1'y_1 &= -v_2'y_2 \\\\\n    v_1' &= -\\frac{g(t)y_1}{W(y_1,y_2)(t)}\\frac{y_2}{y_1} \\\\\n    &= -\\frac{g(t)y_2}{W(y_1,y_2)(t)}.\n\\end{aligned}\n$$\nwhich is our desired result.\n","n":0.026}}},{"i":122,"$":{"0":{"v":"Inital-Condition Problems","n":0.707},"1":{"v":"In this section, we will define a new operator $L[\\phi]$ for some function $\\phi(t)$ twice differentiable on a given interval (becaus math just doesn't have enough Greek letters in it, apparently):\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Differential operator).** We define $L[\\phi] = \\phi'' + p\\phi' + q\\phi$, which is a function of $t$; $p$ and $q$ can either be constants or functions of $t$ also, and $L[\\phi](t) = \\phi''(t) + p(t)\\phi'(t) + q(t)\\phi(t)$. This will simplify our notations for second-order differential equations greatly (and prevent me from having to amputate my fingers from excess typing before I turn 21).\n\nThe motivation for this section comes from initial-condition problems arising from real-world scenarios; problems where, through a set of initial conditions (e.g. $t=0, y=...$), we are able to uniquely determine one solution to a DE. Consider, for instance, the general second-order homogeneous ODE $L[y](t) = 0$:\n\n$$\nL[y] = y'' + p(t)y' + q(t)y = 0\n\n$$\n\nWe can associate this equation with the initial condition $y(t_0) = y_0$: the value of the quantity $y$ at a certain time. However, this isn't enough to uniquely determine a solution; as this DE is second-order, two variables exist in its possible solutions. Thus we will need another initial condition - the value of $y'$ at a certain time: $y'(t_0) = y_0'$.\n\nThese types of problems are extremely commonplace in real-world modeling, and we would like to know:\n\n1. Whether we can find a solution to such initial-condition problems,\n2. Whether the solution is unique, and\n3. Whether the solutions have a particular form.\n\nIt turns out that we can answer all these questions in one fell swoop.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>.    ** (Existence and uniqueness theorem).** The initial-value problem $L[y](t) = 0$ with $y(t_0)=y_0, y'(t_0)=y'_0$ on some interval $I (a<t<b, t_0\\in I)$ has exactly one solution. In other words, this solution exists, it is unique, and it exists throughout $I$ where it is twice differentiable.\n\nNote that this interval $I$ is the interval in which $p(t)$ and $q(t)$ - the coefficients to $L[y](t)=0$ - are continuous, and $y(t)$ is also twice-differentiable and continuous.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nTrivial. (Behold but a fraction of my incredible power.)\n\nAt this point, it will be useful for us to formally state a result we have relied on in the past:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Principle</span>. **(Principle of superposition)**. If $y_1$ and $y_2$ are solutions to $L[y] = 0$, then the linear combination $c_1 y_1 + c_2 y_2$ is also a solution to $L[y] = 0$. This can be easily proven by the linearity of differentiation.\n\nBy this principle, we understand that with two independent solutions $y_1$ and $y_2$, we are able to construct an infinite family of solutions to a second-order ODE. The question arises, however, of whether other solutions that lie outside of the family also exist. \n\nTo determine this, we make good use of Theorem 3.4 (Existence and Uniqueness Theorem for initial-value problems). In a certain interval $I$, let $L[y] =0$ have initial conditions $y(t_0) = y_0, y'(t_0) = y_0'$; if $y$ can be written as $y=c_1 y_1 + c_2 y_2$, then\n\n$$\n\\begin{cases}\n        c_1y_1(t_0) + c_2y_2(t_0) = y_0 \\\\\n        c_1y_1'(t_0) + c_2y_2'(t_0) = y_0'\n    \\end{cases}\n$$\n\nWe can treat $y_1(t_0), y_1'(t_0)$ and such as constants to obtain a simple linear system of equations in $c_1$ and $c_2$; we can also rewrite this system in matrix form as\n\n$$\n\\begin{bmatrix}\n        y_1(t_0) & y_2(t_0) \\\\\n        y_1'(t_0) & y_2'(t_0) \n    \\end{bmatrix}\n    \\begin{bmatrix}\n        c_1 \\\\\n        c_2\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n        y_0 \\\\\n        y_0'\n    \\end{bmatrix}\n$$\n\nWe know that this system has a unique solution if the determinant to the first matrix of coefficients is nonzero:\n\n$$\n W = \\begin{vmatrix}\n        y_1(t_0) & y_2(t_0) \\\\\n        y_1'(t_0) & y_2'(t_0)\n    \\end{vmatrix}\n$$\n\nThis determinant is known as the **Wronskian**.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **Wronskian** of solutions $y_1, y_2$ to $L[y](t) = 0$ is the matrix determinant\n$$\n W = \\begin{vmatrix}\n            y_1 & y_2 \\\\\n            y_1' & y_2'\n        \\end{vmatrix}.\n$$\nThe equations above establish the following result:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>.** (Wronskian and Initial Value).** Let $y_1, y_2$ be two solutions to $L[y](t)=0$ with assigned initial values $y(t_0)=y_0, y'(t_0)=y_0'$. Then the unique solution to this initial-value problem can be found by choosing constants $c_1$, $c_2$ for $y=c_1 y_1 + c_2 y_2$ if and only if the Wronskian at that initial value $t_0$ is nonzero: \n\n$$\nW(y_1,y_2)(t_0) \\neq 0.\n$$\n\nAs such, from the above theorem and the Existence and Uniqueness Theorem, we derive our conclusion:\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **(Wronskian and Family of Solutions).** Let $y_1, y_2$ be two solutions to $L[y](t)=0$ over an interval $I$. Then the family of solutions\n\n$$\ny = c_1y_1 + c_2y_2\n$$\n\n> for constants $c_1, c_2$ encompasses every solution to $L[y](t) = 0$ if and only if there is a point $t_0$ in $I$ where the Wronskian is nonzero.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nLet $\\phi$ be any solution to $L[y](t)=0$. To prove the theorem, we must show that $\\phi$ will necessarily be in the form $c_1 y_1 + c_2 y_2$.\n\nLet $t_0$ be a point where $W(y_1,y_2)$ is nonzero. We write $y_0 = \\phi(t_0)$ and $y_0' = \\phi'(t_0)$; naturally, $phi(t)$ is a solution to the initial-value problem\n\n$$\nL[y](t) = 0, y(t_0) = y_0, y'(t_0) = y_0'\n$$\n\nBy the above theorems, as the Wronskian is nonzero at the initial point $t = t_0$, the system of equations that determines $c_1$ and $c_2$ has one unique solution; by the Existence and Uniqueness theorem, this solution is the only solution to this initial-value problem, so $\\phi$ must equal $c_1 y_1 + c_2 y_2$. The same process can be applied to any $\\phi$, and thus $c_1 y_1 + c_2 y_2$ encompasses every solution of the DE.\n\nConversely, assume that the Wronskian is always zero. Therefore, by the above theorems, it is not always possible to find constants $c_1$ and $c_2$ such that $c_1y_1 + c_2y_2$ is a solution to some initial-value problem. However, by the Existence and Uniqueness Theorem, such a solution must exist; thus, we conclude that $L[y](t) = 0$ has solutions outside the family $c_1y_1 + c_2y_2$. \n\n****\n\nThese theorems formally guarantee that if two fundamental (independent) solutions to a second-order DE can be found, then the general solution can be found. However, is it always possible to find such fundamental solutions? The answer is indeed yes.\n\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **(Fundamental solutions).** Consider the equation $L[y](t)=0$. Choose some point $t_0$ in an interval $I$ where the coefficients of the equation are continuous. Let $y_1$ be the solution to the equation that satisfies the initial conditions\n\n$$\ny(t_0) = 1, y'(t_0) =0\n$$\n\n> and $y_2$ be the solution that satisfies\n\n$$\ny(t_0) = 0, y'(t_0) =1.\n$$\n\n> Then $y_1$ and $y_2$ form fundamental solutions to $L[y](t) = 0$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nBy the Uniqueness and Existence theorem, $y_1$ and $y_2$ exist and are unique. The Wronskian $W(y_1, y_2)(t_0)$ equals\n$$\n        \\begin{vmatrix}\n            y_1(t_0) & y_2(t_0) \\\\\n            y_1'(t_0) & y_2'(t_0)\n        \\end{vmatrix}\n        = \n        \\begin{vmatrix}\n            1 & 0 \\\\\n            0 & 1\n        \\end{vmatrix}\n        =1\n$$\nand is thus nonzero. As such, $y_1$ and $y_2$ form a fundamental set of solutions. \n\n****\n\nThis theorem informs us that any second-order DE has two fundamental solutions; however, it's a bit too shy to tell us exactly how to find these solutions. Oh dear. So what can we do when we don't know these solutions? Are there any commonalities that are shared between the fundamental solutions of the same equation?\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **(Abel's Theorem).** If $y_1$ and $y_2$ are any two solutions to a second-order differential equation $L[y](t) = y''+p(t)y'+q(t) = 0$ where its coefficients are continuous on an interval $I$, then the Wronskian $W(y_1,y_2)(t)$ is given directly by \n$$\n        W(y_1,y_2)(t) = ce^{-\\int p(t)\\ dt}\n$$\n> where $c$ is a constant that only depends on what the choice of $y_1$ and $y_2$ is, not on $t$ (i.e. it is not a function of $t$). This implies that - as exponential functions are never zero - the Wronskian for $L[y](t)$ is either always zero ($c=0$) or never zero ($c \\neq 0$).\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nWe note that $y_1$ and $y_2$ satisfy    \n$$\n\\begin{cases}\n            y_1'' + p(t)y_1' + q(t) = 0 \\\\\n            y_2'' + p(t)y_2' + q(t) = 0\n\\end{cases}\n$$\no cancel out $q(t)$, we multiply the first equation by $-y_2$ and the second equation by $y_1$, then add them together to obtain\n$$\ny_2''y_1 - y_1''y_2 + p(t)(y_1y_2' - y_1'y_2) = 0\n$$\nNow we turn to $W(y_1, y_2)(t) = y_1 y_2' - y_1' y_2$. Note that \n$$\n\\begin{aligned}\n            W' &= y_1'y_2' + y_1y_2'' - y_1''y_2 - y_1'y_2' \\\\\n            &= y_1y_2'' - y_1''y_2.\n\\end{aligned}\n$$\nThis means that we can rewrite the above equation as\n$$\nW' + p(t)W = 0\n$$\nwhich is a first-order $DE$ that can be solved with integrating factor $e^{\\int p(t)\\ dt}$, yielding Abel's theorem.\n\nThe implications of Abel's theorem are:\n1. That all possible Wronskians of a second-order DE differ only by a multiplicative constant, no matter what fundamental solutions are picked.\n2. That the Wronskian can be determined without actually solving the equation.\n3. That the Wronskian is either always zero or never zero.\n\n\n\n","n":0.026}}},{"i":123,"$":{"0":{"v":"Impulse Functions","n":0.707},"1":{"v":"The last section was concerned with the existence of a persistent and periodic external force; however, in many cases, the forces that act on a system are not periodic, but \\it impulsive \\normalfont- a force of large magnitude that acts only over a short time interval. In other words, this section is concerned with systems modeled by differential equations of type\n\n$$\nau'' + bu' + cu = g(t)\n$$\n\nwhere $g(t)$ is large in a specific interval $t_0 - \\tau < t_0 < t_0 + \\tau$, but zero anywhere else. \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. We define the integral\n\n$$\nI(t) = \\int_{t_0-\\tau}^{t_0+\\tau} g(t)\\ dt\n$$\n\n> as the *impulse* of the force $g(t)$, a total measure of its strength; or, as $g(t)$ is zero everywhere outside of the interval, we can equivalently write\n\n$$\n    I(t) = \\int_{-\\infty}^{\\infty} g(t)\\ dt\n$$\n\nAs an example of this, let us consider the following piecewise (discontinuous) function $g(t)$:\n\n$$\ng(t) = \\begin{cases}\n        \\frac{1}{2\\tau},\\ -\\tau < t < \\tau\\\\\n        0\\ \\text{otherwise}\n    \\end{cases}\n$$\n\nwhere it follows that $I(t)$ is always equal to 1, as is verifiable by simple integration. In an ideally impulsive scenario - that is, when this force is as impulsive as possible - the force will act over an infinitesimally small time period; that is, we want $\\tau \\to 0$, at which point $\\lim_{\\tau \\to 0}I(t)$ is still 1. \n\nAt this limit, the impulsive force is instantaneous; in physical terms, this can mean that for a ball bouncing on the ground, the duration of its contact with the ground is negligible. A figure of this limiting procedure is shown below:\n\n![alt text](assets/images/DE-ch3-diracdelta.png)\n\nHence, we define the limit as $\\tau \\to 0$ of $g(t)$ to be \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. ** (Dirac delta function).** We define the idealized **unit impulse function**, or the **Dirac delta function** $\\delta(t)$, as the function which satisfies:\n\n$$\n\\begin{cases}\n    \\delta(t) = 0, t \\neq 0 \\\\\n    \\int_{-\\infty}^{\\infty} \\delta(t)\\ dt = 1\n\\end{cases}\n$$\n\nThis represents a function that is only equal to 1 at a discontinuity at $t = 0$; it must be noted that this is not a \"function\" in the usual sense, as it is zero everywhere except for one point, where it is essentially infinite. It follows from this definition that \n\n$$\n\\delta(t-t_0) = 0,\\ t \\neq t_0\n$$\n\nwhere $\\delta(t-t_0)$ can be used to represent a unit impulse at time $t_0$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>.** (Integrals involving delta functions).** For any function $g(x)$ continuous at $c$, we have\n\n$$\n\\int_{-\\infty}^{\\infty}g(x)\\delta(x-c)\\ dx = g(c)\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nFirst, by the definition of the delta function, we know that outside the specific interval $(c - \\tau, c + \\tau)$ as $\\tau \\to 0$, the function is zero:\n$$\n\\int_{-\\infty}^{\\infty}g(x)\\delta(x-c)\\ dx = \\lim_{\\tau \\to 0} \\int_{c-\\tau}^{c+\\tau} g(x)\\delta(x-c)\\ dx\n$$\nas everywhere else the delta function is zero, resulting in the integral being zero as well. By the Mean Value Theorem, we know that in the interval $(c-\\tau, c+\\tau)$ there is a value $k$ such that \n$$\n g(k) = \\frac{\\int_{c-\\tau}^{c+\\tau} g(x)\\delta(x-c)\\ dx}{(c+\\tau)-(c-\\tau)} = \\frac{\\int_{c-\\tau}^{c+\\tau} g(x)\\frac{1}{2\\tau}\\ dx}{(c+\\tau)-(c-\\tau)}\n$$\nby definition of $\\delta$. The $\\tau$ terms cancel and we are left with\n$$\ng(k) =\\int_{c-\\tau}^{c+\\tau} g(x)\\delta(x-c)\\ dx\n$$\nHowever, as $\\tau \\to 0$, the interval becomes infinitely small and only contains the value $x = c$. As such, $c = k$ and the theorem is proven.\n\n****\n\nLet's see an example of the Dirac delta function in action for impulse problems. \n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Consider a block of mass 1 kg applied to a spring with spring constant $k = 1$. Assume that there is no damping in this scenario, and set the initial conditions $u = 0$ at $t= 0$ and $t=\\pi$. At $t = \\frac{\\pi}{2}$, there is an instantaneous impulsive force applied to the system, modeled by the delta function $\\delta(t-1)$. Find an expression for the displacement $u(t)$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\nWe have:\n\n$$\n u'' + u = \\delta(t-\\frac{\\pi}{2})\n$$\n\nThe homogeneous equation has solutions\n$$\nu =  A\\sin t + B \\cos t\n$$\nThe initial conditions $u=0,\\ t=0$ gives $B = 0$. To deal with the inhomogeneous part (the delta function), we utilize the delta function's piecewise definition and consider the intervals $t < \\frac{\\pi}{2}$, $t = \\frac{\\pi}{2}$ and $t > \\frac{\\pi}{2}$. We assume that $u$ is continuous (both because this is a physical system where discontinuities would not make sense, and because it simplifies the calculations). \n\nWhen $t < \\frac{\\pi}{2}$, $\\delta(t-\\frac{\\pi}{2}) = 0$; thus the full solution is the same as the complementary function. When $t = \\frac{\\pi}{2}$, the delta function is infinite (and discontinuous), so we will have to use its limit definition inside an integral. First, we integrate both sides from $\\frac{\\pi}{2}^-$ to $\\frac{\\pi}{2}^+$:\n\n$$\n\\int_{\\frac{\\pi}{2}^-}^{\\frac{\\pi}{2}^+} u'' \\ dt + \\int_{\\frac{\\pi}{2}^-}^{\\frac{\\pi}{2}^+} u \\ dt = \\int_{\\frac{\\pi}{2}^-}^{\\frac{\\pi}{2}^+} \\delta(t-\\frac{\\pi}{2})\\ dt\n$$\n\nAs $u$ is continuous, the integral of $u$ at a single point is zero; the right-hand side equals 1 by definition. Thus we have\n\n$$\n[u']^{\\frac{\\pi}{2}^+}_{\\frac{\\pi}{2}^-} = 1 = A\\sin\\frac{\\pi}{2} + B\\cos\\frac{\\pi}{2} = A\n$$\n\nso $A = 1$. \n\n\n## Heaviside step function\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. We define $H(x)$, the Heaviside step function, as \n\n$$\n    H(x) = \\int_{-\\infty}^{x} \\delta(t)\\ dt.\n$$\n\n> Thus, we have\n\n$$\nH(x)=\\begin{cases}\n    0,\\ x<0 \\\\\n    1,\\ x>0 \\\\\n    \\text{undefined, }x=0\n\\end{cases}\n$$\n> with $\\frac{dH}{dx} = \\delta(x)$ by the Fundamental Theorem of Calculus. These relationships can only be used inside of integrals. \n\nThe Heaviside step function is useful in modeling many physical scenarios, namely systems like switches of an electronic circuit which can be turned on and off indefinitely.","n":0.034}}},{"i":124,"$":{"0":{"v":"Equidimensional Equations","n":0.707},"1":{"v":"We take a brief, contractually-obligated segue from our studies in inhomogeneous equations to arrive at equidimensional DEs.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Equidimensional equations).** Also known as the Euler equation, because of course it is, we define an equidimensional equation to be of the form\n$$\n        L[y] = x^2y'' + \\alpha xy' + \\beta y = g(x)\n$$\n\n> It is named so because every term ($x^2y'', xy', y$) is of the same dimension; if $y$ was in meters and $x$ in seconds, then $y'$ would be the rate of change of $y$ (m/s), $y''$ would be in m/s$^2$, and so every term in the DE have the same dimensions.\n\nThe method behind solving such equations is largely similar to constant-coefficient equations: solving by eigenfunctions.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. **(Solving equidimensional equations by eigenfunctions).**\n\nNotice that $y=x^k$ for any $k$ is an eigenfunction of the equation: e.g. $x^2y'' = x^2(k)(k-1)x^{k-2} = k(k-1)y$, a constant multiple of $y$. Simply set $y = x^k$ to obtain a characteristic equation in $k$:\n\n$$\n k(k-1) + \\alpha k + \\beta = 0\n$$\n\nIf there are two real solutions for $k$ - $k_1$ and $k_2$ - then (as the Wronskian would attest) $x^{k_1}$ and $x^{k_2}$ are the two fundamental solutions. The same goes for two distinct complex roots $a+bi$ and $a-bi$:\n\n$$\n\\begin{aligned}\ny &= Ax^{a+bi} + Bx^{a-bi} \\\\\n&= Ae^{\\ln x^{a+bi}} + Be^{\\ln x^{a-bi}}  \\\\\n&= Ae^{(a+bi)\\ln x} + Be^{(a-bi)\\ln x} \\\\\n&= Ae^{a\\ln x}e^{b(\\ln x)i} + Be^{a\\ln x}e^{-b(\\ln x)i} \\\\\n&= Ax^a(\\cos (b\\ln x) + i\\sin (b\\ln x)) + Bx^a(\\cos (b\\ln x) - i\\sin (b\\ln x))\n\\end{aligned}\n$$\nby Euler's formula. This has real part\n$$\nAx^a\\cos (b\\ln x) + Bx^a\\sin (b \\ln x)\n$$\n\nIn order to deal with the repeated root (degeneracy) case, we need a different method as we can no longer multiply the first solution by $x$. Thus, we attempt a substitution:\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. **(Substitution for repeated-root equidimensional equations).**\n\nWe don't quite know how to deal with equidimensional equations (with $x^k$ as the solutions) yet, but we do know how to deal with constant-coefficient equations (with $e^{kx}$ as solutions). Let's try the substitution $z=\\ln x, \\frac{dz}{dx} = \\frac{1}{x}$. Then:\n\n$$\n\\begin{aligned}\n    z = \\ln x &\\iff x = e^z \\\\\n    y(x) &= y(e^z) \\\\\n    \\frac{dy}{dz} &= y'(e^z) e^z = xy' \\\\\n    \\frac{d^2y}{dz^2} &= e^z y'(e^z) + e^{2z}y''(e^z) = xy' + x^2y''\n\\end{aligned}\n$$\nand thus \n$$\n\\begin{aligned}\n    x^2 y'' + \\alpha x y' + \\beta y = g(x) &\\iff (\\frac{d^2 y}{dz^2} - \\frac{dy}{dz}) + \\alpha \\frac{dy}{dz} + \\beta y = g(e^z) \\\\\n    &= \\frac{d^2 y}{dz^2} + (\\alpha - 1)\\frac{dy}{dz} + \\beta y \n\\end{aligned}\n$$\nwhich is a standard constant-coefficient DE, with characteristic equation $\\lambda^2 + (\\alpha-1)\\lambda + \\beta=0$. If there is a repeated root $\\lambda$, then:\n$$\ny = (Az+B)e^{\\lambda z} = (A\\ln x + B)e^{\\lambda \\ln x} = (A\\ln x + B)x^{\\lambda}\n$$\nwhich is the desired result for the repeated root case. It is worth noting that, analogously to how we multiply by $x$ when a guess for a particular integral fails, we multiply by $\\ln x$ in the case of an equidimensional equation.\n","n":0.045}}},{"i":125,"$":{"0":{"v":"Constant-Coefficient Equations","n":0.707},"1":{"v":"Second-order ODEs with constant coefficients are the simplest case; they can be expressed in the form\n$$\n    ay'' + by' + cy = f(x)\n$$\nwhere $a$, $b$ and $c$ are constants. Our method for solving such equations are to be expected: we first determine the solution to the equivalent homogeneous equation (call this the *complementary function*), then determine the particular integral that would satisfy the inhomogeneous case.\n\n## Complementary functions\nThe basic idea of finding the complementary function of a second-order ODE boils down to the concept of eigenfunctions in linear algebra. Differentiation can be viewed as a linear map; that is, it satisfies the property $\\frac{d}{dx}(au + bv) = a\\frac{d}{dx}u + b\\frac{d}{dx}v$ for functions $u, v$ of $x$ and constants $a, b$ - this is known as \"linearity\".\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. **Characteristic equations**. The two linearly independent solutions to homogeneous linear second-order ODE $ay''+by'+cy=0$ can be obtained as $e^{\\lambda_1 x}$ and $e^{\\lambda_2 x}$, where $\\lambda_1$ and $\\lambda_2$ are solutions to the **characteristic equation** $a\\lambda^2+b\\lambda+c=0$, as long as $\\lambda_1 \\neq \\lambda_2$ (no repeated roots).\n\nIf $\\lambda_1 \\neq \\lambda_2$, then the complementary function in full is all possible linear combinations of the two solutions: $Ae^{\\lambda_1 x} + Be^{\\lambda_2 x}$, where $A$ and $B$ are arbitrary constants.\n\nIf $\\lambda_1$, $\\lambda_2$ are complex conjugates ($a\\pm bi$), we have the complementary function $e^{ax}(A \\sin bx + B \\cos bx)$ by Euler's formula.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **(Repeated root case).** If the characteeristic equation above yields a repeated root $\\lambda_1 = \\lambda_2$, we instead have linearly independent solutions $xe^{\\lambda x}$ and $e^{\\lambda x}$.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nIf $\\lambda_1 = \\lambda_2$ (repeated root), we only have one independent solution when two are expected of a second-order ODE; this is called *degeneracy*. In order to find the other solution $y_2$, we set $y_2 = v(x)y_1$ where $y_1$ is the known solution $e^{\\lambda_1 x}$; this is particularly convenient for us because we already know that $y_1$ satisfies the equation. Thus, we have:\n\n$$\n\\begin{aligned}\n    a(vy_1)'' + b(vy_1)' + cvy_1 &= 0 \\\\\n    a(v'y_1 + vy_1')' + b(v'y_1 + vy_1') + cvy_1 &= 0 \\\\\n    a(v''y_1 + 2v'y_1' + vy_1'') + bv'y_1 + bvy_1' + cvy_1 &= 0 \\\\\n    (ay_1)v'' + (2ay_1' + by_1)v' + (ay_1''+by_1'+cy_1)v &= 0 \\\\\n\\end{aligned}\n$$\nwhere the last term is 0 due to the original DE. Therefore, we have\n$$\n            (ay_1)v'' + (2ay_1' + by_1)v' = 0\n$$\nNow substitute $y_1$ for $e^{\\lambda_1 x}$. We have $av'' + (2\\lambda_1 a + b)v' = 0$ (dividing the exponential function off, as it is nonzero); it is known that $a\\lambda^2 + b\\lambda + c = 0$, so differentiating both sides with respect to $\\lambda$ gives $2a\\lambda + b = 0$. Thus, our final equation is $av'' = 0$ or $v'' = 0$, yielding that $v$ is a linear function in $x$. \n\nWe finally conclude that, for the repeated root case, our complementary function is $(Ax+B)e^{\\lambda x}$ with repeated root $\\lambda$ and arbitrary constants $A, B$. \n\n\n","n":0.046}}},{"i":126,"$":{"0":{"v":"Partial Differential Equations","n":0.577},"1":{"v":"The big cheese.\n\nNormal equations - the ones we grew up with, ones of the form $x+5=10$ or $2x+3=5$ or \"solve for $t$ in $e^{-t} = 0$, where $t$ is the time elapsed before your dad comes back with the milk - describe what things are like now: the $x$ in $x+5=10$, for instance, is equal to $5$ now, and it always will be. Differential equations, in contrast, describe how things change over time; instead of finding a single solution that tells us what it is at that present moment, we have to find a function that, at all times, is related to its own rate of change in some meaningful way - something necessitated in nearly every realm of the real world, from an object whose air resistance depends on its own speed of motion, to a disease which spreads more quickly the more people it infects.\n\nSo what do partial differential equations do? Things in the real world don't just change based on one variable alone; we live in three dimensions, not two, and the models we need to model that three-dimensional world also needs to be three-dimensional. PDEs give us the ability to express relationships not only through the rate of change of something based on a single variable, but based on multiple variables - space and time, instead of one or the other. \n\n\n","n":0.067}}},{"i":127,"$":{"0":{"v":"Wave Equation","n":0.707},"1":{"v":"\nThe transport equation gave a general model of a function that moved forward without changing its shape; for each value of $t$, the graph for the shape of the wave was identical barring a translation to the left or right. \n\nBut in real life, many examples of waves do not simply translate; they also oscillate up and down as they translate forward, necessitating the use of another model. For instance, see the following animation:\n\n![](assets/images/f1Wqa.gif)\n\nAs we can see, the red wave and the blue wave are both examples of solutions to the transport equation: they translate forward, but the graphs do not oscillate. The magenta wave, however, is an example of a solution to the wave equation: as it travels forward, its peaks and troughs oscillate and constantly change. \n\nThe question is, what physical laws govern these oscillations? We know based on intuition that at the wave's peaks, there should probably be a *restoring force* pulling the wave downwards towards the zero-line, and at its troughs (bottom points), there should also be a force pulling the wave up.\n\n![](assets/images/DE-ch4-wave2.jpg)\n\nWhen the wave is below the horizontal axis, the concavity (second derivative) is positive, as the gradient of the wave is increasing; when the wave is above the horizontal axis, the concavity is instead negative. This leads us to the hypothesis that the force on each point of the wave is proportional to the concavity of the point, or its second derivative; in other words, \n\n$$\n\\frac{\\partial^2 u}{\\partial t^2} = k\\frac{\\partial^2 u}{\\partial x^2}\n$$\n\nfor some constant $k$, where $u$ is the displacement of the wave (in a vertical direction), $u_{tt}$ is the second derivative with respect to time and thus the acceleration of the wave (proportional to the force on the wave, by Newton's Second Law), and $u_{xx}$ is the second derivative of the wave with respect to its horizontal displacement - in other words, its concavity along the $x$-axis.\n\nThis is a reasonable enough hypothesis, and it turns out we have enough information to prove it rigorously. Let's consider a tiny segment of the wave as follows, and make the following three assumptions:\n\n1. This small section of the wave is nearly perfectly straight. ~~That means you won't see it making a profile on Grindr anytime soon~~\n2. The angle of the section to the horizontal can be modelled by a function of $x$ and $t$, $\\theta(x,t)$, which is nearly zero but not constant.\n3. The vertical displacement of this small section is fairly small; we can make this assumption because the section itself is infinitely small, so the force acting on it will be of a similar magnitude. In other words, this small section of the wave is assumed to be nearly horizontal; if we take $\\theta$ to be its angle with the horizontal, then its slope, $\\tan \\theta$, is very close to $\\sin \\theta$ (as $\\cos \\theta \\approx 1$). \n4.  There are no external forces acting on the wave, only the force of its own tension $T(x,h)$ (if we imagine the wave to be a string); in other words, the wave or string is perfectly flexible.\n5. The density of the section is constant: $\\rho(x,t)=\\rho$.\n\nA visualization of this is shown below:\n\n![](assets/images/DE-ch4-wave3.jpg)\n\nLet the starting point of the section be $(x,t)$ and the endpoint be $(x+h,t)$. The length of the section, being nearly horizontal, is approximately $h$. By Newton's Second Law, we know that the net force on the section is equal to $ma = mu_{tt}=\\rho h u_{tt}$. There are no other forces acting on the section other than tension, so the net force is simply \n\n$$\nT(x+h,t)\\sin(\\theta(x+h,t))-T(x,t)\\sin(\\theta(x,t))\n$$\n\nor the vertical component of the tension at the endpoint, subtracted by the vertical component of the tension at the starting point. The horizontal components can be assumed to be equal and opposite (since $T$ and $\\theta$ change negligibly from $x$ to $x+h$). Thus, we write \n\n$$\nT(x+h,t)\\sin(\\theta(x+h,t))-T(x,t)\\sin(\\theta(x,t)) = \\rho h u_{tt}\n$$\n\nor, equivalently, dividing by $h$ obtains \n$$\n    \\frac{T(x+h,t)\\sin(\\theta(x+h,t))-T(x,t)\\sin(\\theta(x,t))}{h} = \\rho u_{tt}\n$$\nwhere, by first principles of derivatives, the left-hand side is equal to \n$$\n    \\frac{\\partial}{\\partial x}(T(x,t)\\sin\\theta(x,t))\n$$\nThe pesky fact of the matter here is that we have a $\\sin \\theta$ term that we do not want. As we stated earlier, however, the slope of our section (which can also be written $u_x$, since the section is infinitely small) is equal to $\\tan \\theta(x,t)$, which, when $\\theta$ is very small, is approximately equal to $\\sin \\theta(x,t)$. Thus, the above expression is equivalent to \n$$\n    \\frac{\\partial}{\\partial x}(T(x,t)u_x)\n$$\nwhich, when we assume that tension is constant throughout the section, leaves us with simply\n$$\n    Tu_xx=\\rho u_{tt}\n$$\nor \n$$\n    c^2 u_xx= u_{tt}\n$$\nwhere $c$ is a constant. (We can show that $c$ is actually equal to the speed of the wave by means of dimensional analysis; $u_xx$ has dimensions $m^{-1}$, $u_{tt}$ has $m\\ s^{-2}$, and thus their ratio is of dimension $m^2\\ s^{-2}$ - the square of $m\\ s^{-1}$.) This gives us our desired wave equation. \n \nThe solution to the wave equation is motivated by its connection to the transport equation. To start with, we need to ask ourselves: does a solution to the transport equation satisfy the wave equation? Intuitively it does, since the solution to the transport equation is simply a wave with no oscillation at all, and should still satisfy the wave equation. To verify this, let's assume that \n$$\n    cu_x + u_t=0\n$$\nsuch that some $u(x,t)$ satisfies the transport equation. We know that $u(x,t)$ is in the form $f(x+ct)$; this is the forward transport equation. Thus, we have \n$$\n    u_{tt}=c^2f_{tt}(x+ct)=c^2 u_{xx}\n$$\nTherefore, the transport equation solves the wave equation too. But to obtain general solutions, we need another function $g$ that can create oscillations on top of the function $f$, which just moves forward without oscillating. This can happen if $g$ is a wave also satisfying the transport equation, but moving in the opposite direction; this way, the two waves will interfere due to superposition. Thus we have \n$$\n    g(x-ct)\n$$\nas the second term of our solution; we can quickly verify that $g(x-ct)$ also solves the wave equation. Thus, our complete solution is \n$$\n    u=f(x+ct)+g(x-ct)\n$$\nfor any differentiable functions $f$ and $g$; essentially, the solution to the wave equation is two waves described by the transport equation moving in opposite directions. Quickly verifying this by plugging this back into the wave equation yields the system of equations \n$$\n    \\begin{cases}\n        u_{t}+cu_x=g \\\\\n        g_t-cg_x=0\n    \\end{cases}\n$$\nwhich gives back $u=f(x+ct)+g(x-ct)$. \n\nTo prove that this is the most general solution, we observe that we can write the wave equation as \n$$\n    \\frac{\\partial^2 u}{\\partial t^2} - c^2\\frac{\\partial^2 u}{\\partial x^2} = (\\frac{\\partial}{\\partial t} + c\\frac{\\partial}{\\partial x})(\\frac{\\partial}{\\partial t} - c\\frac{\\partial}{\\partial x})u = 0\n$$\nAnd thus $u$ solves either $u_t+cu_x=0$ or $u_t-cu_x=0$, yielding the previous two solutions.\n\n(For a visualization of how the solutions to the wave equation arise from two solutions to the transport equation in opposite directions, see the animation above.)\n\n\n","n":0.03}}},{"i":128,"$":{"0":{"v":"Transport Equation","n":0.707},"1":{"v":"\nWe begin our discussion of PDEs with a generalized example that can describe many different situations: a wave traveling in one direction. What do we mean by \"wave\", in this case? As an example, consider two people holding a rope. When one person jiggles the rope a little bit, there's going to be a displacement that occurs on that person's end:\n\n![](assets/images/DE-ch4-transportequation-1.jpg)\n\nNaturally, this disturbance on the rope is going to travel forward and reach the other person as time elapses:\n\n![](assets/images/DE-ch4-transport2.jpg)\n\nWhat we mean by a \"wave\" is just any function - say a function with displacement $u$ at position $x$ - that travels forward as the time $t$ increases, much like how the \"ripple\" on the rope travels forward from left to right. \n\nSuppose that this wave had speed $c$; that is, if at one moment you had a point on the wave with position $x=x_0$ and displacement $u$, then $t$ seconds later, the same point would have travelled to position $x_0 + ct$ instead. \n\n![](assets/images/DE-ch4-transport3.jpg)\n\nAt any one value of $t$, the wave has a single graph: you can say with certainty that, for every value of $x$, we have one displacement $u$. But as $t$ increases, the graph begins shifting to the right at a rate dictated by the speed of the wave $c$. \n\nAs such, we can model a wave traveling forward as an infinite collection of graphs for $u$ against $x$, one for each value of time $t$, each being identical to the other except for being shifted to the left or right by a certain number of units ($ct$ units) - a multivariate function $u = f(x,t)$. \n\nWhat does this mean for us? Most importantly, it means that if we are traveling alongside the wave - if we also travel at a speed $c$, and in the same direction as the wave - we will not see the graph of the wave changing.\n\nFor example, let's say we start at the peak of the wave, at position $x=0$; three seconds pass, and by now, the peak of the wave is at position $x=3c$ for some wavespeed $c$. However, we are also traveling at a speed $c$, so three seconds later, we are also at $x=3c$; and thus we again see the peak of the wave, and nothing has changed. \n\n![](assets/images/DE-ch4-transport4.jpg)\n\nThis means that, if we travel in the direction $x = x_0 + ct$ - if we travel at a speed of $c$, increasing our position $x$ by $c$ units every 1 unit of time $t$ - we will experience no change in the displacement of the wave $u$. In essence, if we keep up with the speed of the wave, the wave will look like it isn't changing at all; in differential-equation terms, its directional derivative along the direction we are traveling, $\\begin{bmatrix}    c\\\\1\n\\end{bmatrix}$, is zero:\n\n$$\n\\begin{bmatrix}\n            c\\\\1\n        \\end{bmatrix}\\cdot \\begin{bmatrix}\n            u_x \\\\ u_t\n        \\end{bmatrix} = 0\n$$\n\nyielding the *transport equation*\n\n$$\nu_t+cu_x=0.\n$$\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. The **transport equation**, which describes all wave functions traveling in one direction, is given by the PDE\n$$\nu_t+cu_x=0.\n$$\n> where $u(x,t)$ is the displacement of the wave, $x$ is its position, $t$ the time elapsed, and $c$ the wave speed.\n\nIn basic terms, this equation encodes the information that the dot product between the gradient and the direction $[c,1]$ - the direction of traveling at a constant speed $c$ in the $x$-direction - is zero; the displacement of the wave does not change if you travel in that direction.\n\nHow do we go about solving this equation? The crux of the matter again relies on the fact that the lines $x-ct=x_0$ are *characteristic lines* of the transport equation - they are lines where the function $u$ does not change at all with respect to time: $\\frac{\\partial u}{\\partial t}=0$. If we move along these lines $x = x_0 + ct$, then $u(x,t)=u(x_0+ct, t)$ - a function in $t$ only - and we know that $\\frac{\\partial u}{\\partial t}=\\frac{du}{dt}=0$.\n\nThus we have the system of ordinary differential equations \n\n$$\n\\begin{cases}\n        \\frac{\\partial u}{\\partial t} = 0 \\\\\n        x = x_0 + ct\n    \\end{cases}\n$$\n\nFrom the first equation, we know that $u$ is independent of $t$ along $x = x_0 + ct$ and only depends on $x$: $u = f(x)$. Substituting the second equation gives \n\n$$\nu=f(x_0+ct)\n$$\n\nas our general solution for any function $f$ that is differentiable, or simply $u=f(x+ct)$ as $x_0$ is not fixed and can be any value of $x$. If the initial condition $u(x,0)=g(x)$ is imposed, then we have $u=g(x+ct)$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The solution to the **transport equation** is given by $u(x,t) = f(x+ct)$ for a constant **wave-speed** $c$.\n\n\n\n\n\n","n":0.036}}},{"i":129,"$":{"0":{"v":"Heat Equation","n":0.707},"1":{"v":"\nThe heat equation, or the diffusion equation, models the conduction of heat throughout a solid (in one dimension only, though it is possible to generalize to more dimensions). \n\nTo understand it, let us consider the case of a 1D metal rod with zero (or negligible) thickness, and some length $L$. Let the temperature of the rod at any given point $x$ along the rod at a time $t$ be $T(x,t)$, such that $T(x,0)$ describes the temperature distribution of the rod at time $t=0$, $T(x,1)$ at $t=1$, and so on:\n\n![](assets/images/DE-ch4-heat1.jpg)\n\nAt time $t_0$, every point on the rod has a temperature $T(x)$, and the value of $T(x)$ is expressed on the plot above. If we combine such plots for every value of $t$, we obtain something as follows:\n\n![](assets/images/DE-ch4-heat2.jpg)\n\nIntuition tells us that as time elapses, the graph will \"flatten out\" (as shown above); heat will disperse evenly throughout the rod. If we can codify that sense of intuition into the language of differential equations, we'll have our heat equation. \n\nTo do that, let's consider a discrete, rather than continuous, version of the rod. Say, for instance, that we are measuring the temperature of the rod at ten distinct points:\n\n![](assets/images/DE-ch4-heat3.jpg)\n\nLet's consider only the behavior of a single point:\n\n![](assets/images/DE-ch4-heat4.jpg)\n\nOn the left-hand case, the point $x_2$ has a higher temperature than both of its neighbors $x_1$ and $x_3$, telling us that its temperature will tend to decrease; on the right-hand case, though $x_1$ has a higher temperature than $x_2$ and $x_3$ has lower, the average of $x_1$ and $x_3$ still exceeds $x_3$, making it tend to increase in temperature. \n\nThus, we can write that for a discrete point $x_2$ on the rod with temperature $T(x_2,t)$, the rate of change of its temperature is proportional to the average of its temperature difference with its neighboring points:\n\n$$\n    \\frac{\\partial T}{\\partial t} = \\alpha(\\frac{(T_3-T_2)-(T_2-T_1)}{2})\n$$\nfor a certain proportionality constant $\\alpha$. $(T_3-T_2)-(T_2-T_1) = \\Delta T_2 - \\Delta T_1 = \\Delta \\Delta T_1$, the *second difference* of $T_1$; as the distance between neighboring points becomes infinitesimal, the second difference is analogous to the second derivative with respect to $x$. Thus, we obtain the heat equation: \n$$\n    \\frac{\\partial T}{\\partial t}=\\kappa \\frac{\\partial^2 T}{\\partial x^2}\n$$\nwhere $\\kappa$ is the *diffusivity* of the rod. \n\nSolutions to the heat equation lie once again in the method of changing variables from $(x,t)$ to a single variable $\\eta$ such that the function $T$ can be rewritten entirely in terms of $\\eta$: $T(x,t)=T(\\eta)$. \n\nNote that this is the same method we used with solving the transport equation, where we had $x = x_0 + ct$ which yielded a function entirely in $t$. The question is, what is $\\eta$? \n\nUnlike the transport equation, there are many, many different possible ways to define $\\eta$ as some function of $t$ and $x$ such that it helps us solve the heat equation, so let us consider only one:\n$$\n    \\eta = \\frac{x}{2\\sqrt{\\kappa t}}\n$$\nwhere $\\kappa$ is again the diffusivity. This is known as a *similarity solution*, as it only encompasses all solutions which are in exactly this form (can be written in terms of $\\eta$) and are similar to one another. \n\nIf we posit that $T(x,t)$ can be written entirely as $T(\\eta(x,t))=F(\\eta)$, a single-variable function of $\\eta$, then we have\n$$\n    \\begin{cases}\n        \\frac{\\partial T}{\\partial x} = \\frac{\\partial F}{\\partial \\eta} \\frac{\\partial \\eta}{\\partial x} \\\\\n        \\frac{\\partial T}{\\partial t} = \\frac{\\partial F}{\\partial \\eta} \\frac{\\partial \\eta}{\\partial t}\n    \\end{cases}\n$$\nand with $\\eta = \\frac{x}{2\\sqrt{\\kappa t}}$, we have \n$$\n    \\begin{cases}\n        \\frac{\\partial \\eta}{\\partial x} = \\frac{1}{2\\sqrt{\\kappa t}} \\\\\n        \\frac{\\partial \\eta}{\\partial t} = -\\frac{\\kappa x}{4}(\\kappa t)^{-\\frac{3}{2}}\n    \\end{cases}\n$$\nresulting in\n$$\n    \\begin{cases}\n        \\frac{\\partial T}{\\partial x} = \\frac{1}{2\\sqrt{\\kappa t}}\\frac{\\partial F}{\\partial \\eta} \\\\\n        \\frac{\\partial^2 T}{\\partial x^2} = \\frac{1}{2\\sqrt{\\kappa t}}\\frac{\\partial^2 F}{\\partial \\eta^2} \\frac{\\partial \\eta}{\\partial x} = \\frac{1}{4\\kappa t}\\frac{\\partial^2 F}{\\partial \\eta^2} \\\\\n        \\frac{\\partial T}{\\partial t} = \\frac{\\partial F}{\\partial \\eta} \\frac{\\partial \\eta}{\\partial t} = -\\frac{\\kappa x}{4}(\\kappa t)^{-\\frac{3}{2}}\\frac{\\partial F}{\\partial \\eta} = -\\frac{1}{2}\\frac{x}{2\\sqrt{\\kappa t}t}\\frac{\\partial F}{\\partial \\eta} = -\\frac{\\eta}{2t}\\frac{\\partial F}{\\partial \\eta}\n    \\end{cases}\n$$\nSubstituting this into the original heat equation yields\n$$\n    \\begin{aligned}\n        -\\frac{\\eta}{2t}\\frac{dF}{d\\eta}&=\\frac{1}{4t}\\frac{d^2F}{d\\eta^2} \\\\\n        \\frac{d^2F}{d\\eta^2}+2\\eta\\frac{dF}{d\\eta}=0\n    \\end{aligned}\n$$\nwhich solves to obtain $\\frac{dF}{d\\eta} = Ae^{-\\eta^2}$ and thus the non-elementary $F=\\alpha\\ \\text{erf}(\\eta) + \\beta$ for the error function erf, defined as \n$$\n    \\text{erf}(\\eta)=\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\eta}e^{-u^2}\\ du\n$$\n\n> <span style=\"background-color: #12ffd7; color: black;\">Fin</span>!\n","n":0.039}}},{"i":130,"$":{"0":{"v":"Multivariate Differential Equations","n":0.577},"1":{"v":"In this section, we will be focusing our efforts on the multivariate function $z=f(x,y)$ and all its generalized relatives: how we can bring our tools of calculus from two dimensions to three-dimensional space, and how differential equations operate within that space.\n","n":0.156}}},{"i":131,"$":{"0":{"v":"Stationary Points","n":0.707},"1":{"v":"Consider a point $(x,y)=(a,b)$ on the contours of the function $f(x,y)=z$; in particular, consider its gradient \n$$\n \\nabla f = \\begin{bmatrix}\n        \\frac{\\partial f}{\\partial x} (a,b) \\\\\n        \\frac{\\partial f}{\\partial y} (a,b)\n    \\end{bmatrix}.\n$$\nIf either partial derivative is nonzero, that implies that we can increase (or decrease) the value of $f$ by taking a small step in that direction; that tells us $(a,b)$ is not a stationary (maximum or minimum) point, as we can reach a higher or lower value of $f$ immediately next to $(a,b)$. Thus, our condition for a critical point is \n$$\n\\nabla f = \\vec{0}\n$$\nor, equivalently, that all partial derivatives of $f$ are zero at that point. Much as univariate calculus gave rise to points of inflection, however, a special case arises where $\\nabla f = 0$ but the point is not a stationary point; call these types of points *saddle points*.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. A **saddle point**, analogously to points of inflection for single-variable functions, is a point where the gradient is zero but the function is not locally maximized or minimized.\n\n![alt text](/assets/images/DE-ch4-saddlepoint.png)\n\nWhy does this happen? The above graph might give us a bit of a clue; it is the graph of $f(x,y)=x^2-y^2$, with saddle point $(x,y)=(0,0)$. At that point, the function $f(x,0)=x^2$ and the function $f(0,y)=-y^2$ are minimized and maximized respectively; this means that at $(0,0)$, the function curves up in one direction and down in another direction, creating a point where the function is transitioning between turning up and down (similar to points of inflection).\n\n## Taylor series for multivariate functions\n\nConsider a point $\\vec{s_0}=(x_0,y_0)$ on $f(x,y)=f(\\vec{s})=z$, and a small displacement $\\delta \\vec{s}=(\\delta x, \\delta y)$ from that point. We want to find an approximation, similar to the one we obtained for single-variable functions through Taylor series, for the value of:\n\n$$\nf(x_0+\\delta x, y_0+\\delta y) = f(\\vec{s_0}+\\delta \\vec{s})\n$$\n\nLet $x = x_0 + \\delta x$, $y=y_0 + \\delta y$. We'll consider the terms of the approximation one by one. The first term, as is known by Taylor's theorem, is simply $f(x_0,y_0)$. The second term should be equal to the change in $(x_0,y_0)$ multiplied by its rate of change - in multidimensional terms, this is the directional derivative of $\\begin{bmatrix}x-x_0\\\\y-y_0\n\\end{bmatrix}=\\delta \\vec{s}$:\n\n$$\nf(x,y)=f(x_0,y_0)+\\nabla_{\\delta \\vec{s} f} =f(x_0,y_0)+[(x-x_0)\\frac{\\partial f}{\\partial x} + (y-y_0)\\frac{\\partial f}{\\partial y}].\n$$\n\nBy Taylor's theorem, the quadratic term will be $\\frac{1}{2!}$ times the change in $x$ and $y$, $\\delta \\vec{s}$, times the second directional derivative in that direction. This is given by \n\n$$\n\\begin{aligned}\n        \\frac{1}{2!}(\\nabla_{\\delta \\vec{s}}\\nabla_{\\delta \\vec{s}})f &= \\frac{1}{2!}\\nabla_{\\delta \\vec{s}}[(x-x_0)\\frac{\\partial f}{\\partial x} + (y-y_0)\\frac{\\partial f}{\\partial y}] \\\\\n        &= \\frac{1}{2!}\\delta \\vec{s}\\cdot \\nabla([(x-x_0)\\frac{\\partial f}{\\partial x} + (y-y_0)\\frac{\\partial f}{\\partial y}])\\\\\n        &= \\frac{1}{2!}\\delta \\vec{s}\\cdot\\begin{bmatrix}\n            (x-x_0)\\frac{\\partial^2 f}{\\partial x^2} +(y-y_0)\\frac{\\partial^2 f}{\\partial x \\partial y}\\\\\n            (y-y_0)\\frac{\\partial^2 f}{\\partial y^2}+(x-x_0)\\frac{\\partial^2 f}{\\partial x \\partial y}\n        \\end{bmatrix} \\\\\n        &= \\frac{1}{2!}[(x-x_0)^2 \\frac{\\partial^2 f}{\\partial x^2} + 2(x-x_0)(y-y_0)\\frac{\\partial^2 f}{\\partial x \\partial y}+(y-y_0)\\frac{\\partial^2 f}{\\partial y^2}]\n    \\end{aligned}\n$$\n\nNote that $(x-x_0)$ and $(y-y_0)$ are not treated as variables in the above because they are $\\delta x$ and $\\delta y$, independent from $x$ and $y$. According to the above, we define \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Hessian matrix**. The Hessian matrix for $f(x,y)$ is the matrix\n$$\n\\nabla \\nabla f = \\begin{bmatrix}\n    f_{xx} & f_{xy} \\\\\n    f_{yx} & f_{yy}\n\\end{bmatrix}\n$$\n> such that the above quadratic term in the Taylor expansion for $f(x,y)$ can also be written \n$$\n \\frac{1}{2!}\\begin{bmatrix}\n            \\delta x & \\delta y\n        \\end{bmatrix}\\nabla \\nabla f \\begin{bmatrix}\n            \\delta x \\\\ \\delta y\n        \\end{bmatrix}\n$$\n\nThe Hessian matrix encodes information about the second-order partial derivatives for $f$. It can be generalized for $f(x_1,x_2,\\dots,x_n)$, a multivariate function in $n$ variables, as \n\n$$\n\\nabla \\nabla f = \\begin{bmatrix}\n            f_{x_1x_1} & f_{x_1x_2} & \\dots & f_{x_1x_n} \\\\\n            f_{x_2x_1} & f_{x_2x_2} & \\dots & f_{x_2x_n} \\\\\n            \\vdots & \\vdots & \\vdots & \\vdots \\\\\n            f_{x_nx_1} & f_{x_n x_2} & \\dots & f_{x_nx_n}\n        \\end{bmatrix}\n$$\n\nand the Taylor expansion term as \n\n$$\n \\frac{1}{2!}(\\delta s)^{\\mathbf{T}}\\nabla \\nabla f (\\delta s)\n$$\nwhere \n$$\n\\delta s = \\begin{bmatrix}\n            \\delta x_1 \\\\ \\delta x_2 \\\\ \\vdots \\\\ \\delta x_n\n        \\end{bmatrix}\n$$\n\nand the $\\mathbf{T}$ indicates matrix transposition.\n\n## Classification of multivariate stationary points\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **(Hessian determinant test.)** The Hessian determinant test for classifying stationary points of multivariate functions is as follows. Let $H = \\nabla \\nabla f(x,y)$ be the Hessian matrix as defined above:\n\n$$\nH = \\begin{bmatrix}\n            f_{xx} & f_{xy} \\\\\n            f_{yx} & f_{yy}\n        \\end{bmatrix}\n$$\n\n> If the point $(x_0,y_0)$ with associated Hessian $H(x_0,y_0)$ is a stationary point, then we can classify its nature as follows:\n\n$$\n\\begin{cases}\n            \\det H>0,\\ f_{xx}(x_0,y_0) > 0: \\text{ $(x_0, y_0)$ is a minimum.}\\\\\n            \\det H>0,\\ f_{xx}(x_0,y_0) < 0: \\text{ $(x_0, y_0)$ is a maximum.}\\\\\n            \\det H<0: \\text{ $(x_0, y_0)$ is a saddle point.}\\\\\n            \\det H = 0: \\text{ the point is indeterminate in nature.}\n        \\end{cases}\n$$\n\nWe can also refer to this test through the *signature* of the Hessian, which will be useful in higher-dimensional cases. For an $n$-variable Hessian matrix in $(x_1,x_2,\\dots,x_n)$, let its signatures be\n\n$$\n f_{x_1 x_1},\\ \\begin{vmatrix}\n            f_{x_1x_1} & f_{x_1x_2}\\\\\n            f_{x_2x_1} & f_{x_2x_2}\n        \\end{vmatrix},\\ \\dots,\\ \\begin{vmatrix}\n            f_{x_1x_1} & f_{x_1x_2} & \\dots & f_{x_1x_n} \\\\\n            f_{x_2x_1} & f_{x_2x_2} & \\dots & f_{x_2x_n} \\\\\n            \\vdots & \\vdots & \\vdots & \\vdots \\\\\n            f_{x_nx_1} & f_{x_n x_2} & \\dots & f_{x_nx_n}\n        \\end{vmatrix}\n$$\n\nName these terms $H_1, H_2, \\dots, H_n$ respectively. If $H_1, H_2, \\dots, H_n$ have signs $+, +, \\dots, +$ (all positive) at a certain stationary point, then the point is a local minimum; if the signatures are $-, +, -, \\dots, +, \\dots$ (alternating signs starting with negative), then the point is a local maximum. Otherwise, if the determinant is not zero, the point is a saddle point.\n\n****  \n\nWhy does this hold true? It is beyond our abilities to provide a full proof at the moment, but using the tools of eigenvalues in linear algebra, we can provide an intuitive explanation for all of this. \n\nWe first note that $H$ is \"symmetric\" no matter how many dimensions or variables there are; when we flip it around the diagonal, it remains the same because for a continuous function, $f_{xy}=f_{yx}$. (This symmetry is most evident in the two-dimensional case). \n\nA result from linear algebra tells us that because of this symmetry, $H$ can be diagonalized, i.e. it can be written in terms of its eigenvalues:\n$$\n(\\delta \\vec{s})^{\\mathbf{T}}\\cdot H\\cdot \\delta\\vec{s} = \\begin{bmatrix}\n        \\delta x_1 & \\delta x_2 & \\dots & \\delta x_n \n    \\end{bmatrix}\\begin{bmatrix}\n        \\lambda_1 & & & \\\\\n        & \\lambda_2 & & \\\\\n        & & \\ddots & \\\\\n        & & & \\lambda_n\n    \\end{bmatrix}\\begin{bmatrix}\n        \\delta x_1 \\\\ \\delta x_2 \\\\ \\dots \\\\ \\delta x_n \n    \\end{bmatrix}\n$$\n\nwhich then equals the sum\n\n$$\n \\lambda_1(\\delta x_1)^2+\\lambda_2(\\delta x_2)^2 + \\dots + \\lambda_n(\\delta x_n)^2\n$$\n\nwhere $\\lambda_1, \\lambda_2, \\dots$ denote the eigenvalues of $H$. This is relevant because the above term, $(\\delta \\vec{s})^{\\mathbf{T}}\\cdot H\\cdot \\delta\\vec{s}$, is a part of the Taylor expansion of $f$:\n\n$$\nf(x_1+\\delta x_1, x_2+\\delta x_1, \\dots, x_n+\\delta x_n) =f(x_1, x_2, \\dots, x_n) + \\delta\\vec{s} \\cdot \\nabla f +  \\frac{1}{2} (\\delta \\vec{s})^{\\mathbf{T}}\\cdot H\\cdot \\delta\\vec{s}\n$$\n\nwith $\\nabla f$ being 0 at any stationary point, so \n\n$$\nf(x_1+\\delta x_1, x_2+\\delta x_1, \\dots, x_n+\\delta x_n) =f(x_1, x_2, \\dots, x_n) + \\frac{1}{2} (\\delta \\vec{s})^{\\mathbf{T}}\\cdot H\\cdot \\delta\\vec{s}\n$$\n\nIf the point $(x_1, x_2, ..., x_n)$ is a minimum, then we can expect any point close to it to be greater than it; thus, the last term in the above equation must be positive. Conversely, if the point is instead a minimum, any point next to it must be smaller than it, and the last term is a negative. We know that the last term is the sum\n\n$$\n \\lambda_1(\\delta x_1)^2+\\lambda_2(\\delta x_2)^2 + \\dots + \\lambda_n(\\delta x_n)^2\n$$\n\nwhich is positive if every eigenvalue is positive (minimum point), negative if every eigenvalue is negative (maximum point), and indeterminate if the signs are mixed (saddle point). This information about eigenvalues, as will become evident in the Linear Algebra course, is encoded within the determinant of the Hessian.\n\n\n","n":0.028}}},{"i":132,"$":{"0":{"v":"Directional Derivative and Gradient","n":0.5},"1":{"v":" Before we can be concerned with differential equations, we have to concern ourselves with rates of change.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Gradient for multivariate functions).** Define $\\nabla f$, the \\it gradient \\normalfont of $f(x,y)=z$, as the vector\n\n$$\n\\nabla f = \\begin{bmatrix}\n            \\frac{\\partial f}{\\partial x} \\\\\n            \\frac{\\partial f}{\\partial y}\n        \\end{bmatrix}\n$$\n> and for the generalized multivariate case, a column vector of partial derivatives of $f$ with respect to all its variables.\n\n\nNote that this is simply a definition; we haven't attached any geometric significance to the gradient yet, but we will - eventually. \n\nTo get there, consider the following question. We know that $\\frac{\\partial f}{\\partial y}$ gives us the rate of change of $f$ with small changes in $x$, and the partial derivative in $y$ with small changes in $y$; however, what if we choose to move not along one of the axes, but (as is possible in three dimensions) along a certain vector? For instance, take the unit vector\n$$\n\\vec{s} = \\begin{bmatrix}\n        a \\\\ b\n    \\end{bmatrix}\n$$\nfor some $a$, $b$ with $\\sqrt{a^2+b^2}=1$. If we're going to be moving along $\\vec{s}$, then we'll be moving $a$ units in the $x$-direction and $b$ units in the $y$-direction. The partial derivatives give us how much $f$ changes when we move along each direction, so we can write \n$$\n\\frac{\\partial f}{\\partial s} = a\\frac{\\partial f}{\\partial x} + b\\frac{\\partial f}{\\partial y}\n$$\nas the rate of change of $f$ along $\\vec{s}$. Note that we can also rewrite the above as \n$$\n\\frac{\\partial f}{\\partial \\vec{s}} = \\nabla f \\cdot \\vec{s} \\ (\\text{notated }\\nabla_{\\vec{s}} f)\n$$\nwhich gives us an insight on what the gradient represents: essentially, it is a vector representing the \"base\" rate of change at a point that all other directional derivatives are based off of. \n\nWe also note that, since the dot product is maximized when $\\cos \\theta = 1$ and the angle $\\theta$ between the two vectors equalling zero, the directional derivative is at a maximum when $\\vec{s}$, the direction travelled in, is in the same direction as the gradient. Thus, the gradient is the direction that maximizes the rate of change of the function. If instead $\\vec{s}$ is parallel to the gradient, then the directional derivative is zero. \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The gradient $\\nabla f$ of a multivariate function $f$ at a certain point is a vector that geometrically symbolizes the direction of greatest rate of change of $f$ from that point.\n\n\n\n","n":0.05}}},{"i":133,"$":{"0":{"v":"First-Order Differential Equations","n":0.577},"1":{"v":"A differential equation is any equation that involves derivatives; solutions to differential equations are functions which satisfy the equation.\n*First-order* differential equations are such equations in which only first derivatives are involved.\n## The exponential function\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. The derivative of an exponential function is always a constant multiple of itself.\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nConsider $f(x) = a^x$ where $a$ is a constant. By first principles, the derivative of this function is given by\n$$\n    \\begin{aligned}\n    \\frac{df}{dx} &= \\lim_{h \\to 0} \\frac{a^{x+h}-a^x}{h}\\\\\n    &= \\lim_{h \\to 0} \\frac{a^{x}(a^h-1)}{h}\\\\\n    &= a^x \\lim_{h \\to 0}\\frac{a^h-1}{h} \\\\\n    &= \\lambda a^x\n    \\end{aligned}\n$$\nwhere $\\lambda = \\frac{a^h-1}{h}$ is a constant because it is equal to $f'(0)$. Therefore, the derivative of an exponential function is always a constant multiple of itself.\n\n\nIn particular, we define\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>.\n    **(Exponential function)**. $\\exp{x}=e^x$ is the unique function $f(x)$ such that $f'(x)=f(x)$, satisfying $f(0)=1$. We write its inverse function as $\\ln x$. The importance of this function is that it is an * eigenfunction * of the differential operator; under the operator, the function is unchanged except for a scalar (constant) multiple.\n\n## Defining and classifying differential equations\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>.\n    **(Linear differential equation).** A DE is linear if its dependent variables ($y, y', y''$ etc.) appear only linearly (with powers of 1).\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>.\n    **(Homogeneous differential equation).** A DE is homogeneous if $y=0$ is a solution.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>.\n   ** (Differential equation with constant coefficients).** A DE has constant coefficients if the variable $x$ does not appear in the coefficients.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>.\n    **($n$th-order differential equations).** An $n$th-order DE is one which has derivatives of $y$ up to the $n$th derivative.\n","n":0.06}}},{"i":134,"$":{"0":{"v":"Solution Curves","n":0.707},"1":{"v":"How can we understand the nature of the family of solutions to a differential equation (when the constant term in the solution varies) without actually solving it? The following methods will introduce several key points to plotting such solution curves:\n1. Spotting constant solutions (e.g. $y = \\pm 1$)\n2. Analyzing the sign of $\\frac{dy}{dx}$ for $x=0, x>0$ and $x<0$\n3. **Determining stability**. (See below)\n4. Finding ***isoclines*: **curves along which $\\frac{dy}{dx}$ is constant.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. **(Isoclines)**. For a differential equation $\\frac{dy}{dx} = ...$, an *isocline* is a curve on which $\\frac{dy}{dx}$ is constant, e.g. $\\frac{dy}{dx} = 1$; they can be found simply by setting $\\frac{dy}{dx}$ to that value. (For instance, if $\\frac{dy}{dx}=x^2+3, x^2+3=1 \\iff x^2 = -2$ is the curve on which $\\frac{dy}{dx}=1$.)\n\nIsoclines are useful in plotting solution curves because they can be plotted with a slope field, i.e. at every point on the isocline, its slope can be drawn as an arrow. The solution curves to the DE must have a shape that follow these arrows.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. **(Equilibrium points and stability).**\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Equilibrium point).** An **equilibrium point** or **fixed point** of a DE is a constant solution $y=c$, which corresponds to $\\frac{dy}{dt} = 0$.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Stability of an equilibrium point)**. An equilibrium is **stable** if, whenever a solution to the DE $y = c+\\epsilon(t)$ for an arbitrarily small $\\epsilon(t)$, $y$ eventually converges to $c$ as $x \\to \\infty$. Similarly, an equilibrium is **unstable** if it diverges as $x \\to \\infty$. \n\nIn graphical terms, if the solutions to the DE converge to a constant which is also a solution, it is a stable equilibrium/fixed point.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **(Perturbation theorem)**. For a fixed point of a DE $y=a$ and an arbitrarily small $\\epsilon(t) << 1$, the change of the perturbation of the solution function $y=a+\\epsilon(t)$ about $y=a$ can be approximated by \n$$\n        \\frac{d\\epsilon}{dt} \\approxeq \\epsilon \\frac{\\partial f}{\\partial y}(a, t)\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nSuppose $y=a$ is a fixed point of $\\frac{dy}{dt} = f(y,t)$, with $\\frac{dy}{dt} = 0$ and thus $f(a,t) = 0$. Consider $y = a + \\epsilon (t)$ for an arbitrarily small function of $t$, $\\epsilon(t)$. Plugging this into the DE obtains\n$$\n\\begin{aligned}\n        \\frac{d}{dt} y &= \\frac{d}{dt} (a + \\epsilon(t)) \\\\\n        &= \\frac{d\\epsilon}{dt} \\\\\n        &= f(a+\\epsilon(t), t) \\\\\n        &= f(a,t) + \\epsilon(t)\\frac{\\partial f}{\\partial y}(a,t) + O(\\epsilon^2)\n    \\end{aligned}\n$$\ndue to Taylor's theorem generalized to multivariate functions. If $\\epsilon$ is arbitrarily small, terms on the order of $O(\\epsilon^2)$ can be ignored, and so $\\frac{d\\epsilon}{dt} \\approxeq \\epsilon \\frac{\\partial f}{\\partial y}$.\n\nConsider the following example to demonstrate the method of perturbation analysis:\n\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Find the equilibrium points of $\\frac{dy}{dt} =y(y-1)(y-2)$ and analyze their stability.\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\nWe begin by setting $\\frac{dy}{dt} = 0$ to find all the equilibrium points; then $y(y-1)(y-2)=0$, so $y = 0, 1, 2$ are the equilibrium points of this DE. \n\nNow consider the perturbation function $\\epsilon(t)$. First note that $f(t,y) = y(y-1)(y-2)$ in this case, so $\\frac{\\partial f}{\\partial y} = (y-1)(y-2) + y(y-2) + y(y-1) = 3y^2 - 6y + 2.$ For $y = 0$ we have\n$$\n\\begin{aligned}\n        \\frac{d\\epsilon}{dt} &\\approx \\epsilon \\frac{\\partial f}{\\partial y}(0) = 2\\epsilon \\\\\n        \\int \\frac{d\\epsilon}{\\epsilon} &= \\int 2\\ dt \\\\\n        \\ln \\epsilon &= 2t + C \\\\\n        \\epsilon &= \\epsilon_0 e^{2t}\n\\end{aligned}\n$$\nwhich is unstable as it tends to infinity as $t$ grows; for $y = 1$ we have\n$$\n    \\begin{aligned}\n        \\frac{d\\epsilon}{dt} &\\approx \\epsilon \\frac{\\partial f}{\\partial y}(1) = -\\epsilon \\\\\n        \\epsilon &= \\epsilon_0 e^{-t} \\\\ \\\\\n    \\end{aligned}\n$$\nwhich is stable as $e^{-t}$ tends to 0 as $t$ grows. Finally, for $y=2$ we have\n$$\n    \\begin{aligned}\n        \\frac{d\\epsilon}{dt} &\\approx \\epsilon \\frac{\\partial f}{\\partial y} = 2\\epsilon \\\\\n        \\epsilon &= \\epsilon_0 e^{2t}\n    \\end{aligned}\n$$\nwhich is the same case as $y=0$ (unstable equilibrium).\n","n":0.04}}},{"i":135,"$":{"0":{"v":"Non-Linear Equations","n":0.707},"1":{"v":"This section studies any general equation of the form\n$$\nQ(x, y)\\frac{dy}{dx} + P(x,y) = 0\n$$\nwhere, if the power of $y$ or $y'$ is higher than 1, the equation is said to be *non-linear*. Two cases arise for these equations:\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Separable equations).** This case applies for equations that can be separated into the form $f(x) dx = g(y) dy$, making the equation relatively simple as both sides can simply be integrated. For instance, the equation $(3x+2)\\frac{dy}{dx} = y$ can be separated into $\\frac{dy}{y} = \\frac{dx}{3x+2}$, with each side containing one variable only.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Exact equations)**. (Exact equations). We define a DE in the form $Q(x,y)\\frac{dy}{dx} + P(x,y) = 0$ to be **exact** if and only if there is a function $f(x,y)$ such that $\\frac{\\partial f}{\\partial x} = P(x,y)$ and $\\frac{\\partial f}{\\partial y} = Q(x,y)$; that is, if $Q(x,y)$ is one partial derivative of $f$ and $P$ is the other partial derivative.\n\nAlternatively, if the equation is rewritten $Q(x,y) dy + P(x,y) dx = 0$, it is exact if and only if there is a function $f(x,y)$ such that $df = P\\ dx + Q\\ dy$.\n\nTo check whether a DE is exact, notice that mixed second derivatives are equal ($f_{xy} = f_{yx}$); thus, if $P$ and $Q$ are both partial derivatives of the same function $f$, then $P_y$ = $Q_x$. This method only holds if the domain of $f$ is *simply-connected*.\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Simply-connected domain.** A domain $\\mathbf{D}$ is simply-connected if:\n1. The domain is connected (closed) and not open.\n2. Any closed loop in $\\mathbf{D}$ can be shrunk to a point that is still on $\\mathbf{D}$.\n\nFor instance, a sphere is simply-connected while a donut is not. Yum!\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. If $\\frac{\\partial P}{\\partial y} = \\frac{\\partial Q}{\\partial x}$ through a simply-connected domain $\\mathbf{D}$, then the differential equation $Qy' + P = 0$ is exact.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. **(Solving exact differential equations)**. \n\nIf it is known that $df = P\\ dx + Q\\ dy$, then $f(x,y) = \\lambda$ for some constant $\\lambda$ is a solution to the DE. In order to find $f$, we integrate $\\frac{\\partial f}{\\partial x} = P$ with respect to $x$, giving a function plus a function depending only on $y$ (due to $y$ being held constant in the partial derivative).\n\nThis result can then be differentiated with respect to $y$ to obtain $\\frac{\\partial f}{\\partial y} + h'(y) = Q + h'(y)$, which can be compared to $Q$ to obtain $h'(y)$. This can be best illustrated with the following example.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Find the general solution to $6y(y-x)\\frac{dy}{dx} + (2x-3y^2) = 0$.\n \n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\nSet $Q = 6y(y-x), P= 2x-3y^2$. Then $P_y = -6y = Q_x$, meaning that the differential form is exact. First integrate $P$ with respect to $x$:\n$$\n        f = \\int P \\ dx = \\int \\frac{\\partial f}{\\partial x} \\ dx  = \\int 2x-3y^2 \\ dx =x^2 - 3xy^2 + h(y)\n$$\nfor some function $h(y)$ depending only on $y$. Differentiating this with respect to $y$ gives\n$$\n        Q + h'(y) =6y^2 - 6xy + h'(y) =  -6xy\n$$\nand thus $h'(y) = -6y^2, h(y) = -2y^3 + C$ for a constant $C$. The final solution is $f = x^2 - 3xy^2 + h(y) = x^2 - 3xy^2 - 2y^3 = C$.\n\n\n\n\n","n":0.043}}},{"i":136,"$":{"0":{"v":"Integrating Factors","n":0.707},"1":{"v":"## Non-constant coefficient DEs\n\nThis section will deal with first-order differential equations of the following form:\n\n$$\n    a(x)y' + b(x)y = c(x)\n$$\n\nin which the coefficients of $y$ and $y'$ are non-constant. In particular, the method of integrating factors will be introduced.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. **Integrating factors**. \n\nDivide the given equation by $a(x)$ to obtain the form\n$$\n    y' + \\frac{b(x)}{a(x)}y = \\frac{c(x)}{a(x)}\n$$\nor, equivalently, \n$$\n    y' + p(x)y = f(x)\n$$\nWe want the left-hand side to be expressible in the form of the derivative of a function $g'(x) = y' + p(x)y$; thus consider multiplying throughout by an *integrating factor* $\\mu$, such that $\\mu y' + \\mu y p(x)$ is $(\\mu y)' = \\mu y' + \\mu' y$. \n\nThis necessitates that $\\mu' = \\mu p(x)$, or $\\int \\frac{\\mu'}{\\mu} \\ dx = \\int p(x)\\ dx, \\ln \\mu = \\int p(x)\\ dx, \\mu = e^{\\int p(x)\\ dx}$. \n\n\n","n":0.084}}},{"i":137,"$":{"0":{"v":"Inhomogeneous Equations","n":0.707},"1":{"v":"Recall that an inhomogeneous equation is one that does not have $y=0$ as a solution; e.g. $5y' - 3y = 10$ is inhomogeneous, having an added term 10 on the right-hand side compared to the homogeneous equation. These added terms are \"forcing\" terms.\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. The **particular integral** solution. This method applies to inhomogeneous equations where one side is completely homogeneous with constant coefficients (all terms contain $y$ or a derivative of $y$) and the other side is a function of $x$. For instance, $2y' + 3y = 3x^2 + x$ is applicable.\n\nThe method for solving this type of equation is to first solve the equation as though it was homogeneous (turn the right-hand side to 0), which yields the *complementary function*, then guessing one particular solution to the equation, known as a *particular integral*.\n The general solution is the sum of both. \n\n Take the following radioactive decay model as an example:\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. A radioactive isotope $A$ decays at a rate proportional to the remaining amount of nuclei in its sample, $a$, into isotope $B$, which further decays at a rate proportional to the remaining amount of nuclei in its sample, $b$. <br/><br/>\nGiven that the constants of proportionality are $k_a$ and $k_b$ respectively, and that the initial number of nuclei present in each sample are $a_0$ and 0 respectively, determine an expression for the remaining nuclei in sample $B$, $b(t)$.\n\nWe have\n$$\n    \\begin{aligned}\n        \\frac{da}{dt} &= -k_a a \\\\\n        \\frac{db}{dt} &= k_a a - k_b b\n    \\end{aligned}\n$$\nSolving the first equation yields $a = \\lambda e^{-k_a t}$ for some constant $\\lambda$, with the initial condition that when $t=0$, $a=a_0$; thus, $a=a_0 e^{-k_a t}$. Substituting into the second equation yields $\\frac{db}{dt} = a_0 e^{-k_a t} - k_b b$, or $b' + k_b b = a_0 e^{-k_a t}$. \n\nThe homogeneous equation has solution $\\lambda e^{-k_b t}$ for any constant $\\lambda$. Try $b = \\alpha e^{-k_a t}, b' = -k_a \\alpha e^{-k_a t}$ for the particular integral; \nplugging this into the equation gives $-k_a \\alpha e^{-k_a t} + k_b \\alpha e^{-k_a t} = a_0 e^{-k_a t}$, and dividing both sides by $e^{-k_a t}$ gives $(k_b - k_a)\\alpha = a_0, \\alpha = \\frac{a_0}{k_b - k_a}$.\n\nWe also need $b(0) = 0$, so if $b = \\frac{a_0}{k_b - k_a}e^{-k_a t} + \\lambda e^{-k_b t}$, $\\frac{a_0}{k_b - k_a} + \\lambda = 0$, so $\\lambda = -\\frac{a_0}{k_b - k_a}$. Thus $b = \\frac{a_0}{k_b - k_a}(e^{-k_a t} - e^{-k_b t}).$ \n\n\n","n":0.05}}},{"i":138,"$":{"0":{"v":"Homogeneous Equations","n":0.707},"1":{"v":"> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. Any homogeneous linear ODE with constant coefficients has solutions of the form $e^{mx}$ (and all multiples of it). \n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>. The following section will present two derivations of the above statement:\n\n> <span style=\"background-color: #bc42f5; color: black;\">**Method 1 (discrete approximation)**. </span>  \n\nSuppose we are given the differential equation $ay' - by = 0$ for some constants $a$, $b$. One method of approaching this equation \nis to divide it into discrete timesteps of length $h$, for some $h$; that is, to define a sequence $y_0, y_1, y_2, ...$ such that $y_0 = y(0), y_1=y(h), y_2=y(2h)$, and so on. As $y'$ is geometrically the slope of $y$, \nwe can write the differential equation above as $a\\frac{y_{n+1}-y_n}{h} - by_n = 0$, which yields\n$$\n    \\begin{aligned}\n        a\\frac{y_{n+1}-y_n}{h}&=by_n \\\\\n        y_{n+1}-y_n &= \\frac{bh}{a}y_n \\\\\n        y_{n+1} &= \\frac{bh + a}{a}y_n \n    \\end{aligned}\n$$\napplying this formula recursively yields\n$$\n    y_n = (1 + \\frac{bh}{a})^n y_0\n$$\nNow let $h$ equal $\\frac{x}{n}$, such that $y_n = f(\\frac{nx}{x}) = f(x)$. As $n$ approaches infinity, the discrete approximation becomes infinitely close to the actual solution of the DE, so\n$$\n    y = \\lim_{n\\to \\infty} (1+\\frac{bx}{a}\\frac{1}{n})^n y_0 = y_0 e^{\\frac{bx}{a}}\n$$\ndue to the exponential limit.\n\n> <span style=\"background-color: #bc42f5; color: black;\">**Method 2 (Series solution).**</span>\n\nLet $y$ be the Taylor series $y = \\sum_{n=0}^{\\infty} a_n x^n$, with $y' = \\sum_{n=1}^{\\infty} n a_n x^{n-1}$. Again consider the DE $ay'-by=0$; substituting $y$ into this equation yields\n$$\n    \\begin{aligned}\n        ay' - by &= \\sum_{n=0}^{\\infty} a(n+1) a_{n+1} x^{n} - \\sum_{n=0}^{\\infty} ba_n x^n \\\\\n        &= \\sum_{n=1}^{\\infty} (a(n+1)a_{n+1} - ba_n)x^n \\\\\n        &= 0\n    \\end{aligned}\n$$\n(note that the $n+1$ term instead of $n$ originates from the fact that the bounds on the series have been adjusted to $0$ to $\\infty$, instead of $1$ to $\\infty$).\n\n\nThis implies that every term in the infinite sum is 0, or $a(n+1)a_{n+1} - ba_n = 0$; therefore, we write recursively that $a_{n+1} = \\frac{b}{a(n+1)}a_n$. Using this formula yields the general term $a_n = (\\frac{b}{a})^n\\frac{1}{n!} a_0$. Thus $y = a_0 \\sum_{n=0}^{\\infty}(\\frac{bx}{a})^n \\frac{1}{n!}$ which is once again the exponential function $e^{\\frac{bx}{a}}$.\n","n":0.055}}},{"i":139,"$":{"0":{"v":"Difference Equations","n":0.707},"1":{"v":"Difference equations are the discrete counterpart to differential equations; in essence, whereas differential equations are concerned with the rate of change of a continuous function, difference equations concern the change of a sequence. For instance, in the logistic population model, instead of a continuous change in population, population may change per season (spring or winter). \n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. We define a **difference equation** using terms similar to that of differential equations. For difference equation in $y(x)$, if $y_{n-1}$ is analogous to $y(x)$, then $y_{n}$ is analogous to $y'$, $y_{n+1}$ to $y''$, etc. We can also classify difference equations in similar ways to differential equations:\n1. Whether the equation is **linear**, i.e. whether it features powers of $y$ beyond the first.\n2. Whether the equation is **homogeneous**, i.e. features the variable $x$.\n3. The **order** of the equation, i.e. how many of $y_{n}, y_{n-1}, ...$ does it feature?\n\nDifference equations also have fixed points, such as in the following example:\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. Investigate the stability of the fixed points of the difference equation\n$$\n    u_{n+1} = 4u_n(1-u_n).\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Solution</span>.\n\nA *fixed point* of a difference equation is a point $u_n$ such that $u_{n+1}=u_n$, thereby making all following terms equal to $u_n$ as well (the sequence is \"fixed\" at that value). If a certain $u_n = u^*$ satisfies this property, then we have $u_{n+1} = 4u^*(1-u^*) = u^*$, which yields $3u^*=4(u^*)^2$ - a quadratic in $u^*$ with solutions $u^* = 0, \\frac{3}{4}$. \n\nTo investigate the stability of these fixed points, we employ a similar method to that of DEs: let $\\epsilon$ represent a small perturbation from the fixed point $u^*$ and consider the value of $u_{n+1}$ if $u_n$ instead equaled $u^* + \\epsilon_n$. \n\nDenote $u_{n+1}$ by $f(u_n)$; also let $u_{n+1}$ equal $u_n + \\epsilon_{n+1}$, where $\\epsilon_{n+1}$ is the perturbation of $u_{n+1}$ away from the fixed point $u_n$ that we are trying to find. At $u_n = u^* + \\epsilon$, $u_{n+1} = u_n + \\epsilon_{n+1} = f(u^* + \\epsilon) \\approx f(u^*) + \\epsilon_n f'(u^*)$; as $u^*$ is a fixed point, $f(u^*) = u_n$ and so $\\epsilon_{n+1} \\approx \\epsilon_n f'(u^*)$. \n\nThis is analogous to the perturbation theorem for differential equations, and yields that $\\epsilon_{n} = \\epsilon_0 [f'(u^*)]^n$ due to it following a geometric progression, for some initial perturbation $\\epsilon_0$. As such, if $|f'(u^*)| > 1$ the perturbation diverges with an unstable fixed point, and if $|f'(u^*)| < 1$ it converges and the fixed point is stable. \n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Fixed point stability for difference equations**. For thee autonomous difference equation\n$$\nu_{n+1} = f(u_n)\n$$\n> analogous to the autonomous system\n$$\n\\frac{dy}{dx}=f(y),\n$$\n> the stability of the point $u^*$ is determined by the value of $|f'(u*)|$: if $|f'(u^*)| > 1$ the perturbation diverges with an unstable fixed point, and if $|f'(u^*)| < 1$ it converges and the fixed point is stable. \n\nApplying this statement to the fixed points $u^* = 0, \\frac{3}{4}$ with $f'(u^*) = 4 - 8u^*$ yields instability for both points. \n","n":0.045}}},{"i":140,"$":{"0":{"v":"Autonomous Systems","n":0.707},"1":{"v":"> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Autonomous systems). **Autonomous systems are systems whose behavior do not change with time; in other words, $\\frac{dy}{dt} = f(y)$ is purely a function of $y$ (itself).\n\nPerturbation analysis on these systems show that $\\frac{d \\epsilon}{dt} = k\\epsilon$ where $k$ is a constant, meaning that the only factor determining whether a fixed point in an autonomous system is stable is the value of $k$ at that particular value of $y$. The following section will introduce two relevant examples related to autonomous systems and perturbation analysis.\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. **Chemical kinetics**.\n\nConsider a chemical reaction NaOH + HCl $\\to$ H$_2$O + NaCl. Initially, there are $a_0$ and $b_0$ molecules of NaOH and HCl respectively; at time $t$, there will be $a(t), b(t), c(t)$ and $c(t)$ molecules of the four substances respectively. If $\\frac{dc}{dt}$ is proportional to the product of the number of molecules of NaOH and HCl, what number will $c$ converge to?\n\nThe equation that governs this reaction is $\\frac{dc}{dt} = \\lambda a b = \\lambda (a_0 - c)(b_0 - c)$, where $\\lambda$ is a constant; this is autonomous as the right-hand side has only one variable, $c$. Thus $\\frac{dc}{dt}$ can be plotted, and using that plot the two equilibrium points can be found ($a_0$ and $b_0$). The stability of these points can be found by analyzing the sign of $\\frac{dc}{dt}$, which can be represented on a phase portrait (not shown here).\n\n> <span style=\"background-color: #03cafc; color: black;\">Example</span>. **Population logistics**.\n\nSuppose we have a population of size $y$; at any time, this population will have a number of births equal to $\\alpha y$ and a number of deaths equal to $\\beta y$. Thus we write\n\n$$\n    \\frac{dy}{dt} = (\\alpha - \\beta)y\n$$\nand thus \n$$\n    y=y_0 e^{(\\alpha - \\beta)y}\n$$\ndepending on initial conditions. Let's now introduce more conditions to this population model. Suppose that this population has been conscripted to fight in a holy crusade against the heathens and devils of the world. They die in this crusade at a rate of $\\gamma y^2$ at every time unit, meaning that\n$$\n    \\frac{dy}{dt} = (\\alpha - \\beta)y - \\gamma y^2\n$$\nwhich is autonomous as the right-hand side depends only on $y$. If $\\frac{dy}{dt}$ is plotted, it can be shown that $\\frac{\\alpha - \\beta}{\\gamma}$ is a stable fixed point.\n\n","n":0.052}}},{"i":141,"$":{"0":{"v":"Differentiation and Integration","n":0.577},"1":{"v":"This section will conduct a simple review of the concepts of integration and differentiation to prepare for later concepts in differential equations.\n\n## Differentiation\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (**Derivatives/differentiation**).\nThe derivative of a function $f(x)$ with respect to $x$ is interpreted as the rate of change of $f(x)$, and is denoted $f'(x)$ or $\\frac{df}{dx}$.\nIt is defined as $\\frac{df}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$.\n\nA function is differentiable at x if this limit exists, i.e. it is the same from the left-hand and right-hand side. For instance, $|x|$ is not differentiable at $x = 0$.\n\n## Big-O and small-o notation\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span> (**Big-O**). $f(x) = O(g(x))$ as $x$ approaches $x_0$ means that $f(x)$ approaches a constant multiple of $g(x)$ at $x_0$. This means that $f(x)$ is roughly on the order of $g(x)$; e.g. $4x^2 + 3x = O(x^2)$.\n\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **(Small-o)**. $f(x) = o(g(x))$ as $x$ approaches $x_0$ means that $f(x)$ is part of the class of functions for which $f$ is much smaller than $g$ at $x_0$, or $\\lim_{x \\to x_0} \\frac{f(x)}{g(x)} = 0$.\n\n## Basic propositions and theorems\n\n> <span style=\"background-color: #ffb812; color: black;\">Proposition</span>. $f(x_0 + h) = f(x_0) +  f'(x_0)h + o(h)$. ($h$ is some infinitesimal number close to 0, and $o(h)$ denotes some function which is much smaller than $h$ when $h$ approaches 0.)\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nWe have\n$$\n    f'(x_0) =  \\frac{f(x_0+h) - f(x_0)}{h}\n$$\nAnd thus   \n$$\n    f'(x_0) = \\frac{f(x_0+h) - f(x_0)}{h} + o(h)\n$$\nfor some $o(h)$, as the last term is 0 when $h$ tends to 0. Reorganizing terms gives the above.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Taylor's theorem**. For some $f$ which is $n$-times differentiable, we have\n$$\n    f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2!}f''(x) + ... + \\frac{h^n}{n!}f^{(n)}(x) + E_n\n$$\n> where $E_n$ is some function on the order of $O(h^{n+1})$ as $h$ tends to 0. A similar approximation to the sum above is\n$$\nf(x) = f(x_0) + (x-x_0)f'(x_0) + \\frac{(x-x_0)^2}{2!}f''(x_0) + ... + E_n\n$$\nwhere, as $n \\to \\infty$, the Taylor series of $f(x)$ is obtained.\n\nTaylor's theorem ranks just barely in the top ten of Taylor's most significant contributions towards humanity, right behind \"Firework\" and, admittedly, far ahead of \"Shake It Off\".\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **L'Hopital's rule**. Let $f(x)$ and $g(x)$ be differentiable at $x = x_0$, and $\\lim_{x \\to 0} f(x) = \\lim_{x \\to 0} g(x) = 0$. Then\n$$\n    \\lim_{x \\to x_0} \\frac{f(x)}{g(x)} = \\lim_{x \\to x_0} \\frac{f'(x)}{g'(x)}.\n$$\n\n> <span style=\"background-color: #1eff12; color: black;\">Proof</span>.\n\nUsing Taylor's theorem, we have \n$$\n    f(x) =  f(x_0) + (x-x_0)f'(x_0) +  o(x-x_0)\n$$\nas the later terms all contain higher powers of $(x-x_0)$, which is smaller than $(x-x_0)$ as $x$ approaches $x_0$. A similar result can be derived for $g(x)$.\nFurthermore, $f(x_0)$ = $g(x_0)$ = 0 when $x$ tends to $x_0$.\n\nAs such, we can rewrite the limit as\n$$\n    \\lim_{x \\to x_0} \\frac{f(x)}{g(x)} = \\lim_{x \\to x_0} \\frac{f(x_0) + f'(x_0)(x-x_0) + o(x-x_0)}{g(x_0) + g'(x_0)(x-x_0)+o(x-x_0)}  \n$$\nWhich is \n$$\n    \\lim_{x \\to x_0} \\frac{f(x)}{g(x)} = \\lim_{x \\to x_0} \\frac{f'(x_0) + \\frac{o(x-x_0)}{x-x_0}}{g'(x_0) + \\frac{o(x-x_0)}{x-x_0}}  \n$$\nrecognizing that $f(x_0)$ = $g(x_0)$ = 0 and dividing by $x-x_0$ throughout. \n\nBy the definition of $o(x-x_0)$, the term $\\frac{o(x-x_0)}{x-x_0}$ approaches 0 at $x_0$, thus yielding the rule.\n\n## Integration\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. (**Integral**). An **integral** is the limit of the sum \n$$\n\\int_{b}^{a} f(x) \\,dx =  \\lim_{\\Delta x \\to 0} \\sum_{n = 0}^{N} f(x_n) \\Delta x    \n$$\n> Where $f(x_i) = b + i\\Delta x$. The special kind of integral studied in current-level calculus is the Riemann integral, which is the area under the curve, approached by sub-dividing the area into an infinite amount of infinitesimal rectangles.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Fundamental theorem of calculus**. Let $F(x)$ be $\\int_{a}^{x} f(t) \\,dt$. Then $F'(x) = f(x)$. Note here that we write $\\int f(x) \\ dx$ as $\\int_{}^{} f(t) \\,dt$ where the unspecified lower limit indicates an unknown constant of integration. \n\n## Partial differentiation\n\n> <span style=\"background-color: #03cafc; color: black;\">Definition</span>. **Partial derivatives.** The partial derivative of a multivariate function $f(x,y)$ with respect to $x$ is the rate of change of $f$ as $y$ is kept constant, but $x$ changes. More precisely, it is given by \n$$\n    \\frac{\\partial f}{\\partial x} = \\lim_{\\delta x \\to 0} \\frac{f(x+\\delta x, y) - f(x,y)}{\\delta x}\n$$\n\n> We write $f_x = \\frac{\\partial f}{\\partial x}$, $f_{xy} = \\frac{\\partial^2 f}{\\partial y \\partial x}$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. If $f$ has continuous second derivatives, then $f_{xy}$ = $f_{yx}$.\n\n> <span style=\"background-color: #12ffd7; color: black;\">Theorem</span>. **Chain rule for partial derivatives**. For $z=f(x,y)$ and some variable $t$, where $x$ and $y$ can be written as functions of $t$,\n$$\n\\frac{dz}{dt}=\\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt}\n$$\n> As such we can also write\n$$\n\\int df =  \\int \\frac{\\partial f}{\\partial x} dx + \\int \\frac{\\partial f}{\\partial y} dy\n$$\n\n> <span style=\"background-color: #bc42f5; color: black;\">Method</span>. **Differentiation under the integral sign**. Consider a group of functions $f(x,c)$.\nDefine $I(b,c) = \\int_{0}^{b } f(x,c) \\,dx$, and by the fundamental theorem of calculus we have $\\frac{\\partial I}{\\partial b} = f(b,c)$.\nHowever, we also have \n$$\n\\begin{aligned}\n\\frac{\\partial I}{\\partial c} &= \\lim_{\\delta c \\to 0} \\frac{I(b, c+\\delta c) - I(b,c)}{\\delta c} \\\\\n&= \\lim_{\\delta c \\to 0} \\int_{0}^{b} \\frac{f(x, c+\\delta c)  - f(x,c)}{\\delta c}\\,dx \\\\\n&= \\int_{0}^{b} \\lim_{\\delta c \\to 0} \\frac{f(x, c+\\delta c)  - f(x,c)}{\\delta c}\\,dx \\\\\n&= \\int_{0}^{b} \\frac{\\partial f}{\\partial c} dx\n\\end{aligned}\n$$\n> If $b$ and $c$ are both functions of $x$, then we obtain the general result\n$$\n\\begin{aligned}\n\\frac{dI}{dx} &= \\frac{\\partial I}{\\partial b} \\frac{\\partial b}{\\partial x} + \\frac{\\partial I}{\\partial c} \\frac{\\partial c}{\\partial x} \\\\\n&= f(b(x), c(x))b'(x) + c'(x)\\int_{0}^{b(x)} \\frac{\\partial f}{\\partial c} dy\n\\end{aligned}\n$$\n> which makes use of the previous fact that $\\frac{\\partial I}{\\partial b} = f(b,c)$ and $y$ is simply a dummy variable.\n","n":0.033}}}]}
